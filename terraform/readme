Enabling EKS Auto Mode with Terraform  requires setting compute_config.enabled, kubernetes_network_config.elastic_load_balancing.enabled, and storage_config.block_storage.enabled to true, as well as bootstrap_self_managed_addons to false.
Let's execute the following command to import the cluster IAM role to the Terraform state:

terraform import aws_iam_role.cluster_role $DEMO_CLUSTER_ROLE_NAME

Since the cluster has already been created for us, we'll have to modify the cluster IAM role trust policy per the EKS Auto Mode documentation . That means that before planning and applying the resources, we should import the AWS IAM role to the Terraform state.


NAME                                         CREATED AT
cninodes.eks.amazonaws.com                   2024-12-04T13:03:50Z
cninodes.vpcresources.k8s.aws                2024-12-04T13:00:46Z
ingressclassparams.eks.amazonaws.com         2024-12-04T13:03:49Z
nodeclaims.karpenter.sh                      2024-12-04T13:03:39Z
nodeclasses.eks.amazonaws.com                2024-12-04T13:03:39Z
nodediagnostics.eks.amazonaws.com            2024-12-04T13:03:39Z
nodepools.karpenter.sh                       2024-12-04T13:03:39Z
policyendpoints.networking.k8s.aws           2024-12-04T13:00:46Z
securitygrouppolicies.vpcresources.k8s.aws   2024-12-04T13:00:46Z
targetgroupbindings.eks.amazonaws.com        2024-12-04T13:03:49Z


These CRDs enable several crucial capabilities of EKS Auto Mode:

nodepools support provisioning compute
ingressclassparams and targetgroupbinding allow exposing applications, and
nodediagnostics provide diagnostics capabilities

To explore these capabilities, we will initially deploy the application in a manner that is self-contained in the Amazon EKS cluster, without using any AWS services that provision load balancers or managed databases. 









cat << EOF > ~/environment/values-ui.yaml

app:
  theme: default
  endpoints:
    catalog: http://retail-store-app-catalog:80
    carts: http://retail-store-app-carts:80
    checkout: http://retail-store-app-checkout:80
    orders: http://retail-store-app-orders:80
EOF

helm upgrade -i retail-store-app-catalog oci://public.ecr.aws/aws-containers/retail-store-sample-catalog-chart --version ${RETAIL_STORE_APP_HELM_CHART_VERSION} --hide-notes
helm upgrade -i retail-store-app-orders oci://public.ecr.aws/aws-containers/retail-store-sample-orders-chart --version ${RETAIL_STORE_APP_HELM_CHART_VERSION} --hide-notes
helm upgrade -i retail-store-app-carts oci://public.ecr.aws/aws-containers/retail-store-sample-cart-chart --version ${RETAIL_STORE_APP_HELM_CHART_VERSION} --hide-notes
helm upgrade -i retail-store-app-checkout oci://public.ecr.aws/aws-containers/retail-store-sample-checkout-chart --version ${RETAIL_STORE_APP_HELM_CHART_VERSION} --hide-notes
helm upgrade -i retail-store-app-ui oci://public.ecr.aws/aws-containers/retail-store-sample-ui-chart --version ${RETAIL_STORE_APP_HELM_CHART_VERSION} -f ~/environment/values-ui.yaml --hide-notes


The commands above should produce helm outputs for each component of the app (catalog, orders, cart, checkout, and ui) similar to the following output:

Amazon EKS Auto Mode will evaluate the resource requirements of these Pods and determine the optimum compute to launch for our applications to run, considering any scheduling constraints configured.

These Services are internal to the cluster, so we cannot access them from the Internet or even the VPC. However, we can use port-forwarding  to access an existing Pod in the EKS cluster to check that the application UI is working.

kubectl port-forward $(kubectl get pods \
 --selector=app.kubernetes.io/name=ui -o jsonpath='{.items[0].metadata.name}') 8080:8080




 ## Environment Variables

All environment variables used in this project are listed in `.env.example`.  
Copy it to `.env` and fill in the real values:

```bash
cp .env.example .env


Node Launch

Event ends in 19 minutes.

Event dashboard
...
Compute
Node Launch
Event dashboard
Module 2 - Explore EKS Auto Mode capabilities & fundamentals
Compute
Node Launch
Node Launch
Overview | Review the EKS Auto Mode configuration | Scale the application | Improve the application resilience | Summary

Overview
When using EKS Auto Mode, cluster compute resources are automatically provisioned and managed by EKS. The service automates routine tasks for creating new EC2 instances and registering them as nodes to your EKS cluster. When a workload cannot be scheduled onto existing nodes, EKS Auto Mode creates a new, appropriately sized EC2 instance.

EC2 instances created by EKS Auto Mode are EC2 managed instances . These provide a simplified way to run compute workloads on Amazon EC2 by delegating operational control to a service provider - in this case, EKS Auto Mode.

By delegating control to EKS Auto Mode, you benefit from AWS's operational expertise and best practices for running Amazon EKS. EKS handles tasks such as provisioning instances, configuring software, scaling capacity, and managing instance failures and replacements. While you maintain visibility of the managed instances through the AWS console and can use instance storage as ephemeral storage for workloads, direct access and software installation on these instances is not permitted. You can review the list of supported instance types here  and see a detailed comparison  between standard EC2 instances and EKS Auto Mode managed instances.



Review the EKS Auto Mode configuration
EKS Auto Mode's node provisioning relies on Karpenter  objects, with dedicated specifications for NodeClasses  and NodePools .

The NodeClass specification allows to define infrastructure-level settings for groups of nodes, including network configuration, storage settings, and resource tagging.

The NodePool specification enables fine-grained control over compute resources through various supported labels and compute requirements configuration. This includes options for EC2 instance categories, CPU configurations, availability zones, architectures (ARM64/AMD64), and capacity types (spot/on-demand). You can also set resource limits for CPU and memory usage to maintain desired operational boundaries.

EKS Auto Mode includes two default managed node pools: general-purpose and system. The general-purpose node pool handles user-deployed applications and services, while the system node pool is dedicated to critical system-level components managing cluster operations. Custom node pools can be created for specific compute or configuration requirements.

Let's explore the built-in node pools and their managed instances.


Review node pools configuration
➤ View the managed instances in the general-purpose node pool:

kubectl get nodes -l karpenter.sh/nodepool=general-purpose

The output should show a single managed instance:

NAME                  STATUS   ROLES    AGE   VERSION
i-05e5427f9dc2f4b2e   Ready    <none>   31m   v1.32.3-eks-7636447
➤ View the managed instances in the system node pool:

kubectl get nodes -l karpenter.sh/nodepool=system

his should produce an empty result:

No resources found
Note: The system node pool is currently empty as no system components have been installed. All application pods are running on the general-purpose node pool.

➤ We can verify this by checking the Pod distribution across the general-purpose nodes:

for node in $(kubectl get nodes -l karpenter.sh/nodepool=general-purpose -o custom-columns=NAME:.metadata.name --no-headers); do
  echo "Pods on $node:"
  kubectl get pods --all-namespaces --field-selector spec.nodeName=$node
done

The expected output should be similar to the following:

Pods on i-0ee0842e974bafc6c:
NAMESPACE   NAME                                        READY   STATUS    RESTARTS   AGE
default     retail-store-app-carts-5f5b7449f-tgscs      1/1     Running   0          8m3s
default     retail-store-app-catalog-dcb5d8d4c-5ftp9    1/1     Running   0          8m6s
default     retail-store-app-checkout-f5bb5c5bb-pkgg7   1/1     Running   0          8m2s
default     retail-store-app-orders-5fbb6b8576-x6vs9    1/1     Running   0          8m5s
default     retail-store-app-ui-7b7c8f6b94-2ltrs        1/1     Running   0          8m
Currently, all pods are running on a single node since it adequately meets the workloads' resource requirements without any specific scheduling constraints.


Scale the application
Let's manually scale the application UI component to observe how Auto Mode automatically provisions new nodes to meet the increased demand.

➤ In a separate IDE terminal, watch for new nodes:

watch kubectl get nodes

This command will continuously monitor and display changes to cluster nodes. To exit, press Ctrl/Cmd+C in the terminal.

➤ Scale the UI component:

kubectl scale --replicas=12 deployment/retail-store-app-ui

This should produce this output:

deployment.apps/retail-store-app-ui scaled
The watch command output should eventually show new nodes (initially with NotReady status) being added to accommodate the additional UI component replicas:

Every 2.0s: kubectl get nodes                                                                ip-192-168-0-253.us-west-2.compute.internal: Tue Feb 25 21:23:39 2025

NAME                  STATUS   ROLES    AGE   VERSION
i-00642042ad4eff3c9   Ready    <none>   17s   v1.30.8-eks-3c20087
i-009db40705eb0417d   Ready    <none>   10m   v1.30.8-eks-3c20087
➤ To see how EKS Auto Mode responded to the scaling event, examine the cluster events:

kubectl events

The output will include events related to pods, NodePools, and Nodes.

Expand to view the events
➤ Let's examine the current Pod distribution:

for node in $(kubectl get nodes -l karpenter.sh/nodepool=general-purpose -o custom-columns=NAME:.metadata.name --no-headers); do
  echo "Pods on $node:"
  kubectl get pods --all-namespaces --field-selector spec.nodeName=$node
done

The command should produce an output similar to the following:

Pods on i-092974f55c2956b90:
NAMESPACE   NAME                                   READY   STATUS    RESTARTS   AGE
default     retail-store-app-ui-7b7c8f6b94-4vdfm   1/1     Running   0          3m13s
default     retail-store-app-ui-7b7c8f6b94-8tgkg   1/1     Running   0          3m14s
default     retail-store-app-ui-7b7c8f6b94-8xcvs   1/1     Running   0          3m14s
default     retail-store-app-ui-7b7c8f6b94-bhlt8   1/1     Running   0          3m13s
default     retail-store-app-ui-7b7c8f6b94-c8rfs   1/1     Running   0          3m14s
default     retail-store-app-ui-7b7c8f6b94-jjlmd   1/1     Running   0          3m14s
default     retail-store-app-ui-7b7c8f6b94-p8frf   1/1     Running   0          3m13s
default     retail-store-app-ui-7b7c8f6b94-qnqrn   1/1     Running   0          3m13s
default     retail-store-app-ui-7b7c8f6b94-xdxqc   1/1     Running   0          3m14s
default     retail-store-app-ui-7b7c8f6b94-xsqds   1/1     Running   0          3m14s
Pods on i-0ee0842e974bafc6c:
NAMESPACE   NAME                                        READY   STATUS    RESTARTS   AGE
default     retail-store-app-carts-5f5b7449f-tgscs      1/1     Running   0          12m
default     retail-store-app-catalog-dcb5d8d4c-5ftp9    1/1     Running   0          12m
default     retail-store-app-checkout-f5bb5c5bb-pkgg7   1/1     Running   0          12m
default     retail-store-app-orders-5fbb6b8576-x6vs9    1/1     Running   0          12m
default     retail-store-app-ui-7b7c8f6b94-2ltrs        1/1     Running   0          12m
default     retail-store-app-ui-7b7c8f6b94-4hwth        1/1     Running   0          3m15s
Note that the pods are currently densely packed and scheduled onto a single node, which reduces resilience. In the next step, we'll explore how to improve that using EKS Auto Mode capabilities.

Improve the application resilience
Topology spread constraints
To enhance application resiliency, it's crucial to distribute pods across multiple AZs and nodes. We'll apply Pod Topology Spread Constraints  to our UI component to ensure better fault tolerance and distribution.

Update the UI component
➤ Update the values-ui.yaml file and re-deploy the Helm chart:

cat  << EOF >~/environment/values-ui.yaml
app:
  theme: default
  endpoints: 
    catalog: http://retail-store-app-catalog:80
    carts: http://retail-store-app-carts:80
    checkout: http://retail-store-app-checkout:80
    orders: http://retail-store-app-orders:80

topologySpreadConstraints:
  - maxSkew: 1
    minDomains: 3
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        app.kubernetes.io/name: ui
EOF

helm upgrade -f ~/environment/values-ui.yaml retail-store-app-ui oci://public.ecr.aws/aws-containers/retail-store-sample-ui-chart --version ${RETAIL_STORE_APP_HELM_CHART_VERSION} --hide-notes

The output should be similar to:

Pulled: public.ecr.aws/aws-containers/retail-store-sample-ui-chart:1.1.0
Digest: sha256:5cd721c10214c306b06c7223367f626f21a8d471eee8f0a576742426f84141f2
Release "retail-store-app-ui" has been upgraded. Happy Helming!
NAME: retail-store-app-ui
LAST DEPLOYED: Mon Jan 20 20:33:32 2025
NAMESPACE: default
STATUS: deployed
REVISION: 2
The configuration specifies a maximum skew of 1 and requires a minimum of 3 AZs, ensuring that the difference in pod count between any two zones does not exceed 1. When the constraints cannot be satisfied, pods will not be scheduled (DoNotSchedule), prioritizing proper distribution over immediate deployment.

Note
The manually-scaled UI component will automatically reset to the default number of replicas (which is 1) after we deploy the updated configuration.

➤ Now, let's scale the application again to see how Pod spread topology impacts the scaling behavior:

kubectl scale --replicas=12 deployment/retail-store-app-ui

As before, this is the expected output:

deployment.apps/retail-store-app-ui scaled
➤ Wait for all the scaled UI component pods to become ready:

kubectl wait --for=condition=Ready pod -l app.kubernetes.io/instance=retail-store-app-ui --namespace default --timeout=300s

➤ Now we can verify that the UI component pods are spread across different availability zones:

kubectl get node -L topology.kubernetes.io/zone --no-headers | while read node status roles age version zone; do
echo "Pods on node $node (Zone: $zone):"
  kubectl get pods --all-namespaces --field-selector spec.nodeName=$node -l app.kubernetes.io/instance=retail-store-app-ui
echo "-----------------------------------"
done

This should produce an output similar to the following:

Pods on node i-08104c0f996d4db79 (Zone: us-west-2a):
NAMESPACE   NAME                                   READY   STATUS    RESTARTS   AGE
default     retail-store-app-ui-7fbf6d97b9-d9tqs   1/1     Running   0          2m1s
default     retail-store-app-ui-7fbf6d97b9-ghkbm   1/1     Running   0          2m1s
default     retail-store-app-ui-7fbf6d97b9-lm6fp   1/1     Running   0          2m1s
default     retail-store-app-ui-7fbf6d97b9-t9pbl   1/1     Running   0          2m27s
-----------------------------------
Pods on node i-09301d4c5dea017fb (Zone: us-west-2c):
NAMESPACE   NAME                                   READY   STATUS    RESTARTS   AGE
default     retail-store-app-ui-7fbf6d97b9-9bncz   1/1     Running   0          2m2s
default     retail-store-app-ui-7fbf6d97b9-wm76b   1/1     Running   0          2m2s
default     retail-store-app-ui-7fbf6d97b9-zqg2z   1/1     Running   0          2m2s
default     retail-store-app-ui-7fbf6d97b9-zwfw7   1/1     Running   0          40s
-----------------------------------
Pods on node i-0ed7e4a8df8557487 (Zone: us-west-2b):
NAMESPACE   NAME                                   READY   STATUS    RESTARTS   AGE
default     retail-store-app-ui-7fbf6d97b9-4vlzz   1/1     Running   0          2m3s
default     retail-store-app-ui-7fbf6d97b9-847q2   1/1     Running   0          2m3s
default     retail-store-app-ui-7fbf6d97b9-f42hz   1/1     Running   0          2m3s
default     retail-store-app-ui-7fbf6d97b9-mkn7q   1/1     Running   0          2m3s
-----------------------------------
The Pod topology spread constraints have successfully distributed the workload across multiple AZs and nodes, ensuring high availability and fault tolerance.

Summary
In this section, we've examined EKS Auto Mode's built-in system and general-purpose node pools, demonstrated manual scaling of the application, and improved its resilience by implementing AZ-level topology spread constraints for the UI component.

In the next section, we'll explore automated scaling policies for the UI application to replace manual scaling operations. We'll examine how EKS Auto Mode manages dynamic resource allocation and workload distribution in response to these policies.

Autoscaling
Overview | Horizontal Pod Autoscaling | Summary

This section will guide you through the basics of EKS Auto Mode and explore, in details, application and cluster autoscaling.

Overview
Disruptions in EKS Auto Mode
Before we dive into autoscaling, let's first understand how disruptions  are managed in EKS Auto Mode. Disruptions can occur in situations such as when nodes are scaled down to reduce cluster costs (like bin-packing), or hit their maximum lifetime (expiry date). This potentially impacts the running pods on those nodes. EKS Auto Mode uses Karpenter under the hood to optimize node scaling and manage these disruptions effectively.

Karpenter manages node disruptions through three key mechanisms: expiration, drift detection, and consolidation, with the latter being the focus of this section, as more relevant to autoscaling.

Consolidation
Consolidation works by continuously monitoring the utilization of nodes and pods in the cluster. When nodes become underutilized or idle, Karpenter initiates a consolidation process to optimize cluster resources. This involves removing nodes without active workloads, efficiently bin-packing pods onto existing nodes where capacity permits, and performing graceful node draining by carefully evicting and rescheduling pods to maintain application availability throughout the consolidation process.

Below you can see the highlighted configuration of the disruption block in the provided general-purpose NodePool that is created and managed by EKS Auto Mode

➤ Get the NodePool configuration:

kubectl get nodepools general-purpose -o yaml

This should show the following output:

apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  annotations:
    karpenter.sh/nodepool-hash: "4012513481623584108"
    karpenter.sh/nodepool-hash-version: v3
  creationTimestamp: "2025-01-15T09:32:29Z"
  generation: 1
  labels:
    app.kubernetes.io/managed-by: eks
  name: general-purpose
  resourceVersion: "241001"
  uid: 9b1c4ad0-d42d-4c63-bd96-b0a201aeec0e
spec:
  disruption:
    budgets:
    - nodes: 10%
    consolidateAfter: 30s
    consolidationPolicy: WhenEmptyOrUnderutilized
  template:
    metadata: {}
    spec:
      expireAfter: 336h
      nodeClassRef:
        group: eks.amazonaws.com
        kind: NodeClass
        name: default
      requirements:
      - key: karpenter.sh/capacity-type
        operator: In
        values:
        - on-demand
      - key: eks.amazonaws.com/instance-category
        operator: In
        values:
        - c
        - m
        - r
      - key: eks.amazonaws.com/instance-generation
        operator: Gt
        values:
        - "4"
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
      - key: kubernetes.io/os
        operator: In
        values:
        - linux
      terminationGracePeriod: 24h0m0s
The configuration property WhenEmptyOrUnderutilized ensures that Karpenter will consider all nodes for consolidation and attempt to remove or replace nodes when it discovers that a node is empty or under-utilized and could be removed or replaced to reduce cost. expireAfter is set to a custom value so that nodes are terminated automatically after 336 hours (14 days). The budget configuration block control the speed Karpenter can scale down nodes.

Horizontal Pod Autoscaling
In this lab, we'll explore how the Horizontal Pod Autoscaler (HPA) automatically scales pods in Kubernetes based on observed metrics. The HPA consists of a resource definition and a controller that periodically checks resource utilization (such as CPU, memory, or custom metrics) against user-defined targets and adjusts the number of replicas accordingly.

The Metrics Server component is essential for collecting and aggregating resource usage data from the cluster and providing HPA with the necessary metrics to make scaling decisions.

Install Metrics Server
To enable application-level autoscaling in EKS Auto Mode, we need to install the Metrics Server. AWS offers a streamlined installation process through Community Add-ons  to simplify deployment and management.

We can create an Amazon EKS add-on using eksctl, the AWS Management Console, or the AWS CLI. For add-ons requiring an IAM role, refer to Available Amazon EKS add-ons from AWS  for details about creating the role.

Create add-on (eksctl)
➤ Use eksctl to install the metrics-server addon:

eksctl create addon --name metrics-server --cluster ${DEMO_CLUSTER_NAME}

Creating the metrics-server add-on should take about 1 minute.

The output should look similar to the following:

2025-01-16 00:06:09 [ℹ]  Kubernetes version "1.32" in use by cluster "demo-cluster"
2025-01-16 00:06:09 [ℹ]  creating addon
2025-01-16 00:07:00 [ℹ]  addon "metrics-server" active
➤ After the above installation commands completes, let's confirm that the Metrics Server is running:

kubectl get deployment metrics-server -n kube-system

with the expected output:

NAME             READY   UP-TO-DATE   AVAILABLE   AGE
metrics-server   2/2     2            2           99s
To get a view of the metrics that HPA will use to drive its scaling behavior, use the kubectl top command.

➤ For example, the below commands will show the resource utilization of the nodes and UI component pods in our cluster:

kubectl top node
kubectl top pods -l app.kubernetes.io/name=ui

The result should be similar to the following:

NAME                  CPU(cores)   CPU(%)   MEMORY(bytes)   MEMORY(%)
i-010ef666810cc74fe   20m          1%       589Mi           18%
i-014b2a0091dc49a40   19m          0%       586Mi           18%
i-0234821e6c1789b57   44m          2%       596Mi           19%
i-04eb412b3e09844ef   39m          2%       595Mi           19%
i-066e1d9c08f13c0ed   28m          1%       1670Mi          53%
i-06d0fd146733a2985   40m          2%       599Mi           19%
i-07c121b110f507617   45m          2%       597Mi           19%
i-0b87fcb18c6146df8   65m          3%       598Mi           19%
i-0c1cfed844ac873ea   36m          1%       1268Mi          40%
i-0db78db542f9b25da   55m          2%       603Mi           19%
i-0ed869e02ea7e95e4   11m          0%       599Mi           19%
i-0f8137d7076ba89f1   20m          1%       686Mi           22%
NAME                                  CPU(cores)   MEMORY(bytes)
retail-store-app-ui-697bbcdb5-2rcbq   1m           221Mi
retail-store-app-ui-697bbcdb5-hxgqp   1m           212Mi
retail-store-app-ui-697bbcdb5-jfdc6   1m           217Mi
retail-store-app-ui-697bbcdb5-jrtdm   2m           215Mi
retail-store-app-ui-697bbcdb5-kv79b   1m           221Mi
retail-store-app-ui-697bbcdb5-p99gf   1m           214Mi
retail-store-app-ui-697bbcdb5-qwpdb   1m           222Mi
retail-store-app-ui-697bbcdb5-sx4sp   2m           217Mi
retail-store-app-ui-697bbcdb5-wczzw   2m           226Mi
retail-store-app-ui-697bbcdb5-whxxf   2m           217Mi
retail-store-app-ui-697bbcdb5-zn667   1m           211Mi
retail-store-app-ui-697bbcdb5-zwhw8   1m           217Mi
Configure HPA
Currently, there are no resources in our cluster that enable Horizontal Pod Autoscaling.

➤ Let's verify this by executing:

kubectl get hpa --all-namespaces

with the expected output:

No resources found
Now, we'll add autoscaling configurations to the UI component based on its CPU usage by updating our values-ui.yaml file with autoscaling config and re-deploying the UI component using the corresponding Helm chart.

Configure and re-deploy the UI component
cat << EOF >~/environment/values-ui.yaml
app:
  theme: default
  endpoints: 
    catalog: http://retail-store-app-catalog:80
    carts: http://retail-store-app-carts:80
    checkout: http://retail-store-app-checkout:80
    orders: http://retail-store-app-orders:80

topologySpreadConstraints:
  - maxSkew: 1
    minDomains: 3
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        app.kubernetes.io/name: ui
  - maxSkew: 1
    topologyKey: kubernetes.io/hostname
    whenUnsatisfiable: ScheduleAnyway
    labelSelector:
      matchLabels:
        app.kubernetes.io/instance: retail-store-app-ui

autoscaling:
  enabled: true
  minReplicas: 3
  maxReplicas: 10
  targetCPUUtilizationPercentage: 80
EOF

helm upgrade -f ~/environment/values-ui.yaml retail-store-app-ui oci://public.ecr.aws/aws-containers/retail-store-sample-ui-chart --version ${RETAIL_STORE_APP_HELM_CHART_VERSION} --hide-notes

The result should be similar to the following:

Pulled: public.ecr.aws/aws-containers/retail-store-sample-ui-chart:1.1.0
Digest: sha256:3862f8ecac30a8cecc0825f5a654c2a8e31871b0342ffe3b5a84b1db1e10a7dd
Release "retail-store-app-ui" has been upgraded. Happy Helming!
NAME: retail-store-app-ui
LAST DEPLOYED: Fri Jan 17 15:13:18 2025
NAMESPACE: default
STATUS: deployed
REVISION: 3
The HPA resource will maintain at least 3 replicas and will scale up to 10 replicas when the average CPU Utilization reaches 80%.

➤ Let's view the HPA resource:

kubectl get hpa  

The expected output should be similar to this (the amount of replicas may be lower while the pods are created):

NAME                  REFERENCE                        TARGETS       MINPODS   MAXPODS   REPLICAS   AGE
retail-store-app-ui   Deployment/retail-store-app-ui   cpu: 2%/80%   3         10        3          13m
Generate load
To observe HPA scale out in response to our configured policy, we need to generate load on our application. We'll do this by calling the home page of the workload with hey .

The following command will run the load generator as a Pod in the cluster with:

10 workers running concurrently
Sending 10 queries per second each
Running for a maximum of 3 minutes
➤ Apply the load:

kubectl run load-generator \
 --image=williamyeh/hey:latest \
 --restart=Never -- -c 10 -q 10 -z 3m http://retail-store-app-ui/utility/stress/100000

Now that we have requests hitting our application, we can watch the HPA resource to follow its progress.

➤ Execute the following command:

kubectl get hpa retail-store-app-ui --watch  

As the load is applied and the HPA adds new replicas, the output should become similar to:

NAME                  REFERENCE                        TARGETS       MINPODS   MAXPODS   REPLICAS   AGE
retail-store-app-ui   Deployment/retail-store-app-ui   cpu: 5%/80%   3         10        3          33m
retail-store-app-ui   Deployment/retail-store-app-ui   cpu: 6%/80%   3         10        3          33m
retail-store-app-ui   Deployment/retail-store-app-ui   cpu: 164%/80%   3         10        3          33m
retail-store-app-ui   Deployment/retail-store-app-ui   cpu: 139%/80%   3         10        6          33m
retail-store-app-ui   Deployment/retail-store-app-ui   cpu: 223%/80%   3         10        7          33m
retail-store-app-ui   Deployment/retail-store-app-ui   cpu: 258%/80%   3         10        10         34m
retail-store-app-ui   Deployment/retail-store-app-ui   cpu: 49%/80%    3         10        10         34m
retail-store-app-ui   Deployment/retail-store-app-ui   cpu: 70%/80%    3         10        10         34m
You can see in the output above how the CPU utilization gradually grows with the load and causes HPA to add more replicas of the UI component to accommodate that load.

➤ You can watch the Pods scaling by executing the following command:

watch kubectl get pods -l app.kubernetes.io/instance=retail-store-app-ui

➤ In another VS Code terminal, we can also observe how EKS Auto Mode adds more nodes to host these replicas:

watch kubectl get nodes

➤ Once we're satisfied with the autoscaling behavior – the CPU value in the TARGETS colums is contained, we can end the watch with Ctrl+C and stop the load generator:

kubectl delete pod load-generator

Once the generator is remove, you can observe the HPA removing the now-unnecessary replicas.

Summary
In this lab, we explored EKS Auto Mode's scaling capabilities through two main features: Karpenter's node-level disruption management and consolidation, and pod-level Horizontal Pod Autoscaling (HPA). We implemented and tested these features by leveraging the general-purpose NodePool's consolidation policies, setting up HPA for our UI application with specific scaling thresholds, and demonstrated automatic scaling in action using a load generator to trigger scale-out events.

In the next section, we will focus on compute customization in EKS Auto Mode. We'll explore how to optimize both performance and cost by configuring specific compute requirements for our applications using Graviton and Spot Instances.


Customization
Amazon EKS Auto Mode offers powerful compute customization capabilities that enable you to optimize both performance and cost efficiency for your applications.

In this section, we'll explore how to leverage AWS Graviton  processors for enhanced price-performance benefits and implement Spot Instances for significant cost savings.

By combining these strategies, you'll learn to configure your workloads to run on cost-effective compute resources while maintaining reliable performance.

We'll migrate application components to Graviton-based instances and implement Spot instance handling for fault-tolerant workloads, showcasing how EKS Auto Mode intelligently manages these compute resources to maximize efficiency and minimize operational costs.

Graviton
Overview | Create a Graviton NodePool | Summary

This section will guide you through the basics of EKS Auto Mode compute customization options and explain in detail customization using Graviton.

Overview
Graviton instances
AWS Graviton-based processors  deliver the best price-performance for cloud workloads and offer up to a 40% better price-performance over comparable x86-based Amazon EC2 instances. Graviton processors also use up to 60% less energy than comparable EC2 instances for the same performance. AWS Graviton-based Amazon EC2 instances provide the best price-performance for a wide variety of Linux-based workloads, such as application servers, microservices, high-performance computing (HPC), CPU-based machine learning inference, video encoding, electronic design automation (EDA), gaming, open-source databases, in-memory caches, etc.

Review the current NodePool
In this section, we will create a new, custom general purpose NodePool to provision AWS Graviton instances. Before we create the new NodePool, let's review the existing general-purpose NodePool and the current state of the nodes available in the cluster.

➤ Execute the following command:

kubectl get nodepool general-purpose -o yaml

The configuration of the general-purpose NodePool should look like this:


apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  annotations:
    karpenter.sh/nodepool-hash: "4012513481623584108"
    karpenter.sh/nodepool-hash-version: v3
  creationTimestamp: "2025-01-15T09:32:29Z"
  generation: 1
  labels:
    app.kubernetes.io/managed-by: eks
  name: general-purpose
  resourceVersion: "1097754"
  uid: 9b1c4ad0-d42d-4c63-bd96-b0a201aeec0e
spec:
  disruption:
    budgets:
    - nodes: 10%
    consolidateAfter: 30s
    consolidationPolicy: WhenEmptyOrUnderutilized
  template:
    metadata: {}
    spec:
      expireAfter: 336h
      nodeClassRef:
        group: eks.amazonaws.com
        kind: NodeClass
        name: default
      requirements:
      - key: karpenter.sh/capacity-type
        operator: In
        values:
        - on-demand
      - key: eks.amazonaws.com/instance-category
        operator: In
        values:
        - c
        - m
        - r
      - key: eks.amazonaws.com/instance-generation
        operator: Gt
        values:
        - "4"
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
      - key: kubernetes.io/os
        operator: In
        values:
        - linux
      terminationGracePeriod: 24h0m0s
status:
  conditions:
  - lastTransitionTime: "2025-01-15T09:32:43Z"
    message: ""
    observedGeneration: 1
    reason: ValidationSucceeded
    status: "True"
    type: ValidationSucceeded
  - lastTransitionTime: "2025-01-15T09:32:44Z"
    message: ""
    observedGeneration: 1
    reason: NodeClassReady
    status: "True"
    type: NodeClassReady
  - lastTransitionTime: "2025-01-15T09:32:44Z"
    message: ""
    observedGeneration: 1
    reason: Ready
    status: "True"
    type: Ready
  resources:
    cpu: "4"
    ephemeral-storage: 163708Mi
    hugepages-1Gi: "0"
    hugepages-2Mi: "0"
    memory: 7717496Ki
    nodes: "2"
    pods: "54"
➤ View the current instances processor architecture:

kubectl get nodes -L kubernetes.io/arch

with the corresponding output:

NAME                  STATUS   ROLES    AGE   VERSION               ARCH
i-07c121b110f507617   Ready    <none>   15h   v1.32.3-eks-7636447   amd64
i-0c90d33aa6fccecff   Ready    <none>   16h   v1.32.3-eks-7636447   amd64
As we can see, all current nodes are using EC2 instances with the amd64 processor architecture, as specified by the general-purpose NodePool.

Your output may vary slightly since EKS Auto Mode provisions instances according to the requirements defined in the node pool.

Create a Graviton NodePool
Now we'll create a new NodePool that includes arm64 (Graviton) architecture in the kubernetes.io/arch requirement.

This configuration enables Auto Mode managed Karpenter to evaluate each new Pod's nodeAffinity or nodeSelector for CPU architecture requirements. If needed, Karpenter will launch a new Graviton node for pending pods. We'll also add a taint with key:GravitonOnly and effect:NoSchedule to our Graviton NodePool to ensure only pods with matching tolerations are scheduled on these nodes.

➤ Create the new NodePool definition:

cat << EOF >~/environment/nodepool-graviton.yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: graviton
  labels:
    app.kubernetes.io/managed-by: app-team
spec:
  disruption:
    budgets:
    - nodes: 10%
    consolidateAfter: 30s
    consolidationPolicy: WhenEmptyOrUnderutilized
  template:
    metadata: {}
    spec:
      expireAfter: 336h
      nodeClassRef:
        group: eks.amazonaws.com
        kind: NodeClass
        name: default
      requirements:
      - key: karpenter.sh/capacity-type
        operator: In
        values:
        - on-demand
      - key: eks.amazonaws.com/instance-category
        operator: In
        values:
        - c
        - m
        - r
      - key: eks.amazonaws.com/instance-generation
        operator: Gt
        values:
        - "4"
      - key: kubernetes.io/arch
        operator: In
        values:
        - arm64
      taints:
      - effect: NoSchedule
        key: GravitonOnly
      terminationGracePeriod: 24h0m0s
  limits:
    cpu: "1000"
    memory: 1000Gi
EOF

kubectl apply -f ~/environment/nodepool-graviton.yaml

This should result in this output:

nodepool.karpenter.sh/graviton created
Run pods on Graviton
With our Graviton NodePool in place, let's configure our application's UI component to utilize it.

➤ First, let's examine the current configuration of the UI component pods:

kubectl describe pod --selector app.kubernetes.io/name=ui

This should produce an output similar to the following:

Name:             retail-store-app-ui-697bbcdb5-jf6bs
Namespace:        default
Priority:         0
Service Account:  retail-store-app-ui
Node:             i-07c121b110f507617/20.0.144.196
Start Time:       Fri, 17 Jan 2025 15:17:29 +0000
Labels:           app.kuberneres.io/owner=retail-store-sample
                  app.kubernetes.io/component=service
                  app.kubernetes.io/instance=retail-store-app
                  app.kubernetes.io/name=ui
                  pod-template-hash=697bbcdb5
Annotations:      prometheus.io/path: /actuator/prometheus
                  prometheus.io/port: 8080
                  prometheus.io/scrape: true
Status:           Running
[...]
Node-Selectors:               <none>
Tolerations:                  node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                              node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Topology Spread Constraints:  kubernetes.io/hostname:ScheduleAnyway when max skew 1 is exceeded for selector app.kubernetes.io/name=ui
                              topology.kubernetes.io/zone:ScheduleAnyway when max skew 1 is exceeded for selector app.kubernetes.io/name=ui
Events:                       <none>
The Pod is Running and has no custom tolerations configured.

Kubernetes automatically adds tolerations for node.kubernetes.io/not-ready and node.kubernetes.io/unreachable with tolerationSeconds=300 unless explicitly set. These tolerations allow Pods to remain bound to nodes for 5 minutes after detecting these issues.

Let's update our UI component to bind its pods to our Graviton NodePool.

We've tainted the NodePool with key:GravitonOnly and it automatically adds a karpenter.sh/nodepool label.

The following values-ui.yaml contains the changes needed to our UI app configuration in order to enable this setup.

Re-deploy the UI component
cat << EOF >~/environment/values-ui.yaml
app:
  theme: default
  endpoints: 
    catalog: http://retail-store-app-catalog:80
    carts: http://retail-store-app-carts:80
    checkout: http://retail-store-app-checkout:80
    orders: http://retail-store-app-orders:80

topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: ScheduleAnyway
    labelSelector:
      matchLabels:
        app.kubernetes.io/name: ui

autoscaling:
  enabled: false
  minReplicas: 3
  maxReplicas: 10
  targetCPUUtilizationPercentage: 80

nodeSelector:
  karpenter.sh/nodepool: graviton
tolerations:
- key: "GravitonOnly"
  operator: "Exists"
EOF

helm upgrade -f ~/environment/values-ui.yaml retail-store-app-ui oci://public.ecr.aws/aws-containers/retail-store-sample-ui-chart --version ${RETAIL_STORE_APP_HELM_CHART_VERSION} --hide-notes

Note that the UI component will revert to its original configuration in the values-ui.yaml file, which specified a single replica.

This should produce the following output:

Pulled: public.ecr.aws/aws-containers/retail-store-sample-ui-chart:1.1.0
Digest: sha256:5cd721c10214c306b06c7223367f626f21a8d471eee8f0a576742426f84141f2
Release "retail-store-app-ui" has been upgraded. Happy Helming!
NAME: retail-store-app-ui
LAST DEPLOYED: Sat Jan 18 23:54:22 2025
NAMESPACE: default
STATUS: deployed
REVISION: 4
➤ Before examining the new Graviton nodes, ensure all UI component pods are ready:

kubectl wait --for=condition=Ready pod -l app.kubernetes.io/instance=retail-store-app-ui --namespace default --timeout=300s

➤ Now check the status of our EKS cluster nodes and UI component pods:

kubectl get nodes -L kubernetes.io/arch -L karpenter.sh/nodepool
kubectl get pods -l app.kubernetes.io/name=ui -o wide

With the expected output containing arm64 instances, similar to below:

NAME                  STATUS   ROLES    AGE    VERSION               ARCH    NODEPOOL
i-078b82d5fe991368d   Ready    <none>   45s    v1.32.5-eks-98436be   arm64   system
i-0b23214bab198a3f1   Ready    <none>   105m   v1.32.5-eks-98436be   amd64   general-purpose
i-0f67fef87c747070d   Ready    <none>   104s   v1.32.5-eks-98436be   arm64   graviton
NAME                                   READY   STATUS    RESTARTS   AGE    IP                NODE
retail-store-app-ui-86df66db68-744sn   1/1     Running   0          2m7s   192.168.190.160   i-0f67fef87c747070d
As you can see, the UI component pods are now running on the Graviton NodePool. You can also see the taint on the node using the kubectl describe node command and the matching tolerations on the pods using the kubectl describe pod command.

Summary
In this lab, we explored using AWS Graviton instances in EKS Auto Mode for improved performance and cost efficiency.

We created a dedicated Graviton NodePool configured for arm64 architecture instances with a GravitonOnly taint to control Pod scheduling. We then modified our application by updating the UI component's configuration with the necessary node selector and toleration to enable running on Graviton instances.

In the next lab we will explore combining On-Demand instances with Spot Instances for additional cost optimization.
