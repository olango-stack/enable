Enabling EKS Auto Mode with Terraform  requires setting compute_config.enabled, kubernetes_network_config.elastic_load_balancing.enabled, and storage_config.block_storage.enabled to true, as well as bootstrap_self_managed_addons to false.
Let's execute the following command to import the cluster IAM role to the Terraform state:

terraform import aws_iam_role.cluster_role $DEMO_CLUSTER_ROLE_NAME

Since the cluster has already been created for us, we'll have to modify the cluster IAM role trust policy per the EKS Auto Mode documentation . That means that before planning and applying the resources, we should import the AWS IAM role to the Terraform state.


NAME                                         CREATED AT
cninodes.eks.amazonaws.com                   2024-12-04T13:03:50Z
cninodes.vpcresources.k8s.aws                2024-12-04T13:00:46Z
ingressclassparams.eks.amazonaws.com         2024-12-04T13:03:49Z
nodeclaims.karpenter.sh                      2024-12-04T13:03:39Z
nodeclasses.eks.amazonaws.com                2024-12-04T13:03:39Z
nodediagnostics.eks.amazonaws.com            2024-12-04T13:03:39Z
nodepools.karpenter.sh                       2024-12-04T13:03:39Z
policyendpoints.networking.k8s.aws           2024-12-04T13:00:46Z
securitygrouppolicies.vpcresources.k8s.aws   2024-12-04T13:00:46Z
targetgroupbindings.eks.amazonaws.com        2024-12-04T13:03:49Z


These CRDs enable several crucial capabilities of EKS Auto Mode:

nodepools support provisioning compute
ingressclassparams and targetgroupbinding allow exposing applications, and
nodediagnostics provide diagnostics capabilities

To explore these capabilities, we will initially deploy the application in a manner that is self-contained in the Amazon EKS cluster, without using any AWS services that provision load balancers or managed databases. 









cat << EOF > ~/environment/values-ui.yaml

app:
  theme: default
  endpoints:
    catalog: http://retail-store-app-catalog:80
    carts: http://retail-store-app-carts:80
    checkout: http://retail-store-app-checkout:80
    orders: http://retail-store-app-orders:80
EOF

helm upgrade -i retail-store-app-catalog oci://public.ecr.aws/aws-containers/retail-store-sample-catalog-chart --version ${RETAIL_STORE_APP_HELM_CHART_VERSION} --hide-notes
helm upgrade -i retail-store-app-orders oci://public.ecr.aws/aws-containers/retail-store-sample-orders-chart --version ${RETAIL_STORE_APP_HELM_CHART_VERSION} --hide-notes
helm upgrade -i retail-store-app-carts oci://public.ecr.aws/aws-containers/retail-store-sample-cart-chart --version ${RETAIL_STORE_APP_HELM_CHART_VERSION} --hide-notes
helm upgrade -i retail-store-app-checkout oci://public.ecr.aws/aws-containers/retail-store-sample-checkout-chart --version ${RETAIL_STORE_APP_HELM_CHART_VERSION} --hide-notes
helm upgrade -i retail-store-app-ui oci://public.ecr.aws/aws-containers/retail-store-sample-ui-chart --version ${RETAIL_STORE_APP_HELM_CHART_VERSION} -f ~/environment/values-ui.yaml --hide-notes


The commands above should produce helm outputs for each component of the app (catalog, orders, cart, checkout, and ui) similar to the following output:

Amazon EKS Auto Mode will evaluate the resource requirements of these Pods and determine the optimum compute to launch for our applications to run, considering any scheduling constraints configured.

These Services are internal to the cluster, so we cannot access them from the Internet or even the VPC. However, we can use port-forwarding  to access an existing Pod in the EKS cluster to check that the application UI is working.

kubectl port-forward $(kubectl get pods \
 --selector=app.kubernetes.io/name=ui -o jsonpath='{.items[0].metadata.name}') 8080:8080




 ## Environment Variables

All environment variables used in this project are listed in `.env.example`.  
Copy it to `.env` and fill in the real values:

```bash
cp .env.example .env


Node Launch

Event ends in 19 minutes.

Event dashboard
...
Compute
Node Launch
Event dashboard
Module 2 - Explore EKS Auto Mode capabilities & fundamentals
Compute
Node Launch
Node Launch
Overview | Review the EKS Auto Mode configuration | Scale the application | Improve the application resilience | Summary

Overview
When using EKS Auto Mode, cluster compute resources are automatically provisioned and managed by EKS. The service automates routine tasks for creating new EC2 instances and registering them as nodes to your EKS cluster. When a workload cannot be scheduled onto existing nodes, EKS Auto Mode creates a new, appropriately sized EC2 instance.

EC2 instances created by EKS Auto Mode are EC2 managed instances . These provide a simplified way to run compute workloads on Amazon EC2 by delegating operational control to a service provider - in this case, EKS Auto Mode.

By delegating control to EKS Auto Mode, you benefit from AWS's operational expertise and best practices for running Amazon EKS. EKS handles tasks such as provisioning instances, configuring software, scaling capacity, and managing instance failures and replacements. While you maintain visibility of the managed instances through the AWS console and can use instance storage as ephemeral storage for workloads, direct access and software installation on these instances is not permitted. You can review the list of supported instance types here  and see a detailed comparison  between standard EC2 instances and EKS Auto Mode managed instances.



Review the EKS Auto Mode configuration
EKS Auto Mode's node provisioning relies on Karpenter  objects, with dedicated specifications for NodeClasses  and NodePools .

The NodeClass specification allows to define infrastructure-level settings for groups of nodes, including network configuration, storage settings, and resource tagging.

The NodePool specification enables fine-grained control over compute resources through various supported labels and compute requirements configuration. This includes options for EC2 instance categories, CPU configurations, availability zones, architectures (ARM64/AMD64), and capacity types (spot/on-demand). You can also set resource limits for CPU and memory usage to maintain desired operational boundaries.

EKS Auto Mode includes two default managed node pools: general-purpose and system. The general-purpose node pool handles user-deployed applications and services, while the system node pool is dedicated to critical system-level components managing cluster operations. Custom node pools can be created for specific compute or configuration requirements.

Let's explore the built-in node pools and their managed instances.


Review node pools configuration
➤ View the managed instances in the general-purpose node pool:

kubectl get nodes -l karpenter.sh/nodepool=general-purpose

The output should show a single managed instance:

NAME                  STATUS   ROLES    AGE   VERSION
i-05e5427f9dc2f4b2e   Ready    <none>   31m   v1.32.3-eks-7636447
➤ View the managed instances in the system node pool:

kubectl get nodes -l karpenter.sh/nodepool=system

his should produce an empty result:

No resources found
Note: The system node pool is currently empty as no system components have been installed. All application pods are running on the general-purpose node pool.

➤ We can verify this by checking the Pod distribution across the general-purpose nodes:

for node in $(kubectl get nodes -l karpenter.sh/nodepool=general-purpose -o custom-columns=NAME:.metadata.name --no-headers); do
  echo "Pods on $node:"
  kubectl get pods --all-namespaces --field-selector spec.nodeName=$node
done

The expected output should be similar to the following:

Pods on i-0ee0842e974bafc6c:
NAMESPACE   NAME                                        READY   STATUS    RESTARTS   AGE
default     retail-store-app-carts-5f5b7449f-tgscs      1/1     Running   0          8m3s
default     retail-store-app-catalog-dcb5d8d4c-5ftp9    1/1     Running   0          8m6s
default     retail-store-app-checkout-f5bb5c5bb-pkgg7   1/1     Running   0          8m2s
default     retail-store-app-orders-5fbb6b8576-x6vs9    1/1     Running   0          8m5s
default     retail-store-app-ui-7b7c8f6b94-2ltrs        1/1     Running   0          8m
Currently, all pods are running on a single node since it adequately meets the workloads' resource requirements without any specific scheduling constraints.


Scale the application
Let's manually scale the application UI component to observe how Auto Mode automatically provisions new nodes to meet the increased demand.

➤ In a separate IDE terminal, watch for new nodes:

watch kubectl get nodes

This command will continuously monitor and display changes to cluster nodes. To exit, press Ctrl/Cmd+C in the terminal.

➤ Scale the UI component:

kubectl scale --replicas=12 deployment/retail-store-app-ui

This should produce this output:

deployment.apps/retail-store-app-ui scaled
The watch command output should eventually show new nodes (initially with NotReady status) being added to accommodate the additional UI component replicas:

Every 2.0s: kubectl get nodes                                                                ip-192-168-0-253.us-west-2.compute.internal: Tue Feb 25 21:23:39 2025

NAME                  STATUS   ROLES    AGE   VERSION
i-00642042ad4eff3c9   Ready    <none>   17s   v1.30.8-eks-3c20087
i-009db40705eb0417d   Ready    <none>   10m   v1.30.8-eks-3c20087
➤ To see how EKS Auto Mode responded to the scaling event, examine the cluster events:

kubectl events

The output will include events related to pods, NodePools, and Nodes.

Expand to view the events
➤ Let's examine the current Pod distribution:

for node in $(kubectl get nodes -l karpenter.sh/nodepool=general-purpose -o custom-columns=NAME:.metadata.name --no-headers); do
  echo "Pods on $node:"
  kubectl get pods --all-namespaces --field-selector spec.nodeName=$node
done

The command should produce an output similar to the following:

Pods on i-092974f55c2956b90:
NAMESPACE   NAME                                   READY   STATUS    RESTARTS   AGE
default     retail-store-app-ui-7b7c8f6b94-4vdfm   1/1     Running   0          3m13s
default     retail-store-app-ui-7b7c8f6b94-8tgkg   1/1     Running   0          3m14s
default     retail-store-app-ui-7b7c8f6b94-8xcvs   1/1     Running   0          3m14s
default     retail-store-app-ui-7b7c8f6b94-bhlt8   1/1     Running   0          3m13s
default     retail-store-app-ui-7b7c8f6b94-c8rfs   1/1     Running   0          3m14s
default     retail-store-app-ui-7b7c8f6b94-jjlmd   1/1     Running   0          3m14s
default     retail-store-app-ui-7b7c8f6b94-p8frf   1/1     Running   0          3m13s
default     retail-store-app-ui-7b7c8f6b94-qnqrn   1/1     Running   0          3m13s
default     retail-store-app-ui-7b7c8f6b94-xdxqc   1/1     Running   0          3m14s
default     retail-store-app-ui-7b7c8f6b94-xsqds   1/1     Running   0          3m14s
Pods on i-0ee0842e974bafc6c:
NAMESPACE   NAME                                        READY   STATUS    RESTARTS   AGE
default     retail-store-app-carts-5f5b7449f-tgscs      1/1     Running   0          12m
default     retail-store-app-catalog-dcb5d8d4c-5ftp9    1/1     Running   0          12m
default     retail-store-app-checkout-f5bb5c5bb-pkgg7   1/1     Running   0          12m
default     retail-store-app-orders-5fbb6b8576-x6vs9    1/1     Running   0          12m
default     retail-store-app-ui-7b7c8f6b94-2ltrs        1/1     Running   0          12m
default     retail-store-app-ui-7b7c8f6b94-4hwth        1/1     Running   0          3m15s
Note that the pods are currently densely packed and scheduled onto a single node, which reduces resilience. In the next step, we'll explore how to improve that using EKS Auto Mode capabilities.

Improve the application resilience
Topology spread constraints
To enhance application resiliency, it's crucial to distribute pods across multiple AZs and nodes. We'll apply Pod Topology Spread Constraints  to our UI component to ensure better fault tolerance and distribution.

Update the UI component
➤ Update the values-ui.yaml file and re-deploy the Helm chart:

cat  << EOF >~/environment/values-ui.yaml
app:
  theme: default
  endpoints: 
    catalog: http://retail-store-app-catalog:80
    carts: http://retail-store-app-carts:80
    checkout: http://retail-store-app-checkout:80
    orders: http://retail-store-app-orders:80

topologySpreadConstraints:
  - maxSkew: 1
    minDomains: 3
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        app.kubernetes.io/name: ui
EOF

helm upgrade -f ~/environment/values-ui.yaml retail-store-app-ui oci://public.ecr.aws/aws-containers/retail-store-sample-ui-chart --version ${RETAIL_STORE_APP_HELM_CHART_VERSION} --hide-notes

The output should be similar to:

Pulled: public.ecr.aws/aws-containers/retail-store-sample-ui-chart:1.1.0
Digest: sha256:5cd721c10214c306b06c7223367f626f21a8d471eee8f0a576742426f84141f2
Release "retail-store-app-ui" has been upgraded. Happy Helming!
NAME: retail-store-app-ui
LAST DEPLOYED: Mon Jan 20 20:33:32 2025
NAMESPACE: default
STATUS: deployed
REVISION: 2
The configuration specifies a maximum skew of 1 and requires a minimum of 3 AZs, ensuring that the difference in pod count between any two zones does not exceed 1. When the constraints cannot be satisfied, pods will not be scheduled (DoNotSchedule), prioritizing proper distribution over immediate deployment.

Note
The manually-scaled UI component will automatically reset to the default number of replicas (which is 1) after we deploy the updated configuration.

➤ Now, let's scale the application again to see how Pod spread topology impacts the scaling behavior:

kubectl scale --replicas=12 deployment/retail-store-app-ui

As before, this is the expected output:

deployment.apps/retail-store-app-ui scaled
➤ Wait for all the scaled UI component pods to become ready:

kubectl wait --for=condition=Ready pod -l app.kubernetes.io/instance=retail-store-app-ui --namespace default --timeout=300s

➤ Now we can verify that the UI component pods are spread across different availability zones:

kubectl get node -L topology.kubernetes.io/zone --no-headers | while read node status roles age version zone; do
echo "Pods on node $node (Zone: $zone):"
  kubectl get pods --all-namespaces --field-selector spec.nodeName=$node -l app.kubernetes.io/instance=retail-store-app-ui
echo "-----------------------------------"
done

This should produce an output similar to the following:

Pods on node i-08104c0f996d4db79 (Zone: us-west-2a):
NAMESPACE   NAME                                   READY   STATUS    RESTARTS   AGE
default     retail-store-app-ui-7fbf6d97b9-d9tqs   1/1     Running   0          2m1s
default     retail-store-app-ui-7fbf6d97b9-ghkbm   1/1     Running   0          2m1s
default     retail-store-app-ui-7fbf6d97b9-lm6fp   1/1     Running   0          2m1s
default     retail-store-app-ui-7fbf6d97b9-t9pbl   1/1     Running   0          2m27s
-----------------------------------
Pods on node i-09301d4c5dea017fb (Zone: us-west-2c):
NAMESPACE   NAME                                   READY   STATUS    RESTARTS   AGE
default     retail-store-app-ui-7fbf6d97b9-9bncz   1/1     Running   0          2m2s
default     retail-store-app-ui-7fbf6d97b9-wm76b   1/1     Running   0          2m2s
default     retail-store-app-ui-7fbf6d97b9-zqg2z   1/1     Running   0          2m2s
default     retail-store-app-ui-7fbf6d97b9-zwfw7   1/1     Running   0          40s
-----------------------------------
Pods on node i-0ed7e4a8df8557487 (Zone: us-west-2b):
NAMESPACE   NAME                                   READY   STATUS    RESTARTS   AGE
default     retail-store-app-ui-7fbf6d97b9-4vlzz   1/1     Running   0          2m3s
default     retail-store-app-ui-7fbf6d97b9-847q2   1/1     Running   0          2m3s
default     retail-store-app-ui-7fbf6d97b9-f42hz   1/1     Running   0          2m3s
default     retail-store-app-ui-7fbf6d97b9-mkn7q   1/1     Running   0          2m3s
-----------------------------------
The Pod topology spread constraints have successfully distributed the workload across multiple AZs and nodes, ensuring high availability and fault tolerance.

Summary
In this section, we've examined EKS Auto Mode's built-in system and general-purpose node pools, demonstrated manual scaling of the application, and improved its resilience by implementing AZ-level topology spread constraints for the UI component.

In the next section, we'll explore automated scaling policies for the UI application to replace manual scaling operations. We'll examine how EKS Auto Mode manages dynamic resource allocation and workload distribution in response to these policies.

Autoscaling
Overview | Horizontal Pod Autoscaling | Summary

This section will guide you through the basics of EKS Auto Mode and explore, in details, application and cluster autoscaling.

Overview
Disruptions in EKS Auto Mode
Before we dive into autoscaling, let's first understand how disruptions  are managed in EKS Auto Mode. Disruptions can occur in situations such as when nodes are scaled down to reduce cluster costs (like bin-packing), or hit their maximum lifetime (expiry date). This potentially impacts the running pods on those nodes. EKS Auto Mode uses Karpenter under the hood to optimize node scaling and manage these disruptions effectively.

Karpenter manages node disruptions through three key mechanisms: expiration, drift detection, and consolidation, with the latter being the focus of this section, as more relevant to autoscaling.

Consolidation
Consolidation works by continuously monitoring the utilization of nodes and pods in the cluster. When nodes become underutilized or idle, Karpenter initiates a consolidation process to optimize cluster resources. This involves removing nodes without active workloads, efficiently bin-packing pods onto existing nodes where capacity permits, and performing graceful node draining by carefully evicting and rescheduling pods to maintain application availability throughout the consolidation process.

Below you can see the highlighted configuration of the disruption block in the provided general-purpose NodePool that is created and managed by EKS Auto Mode

➤ Get the NodePool configuration:

kubectl get nodepools general-purpose -o yaml

This should show the following output:

apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  annotations:
    karpenter.sh/nodepool-hash: "4012513481623584108"
    karpenter.sh/nodepool-hash-version: v3
  creationTimestamp: "2025-01-15T09:32:29Z"
  generation: 1
  labels:
    app.kubernetes.io/managed-by: eks
  name: general-purpose
  resourceVersion: "241001"
  uid: 9b1c4ad0-d42d-4c63-bd96-b0a201aeec0e
spec:
  disruption:
    budgets:
    - nodes: 10%
    consolidateAfter: 30s
    consolidationPolicy: WhenEmptyOrUnderutilized
  template:
    metadata: {}
    spec:
      expireAfter: 336h
      nodeClassRef:
        group: eks.amazonaws.com
        kind: NodeClass
        name: default
      requirements:
      - key: karpenter.sh/capacity-type
        operator: In
        values:
        - on-demand
      - key: eks.amazonaws.com/instance-category
        operator: In
        values:
        - c
        - m
        - r
      - key: eks.amazonaws.com/instance-generation
        operator: Gt
        values:
        - "4"
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
      - key: kubernetes.io/os
        operator: In
        values:
        - linux
      terminationGracePeriod: 24h0m0s
The configuration property WhenEmptyOrUnderutilized ensures that Karpenter will consider all nodes for consolidation and attempt to remove or replace nodes when it discovers that a node is empty or under-utilized and could be removed or replaced to reduce cost. expireAfter is set to a custom value so that nodes are terminated automatically after 336 hours (14 days). The budget configuration block control the speed Karpenter can scale down nodes.

Horizontal Pod Autoscaling
In this lab, we'll explore how the Horizontal Pod Autoscaler (HPA) automatically scales pods in Kubernetes based on observed metrics. The HPA consists of a resource definition and a controller that periodically checks resource utilization (such as CPU, memory, or custom metrics) against user-defined targets and adjusts the number of replicas accordingly.

The Metrics Server component is essential for collecting and aggregating resource usage data from the cluster and providing HPA with the necessary metrics to make scaling decisions.

Install Metrics Server
To enable application-level autoscaling in EKS Auto Mode, we need to install the Metrics Server. AWS offers a streamlined installation process through Community Add-ons  to simplify deployment and management.

We can create an Amazon EKS add-on using eksctl, the AWS Management Console, or the AWS CLI. For add-ons requiring an IAM role, refer to Available Amazon EKS add-ons from AWS  for details about creating the role.

Create add-on (eksctl)
➤ Use eksctl to install the metrics-server addon:

eksctl create addon --name metrics-server --cluster ${DEMO_CLUSTER_NAME}

Creating the metrics-server add-on should take about 1 minute.

The output should look similar to the following:

2025-01-16 00:06:09 [ℹ]  Kubernetes version "1.32" in use by cluster "demo-cluster"
2025-01-16 00:06:09 [ℹ]  creating addon
2025-01-16 00:07:00 [ℹ]  addon "metrics-server" active
➤ After the above installation commands completes, let's confirm that the Metrics Server is running:

kubectl get deployment metrics-server -n kube-system

with the expected output:

NAME             READY   UP-TO-DATE   AVAILABLE   AGE
metrics-server   2/2     2            2           99s
To get a view of the metrics that HPA will use to drive its scaling behavior, use the kubectl top command.

➤ For example, the below commands will show the resource utilization of the nodes and UI component pods in our cluster:

kubectl top node
kubectl top pods -l app.kubernetes.io/name=ui

The result should be similar to the following:

NAME                  CPU(cores)   CPU(%)   MEMORY(bytes)   MEMORY(%)
i-010ef666810cc74fe   20m          1%       589Mi           18%
i-014b2a0091dc49a40   19m          0%       586Mi           18%
i-0234821e6c1789b57   44m          2%       596Mi           19%
i-04eb412b3e09844ef   39m          2%       595Mi           19%
i-066e1d9c08f13c0ed   28m          1%       1670Mi          53%
i-06d0fd146733a2985   40m          2%       599Mi           19%
i-07c121b110f507617   45m          2%       597Mi           19%
i-0b87fcb18c6146df8   65m          3%       598Mi           19%
i-0c1cfed844ac873ea   36m          1%       1268Mi          40%
i-0db78db542f9b25da   55m          2%       603Mi           19%
i-0ed869e02ea7e95e4   11m          0%       599Mi           19%
i-0f8137d7076ba89f1   20m          1%       686Mi           22%
NAME                                  CPU(cores)   MEMORY(bytes)
retail-store-app-ui-697bbcdb5-2rcbq   1m           221Mi
retail-store-app-ui-697bbcdb5-hxgqp   1m           212Mi
retail-store-app-ui-697bbcdb5-jfdc6   1m           217Mi
retail-store-app-ui-697bbcdb5-jrtdm   2m           215Mi
retail-store-app-ui-697bbcdb5-kv79b   1m           221Mi
retail-store-app-ui-697bbcdb5-p99gf   1m           214Mi
retail-store-app-ui-697bbcdb5-qwpdb   1m           222Mi
retail-store-app-ui-697bbcdb5-sx4sp   2m           217Mi
retail-store-app-ui-697bbcdb5-wczzw   2m           226Mi
retail-store-app-ui-697bbcdb5-whxxf   2m           217Mi
retail-store-app-ui-697bbcdb5-zn667   1m           211Mi
retail-store-app-ui-697bbcdb5-zwhw8   1m           217Mi
Configure HPA
Currently, there are no resources in our cluster that enable Horizontal Pod Autoscaling.

➤ Let's verify this by executing:

kubectl get hpa --all-namespaces

with the expected output:

No resources found
Now, we'll add autoscaling configurations to the UI component based on its CPU usage by updating our values-ui.yaml file with autoscaling config and re-deploying the UI component using the corresponding Helm chart.

Configure and re-deploy the UI component
cat << EOF >~/environment/values-ui.yaml
app:
  theme: default
  endpoints: 
    catalog: http://retail-store-app-catalog:80
    carts: http://retail-store-app-carts:80
    checkout: http://retail-store-app-checkout:80
    orders: http://retail-store-app-orders:80

topologySpreadConstraints:
  - maxSkew: 1
    minDomains: 3
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        app.kubernetes.io/name: ui
  - maxSkew: 1
    topologyKey: kubernetes.io/hostname
    whenUnsatisfiable: ScheduleAnyway
    labelSelector:
      matchLabels:
        app.kubernetes.io/instance: retail-store-app-ui

autoscaling:
  enabled: true
  minReplicas: 3
  maxReplicas: 10
  targetCPUUtilizationPercentage: 80
EOF

helm upgrade -f ~/environment/values-ui.yaml retail-store-app-ui oci://public.ecr.aws/aws-containers/retail-store-sample-ui-chart --version ${RETAIL_STORE_APP_HELM_CHART_VERSION} --hide-notes

The result should be similar to the following:

Pulled: public.ecr.aws/aws-containers/retail-store-sample-ui-chart:1.1.0
Digest: sha256:3862f8ecac30a8cecc0825f5a654c2a8e31871b0342ffe3b5a84b1db1e10a7dd
Release "retail-store-app-ui" has been upgraded. Happy Helming!
NAME: retail-store-app-ui
LAST DEPLOYED: Fri Jan 17 15:13:18 2025
NAMESPACE: default
STATUS: deployed
REVISION: 3
The HPA resource will maintain at least 3 replicas and will scale up to 10 replicas when the average CPU Utilization reaches 80%.

➤ Let's view the HPA resource:

kubectl get hpa  

The expected output should be similar to this (the amount of replicas may be lower while the pods are created):

NAME                  REFERENCE                        TARGETS       MINPODS   MAXPODS   REPLICAS   AGE
retail-store-app-ui   Deployment/retail-store-app-ui   cpu: 2%/80%   3         10        3          13m
Generate load
To observe HPA scale out in response to our configured policy, we need to generate load on our application. We'll do this by calling the home page of the workload with hey .

The following command will run the load generator as a Pod in the cluster with:

10 workers running concurrently
Sending 10 queries per second each
Running for a maximum of 3 minutes
➤ Apply the load:

kubectl run load-generator \
 --image=williamyeh/hey:latest \
 --restart=Never -- -c 10 -q 10 -z 3m http://retail-store-app-ui/utility/stress/100000

Now that we have requests hitting our application, we can watch the HPA resource to follow its progress.

➤ Execute the following command:

kubectl get hpa retail-store-app-ui --watch  

As the load is applied and the HPA adds new replicas, the output should become similar to:

NAME                  REFERENCE                        TARGETS       MINPODS   MAXPODS   REPLICAS   AGE
retail-store-app-ui   Deployment/retail-store-app-ui   cpu: 5%/80%   3         10        3          33m
retail-store-app-ui   Deployment/retail-store-app-ui   cpu: 6%/80%   3         10        3          33m
retail-store-app-ui   Deployment/retail-store-app-ui   cpu: 164%/80%   3         10        3          33m
retail-store-app-ui   Deployment/retail-store-app-ui   cpu: 139%/80%   3         10        6          33m
retail-store-app-ui   Deployment/retail-store-app-ui   cpu: 223%/80%   3         10        7          33m
retail-store-app-ui   Deployment/retail-store-app-ui   cpu: 258%/80%   3         10        10         34m
retail-store-app-ui   Deployment/retail-store-app-ui   cpu: 49%/80%    3         10        10         34m
retail-store-app-ui   Deployment/retail-store-app-ui   cpu: 70%/80%    3         10        10         34m
You can see in the output above how the CPU utilization gradually grows with the load and causes HPA to add more replicas of the UI component to accommodate that load.

➤ You can watch the Pods scaling by executing the following command:

watch kubectl get pods -l app.kubernetes.io/instance=retail-store-app-ui

➤ In another VS Code terminal, we can also observe how EKS Auto Mode adds more nodes to host these replicas:

watch kubectl get nodes

➤ Once we're satisfied with the autoscaling behavior – the CPU value in the TARGETS colums is contained, we can end the watch with Ctrl+C and stop the load generator:

kubectl delete pod load-generator

Once the generator is remove, you can observe the HPA removing the now-unnecessary replicas.

Summary
In this lab, we explored EKS Auto Mode's scaling capabilities through two main features: Karpenter's node-level disruption management and consolidation, and pod-level Horizontal Pod Autoscaling (HPA). We implemented and tested these features by leveraging the general-purpose NodePool's consolidation policies, setting up HPA for our UI application with specific scaling thresholds, and demonstrated automatic scaling in action using a load generator to trigger scale-out events.

In the next section, we will focus on compute customization in EKS Auto Mode. We'll explore how to optimize both performance and cost by configuring specific compute requirements for our applications using Graviton and Spot Instances.


Customization
Amazon EKS Auto Mode offers powerful compute customization capabilities that enable you to optimize both performance and cost efficiency for your applications.

In this section, we'll explore how to leverage AWS Graviton  processors for enhanced price-performance benefits and implement Spot Instances for significant cost savings.

By combining these strategies, you'll learn to configure your workloads to run on cost-effective compute resources while maintaining reliable performance.

We'll migrate application components to Graviton-based instances and implement Spot instance handling for fault-tolerant workloads, showcasing how EKS Auto Mode intelligently manages these compute resources to maximize efficiency and minimize operational costs.

Graviton
Overview | Create a Graviton NodePool | Summary

This section will guide you through the basics of EKS Auto Mode compute customization options and explain in detail customization using Graviton.

Overview
Graviton instances
AWS Graviton-based processors  deliver the best price-performance for cloud workloads and offer up to a 40% better price-performance over comparable x86-based Amazon EC2 instances. Graviton processors also use up to 60% less energy than comparable EC2 instances for the same performance. AWS Graviton-based Amazon EC2 instances provide the best price-performance for a wide variety of Linux-based workloads, such as application servers, microservices, high-performance computing (HPC), CPU-based machine learning inference, video encoding, electronic design automation (EDA), gaming, open-source databases, in-memory caches, etc.

Review the current NodePool
In this section, we will create a new, custom general purpose NodePool to provision AWS Graviton instances. Before we create the new NodePool, let's review the existing general-purpose NodePool and the current state of the nodes available in the cluster.

➤ Execute the following command:

kubectl get nodepool general-purpose -o yaml

The configuration of the general-purpose NodePool should look like this:


apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  annotations:
    karpenter.sh/nodepool-hash: "4012513481623584108"
    karpenter.sh/nodepool-hash-version: v3
  creationTimestamp: "2025-01-15T09:32:29Z"
  generation: 1
  labels:
    app.kubernetes.io/managed-by: eks
  name: general-purpose
  resourceVersion: "1097754"
  uid: 9b1c4ad0-d42d-4c63-bd96-b0a201aeec0e
spec:
  disruption:
    budgets:
    - nodes: 10%
    consolidateAfter: 30s
    consolidationPolicy: WhenEmptyOrUnderutilized
  template:
    metadata: {}
    spec:
      expireAfter: 336h
      nodeClassRef:
        group: eks.amazonaws.com
        kind: NodeClass
        name: default
      requirements:
      - key: karpenter.sh/capacity-type
        operator: In
        values:
        - on-demand
      - key: eks.amazonaws.com/instance-category
        operator: In
        values:
        - c
        - m
        - r
      - key: eks.amazonaws.com/instance-generation
        operator: Gt
        values:
        - "4"
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
      - key: kubernetes.io/os
        operator: In
        values:
        - linux
      terminationGracePeriod: 24h0m0s
status:
  conditions:
  - lastTransitionTime: "2025-01-15T09:32:43Z"
    message: ""
    observedGeneration: 1
    reason: ValidationSucceeded
    status: "True"
    type: ValidationSucceeded
  - lastTransitionTime: "2025-01-15T09:32:44Z"
    message: ""
    observedGeneration: 1
    reason: NodeClassReady
    status: "True"
    type: NodeClassReady
  - lastTransitionTime: "2025-01-15T09:32:44Z"
    message: ""
    observedGeneration: 1
    reason: Ready
    status: "True"
    type: Ready
  resources:
    cpu: "4"
    ephemeral-storage: 163708Mi
    hugepages-1Gi: "0"
    hugepages-2Mi: "0"
    memory: 7717496Ki
    nodes: "2"
    pods: "54"
➤ View the current instances processor architecture:

kubectl get nodes -L kubernetes.io/arch

with the corresponding output:

NAME                  STATUS   ROLES    AGE   VERSION               ARCH
i-07c121b110f507617   Ready    <none>   15h   v1.32.3-eks-7636447   amd64
i-0c90d33aa6fccecff   Ready    <none>   16h   v1.32.3-eks-7636447   amd64
As we can see, all current nodes are using EC2 instances with the amd64 processor architecture, as specified by the general-purpose NodePool.

Your output may vary slightly since EKS Auto Mode provisions instances according to the requirements defined in the node pool.

Create a Graviton NodePool
Now we'll create a new NodePool that includes arm64 (Graviton) architecture in the kubernetes.io/arch requirement.

This configuration enables Auto Mode managed Karpenter to evaluate each new Pod's nodeAffinity or nodeSelector for CPU architecture requirements. If needed, Karpenter will launch a new Graviton node for pending pods. We'll also add a taint with key:GravitonOnly and effect:NoSchedule to our Graviton NodePool to ensure only pods with matching tolerations are scheduled on these nodes.

➤ Create the new NodePool definition:

cat << EOF >~/environment/nodepool-graviton.yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: graviton
  labels:
    app.kubernetes.io/managed-by: app-team
spec:
  disruption:
    budgets:
    - nodes: 10%
    consolidateAfter: 30s
    consolidationPolicy: WhenEmptyOrUnderutilized
  template:
    metadata: {}
    spec:
      expireAfter: 336h
      nodeClassRef:
        group: eks.amazonaws.com
        kind: NodeClass
        name: default
      requirements:
      - key: karpenter.sh/capacity-type
        operator: In
        values:
        - on-demand
      - key: eks.amazonaws.com/instance-category
        operator: In
        values:
        - c
        - m
        - r
      - key: eks.amazonaws.com/instance-generation
        operator: Gt
        values:
        - "4"
      - key: kubernetes.io/arch
        operator: In
        values:
        - arm64
      taints:
      - effect: NoSchedule
        key: GravitonOnly
      terminationGracePeriod: 24h0m0s
  limits:
    cpu: "1000"
    memory: 1000Gi
EOF

kubectl apply -f ~/environment/nodepool-graviton.yaml

This should result in this output:

nodepool.karpenter.sh/graviton created
Run pods on Graviton
With our Graviton NodePool in place, let's configure our application's UI component to utilize it.

➤ First, let's examine the current configuration of the UI component pods:

kubectl describe pod --selector app.kubernetes.io/name=ui

This should produce an output similar to the following:

Name:             retail-store-app-ui-697bbcdb5-jf6bs
Namespace:        default
Priority:         0
Service Account:  retail-store-app-ui
Node:             i-07c121b110f507617/20.0.144.196
Start Time:       Fri, 17 Jan 2025 15:17:29 +0000
Labels:           app.kuberneres.io/owner=retail-store-sample
                  app.kubernetes.io/component=service
                  app.kubernetes.io/instance=retail-store-app
                  app.kubernetes.io/name=ui
                  pod-template-hash=697bbcdb5
Annotations:      prometheus.io/path: /actuator/prometheus
                  prometheus.io/port: 8080
                  prometheus.io/scrape: true
Status:           Running
[...]
Node-Selectors:               <none>
Tolerations:                  node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                              node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Topology Spread Constraints:  kubernetes.io/hostname:ScheduleAnyway when max skew 1 is exceeded for selector app.kubernetes.io/name=ui
                              topology.kubernetes.io/zone:ScheduleAnyway when max skew 1 is exceeded for selector app.kubernetes.io/name=ui
Events:                       <none>
The Pod is Running and has no custom tolerations configured.

Kubernetes automatically adds tolerations for node.kubernetes.io/not-ready and node.kubernetes.io/unreachable with tolerationSeconds=300 unless explicitly set. These tolerations allow Pods to remain bound to nodes for 5 minutes after detecting these issues.

Let's update our UI component to bind its pods to our Graviton NodePool.

We've tainted the NodePool with key:GravitonOnly and it automatically adds a karpenter.sh/nodepool label.

The following values-ui.yaml contains the changes needed to our UI app configuration in order to enable this setup.

Re-deploy the UI component
cat << EOF >~/environment/values-ui.yaml
app:
  theme: default
  endpoints: 
    catalog: http://retail-store-app-catalog:80
    carts: http://retail-store-app-carts:80
    checkout: http://retail-store-app-checkout:80
    orders: http://retail-store-app-orders:80

topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: ScheduleAnyway
    labelSelector:
      matchLabels:
        app.kubernetes.io/name: ui

autoscaling:
  enabled: false
  minReplicas: 3
  maxReplicas: 10
  targetCPUUtilizationPercentage: 80

nodeSelector:
  karpenter.sh/nodepool: graviton
tolerations:
- key: "GravitonOnly"
  operator: "Exists"
EOF

helm upgrade -f ~/environment/values-ui.yaml retail-store-app-ui oci://public.ecr.aws/aws-containers/retail-store-sample-ui-chart --version ${RETAIL_STORE_APP_HELM_CHART_VERSION} --hide-notes

Note that the UI component will revert to its original configuration in the values-ui.yaml file, which specified a single replica.

This should produce the following output:

Pulled: public.ecr.aws/aws-containers/retail-store-sample-ui-chart:1.1.0
Digest: sha256:5cd721c10214c306b06c7223367f626f21a8d471eee8f0a576742426f84141f2
Release "retail-store-app-ui" has been upgraded. Happy Helming!
NAME: retail-store-app-ui
LAST DEPLOYED: Sat Jan 18 23:54:22 2025
NAMESPACE: default
STATUS: deployed
REVISION: 4
➤ Before examining the new Graviton nodes, ensure all UI component pods are ready:

kubectl wait --for=condition=Ready pod -l app.kubernetes.io/instance=retail-store-app-ui --namespace default --timeout=300s

➤ Now check the status of our EKS cluster nodes and UI component pods:

kubectl get nodes -L kubernetes.io/arch -L karpenter.sh/nodepool
kubectl get pods -l app.kubernetes.io/name=ui -o wide

With the expected output containing arm64 instances, similar to below:

NAME                  STATUS   ROLES    AGE    VERSION               ARCH    NODEPOOL
i-078b82d5fe991368d   Ready    <none>   45s    v1.32.5-eks-98436be   arm64   system
i-0b23214bab198a3f1   Ready    <none>   105m   v1.32.5-eks-98436be   amd64   general-purpose
i-0f67fef87c747070d   Ready    <none>   104s   v1.32.5-eks-98436be   arm64   graviton
NAME                                   READY   STATUS    RESTARTS   AGE    IP                NODE
retail-store-app-ui-86df66db68-744sn   1/1     Running   0          2m7s   192.168.190.160   i-0f67fef87c747070d
As you can see, the UI component pods are now running on the Graviton NodePool. You can also see the taint on the node using the kubectl describe node command and the matching tolerations on the pods using the kubectl describe pod command.

Summary
In this lab, we explored using AWS Graviton instances in EKS Auto Mode for improved performance and cost efficiency.

We created a dedicated Graviton NodePool configured for arm64 architecture instances with a GravitonOnly taint to control Pod scheduling. We then modified our application by updating the UI component's configuration with the necessary node selector and toleration to enable running on Graviton instances.

In the next lab we will explore combining On-Demand instances with Spot Instances for additional cost optimization.




On-Demand & Spot
Overview | Create NodePools | Deploy the application | Summary

In this section, we will explore the basics of EKS Auto Mode and dive deep into customization options.

Overview
Currently, all our compute nodes are running on On-Demand capacity. However, AWS EC2 offers multiple purchase options  for running EKS workloads.

Amazon EC2 Spot Instances  enable you to leverage unused EC2 capacity in the AWS cloud at discounts of up to 90% compared to On-Demand prices. Spot Instances are ideal for stateless, fault-tolerant, or flexible applications including big data workloads, containerized applications, CI/CD pipelines, web servers, high-performance computing (HPC), and test & development environments. These instances are particularly cost-effective when you have flexibility in application timing and can handle potential interruptions.

In this section, we will see how we can run workloads using both On-Demand and EC2 Spot Instances with a desired ratio to guarantee the base availability of On-Demand nodes, while leveraging Spot Instances for optimizing costs.

Create NodePools
We will create two NodePools that utilize Karpenter's capability to distribute workloads between on-demand and spot instances according to a defined ratio.

➤ Let's create the NodePools:

cat << EOF >~/environment/nodepool-ondemandspotsplit.yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: ondemand
  labels:
    app.kubernetes.io/managed-by: app-team
spec:
  disruption:
    consolidateAfter: 30s
    consolidationPolicy: WhenEmptyOrUnderutilized
  template:
    metadata:
      labels:
        EKSAutoNodePool: OnDemandSpotSplit
    spec:
      expireAfter: 336h
      nodeClassRef:
        group: eks.amazonaws.com
        kind: NodeClass
        name: default
      requirements:
      - key: eks.amazonaws.com/instance-category
        operator: In
        values:
        - c
        - m
        - r
      - key: eks.amazonaws.com/instance-generation
        operator: Gt
        values:
        - "4"
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
      - key: karpenter.sh/capacity-type
        operator: In
        values:
        - on-demand
      - key: capacity-spread
        operator: In
        values:
        - "1"
      taints:
      - effect: NoSchedule
        key: OnDemandSpotSplit
      terminationGracePeriod: 24h0m0s
---
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: spot
  labels:
    app.kubernetes.io/managed-by: app-team
spec:
  disruption:
    consolidateAfter: 30s
    consolidationPolicy: WhenEmptyOrUnderutilized
  template:
    metadata:
      labels:
        EKSAutoNodePool: OnDemandSpotSplit
    spec:
      expireAfter: 336h
      nodeClassRef:
        group: eks.amazonaws.com
        kind: NodeClass
        name: default
      requirements:
      - key: eks.amazonaws.com/instance-category
        operator: In
        values:
        - c
        - m
        - r
      - key: eks.amazonaws.com/instance-generation
        operator: Gt
        values:
        - "4"
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
      - key: karpenter.sh/capacity-type
        operator: In
        values:
        - spot
      - key: capacity-spread
        operator: In
        values:
        - "2"
        - "3"
        - "4"
        - "5"
      taints:
      - effect: NoSchedule
        key: OnDemandSpotSplit
      terminationGracePeriod: 24h0m0s
EOF

kubectl apply -f ~/environment/nodepool-ondemandspotsplit.yaml

The output should be similar:

nodepool.karpenter.sh/ondemand created
nodepool.karpenter.sh/spot created
We leverage Karpenter's node labeling and topology spread capabilities to implement a straightforward method for distributing workloads between on-demand and spot instances at a desired ratio.

To achieve this, we've created separate NodePools for Spot and On-Demand capacity types, each using distinct values for a custom label called capacity-spread. In our configuration, the spot NodePool has four unique values while the On-Demand NodePool has one value. When workloads are spread evenly across this label, we achieve a 4:1 ratio of spot to On-Demand nodes.

Deploy the application
Now, we'll configure the catalog component to distribute its replicas across Spot Instances. We'll set up 5 replicas of our catalog application and use the capacity-spread label to achieve our desired 4:1 ratio of spot to On-Demand nodes.

Configure and re-deploy the catalog component
➤ Let's configure and re-deploy the component:

cat << EOF >~/environment/values-catalog.yaml
replicaCount: 5
  
topologySpreadConstraints:
  - maxSkew: 1
    minDomains: 5
    topologyKey: capacity-spread
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        app.kubernetes.io/name: catalog

nodeSelector:
  EKSAutoNodePool: OnDemandSpotSplit
tolerations:
- key: "OnDemandSpotSplit"
  operator: "Exists"
EOF

helm upgrade -f ~/environment/values-catalog.yaml retail-store-app-catalog oci://public.ecr.aws/aws-containers/retail-store-sample-catalog-chart --version ${RETAIL_STORE_APP_HELM_CHART_VERSION} --hide-notes

The output should be similar to:

Pulled: public.ecr.aws/aws-containers/retail-store-sample-catalog-chart:1.1.0
Digest: sha256:0dc16ac63a8f32f309ea7d69f58973520c156305f69f2703fcb5564da6b67eb6
Release "retail-store-app-catalog" has been upgraded. Happy Helming!
NAME: retail-store-app-catalog
LAST DEPLOYED: Sun Jan 19 23:42:07 2025
NAMESPACE: default
STATUS: deployed
REVISION: 2
Let's verify that the catalog component pods are running on Spot instances.

➤ Execute these commands to check the nodes and pods:

kubectl get node -L karpenter.sh/capacity-type --no-headers | while read node status roles age version capacity_type; do
echo "Pods on node $node (Capacity Type: $capacity_type):"
  kubectl get pods --all-namespaces --field-selector spec.nodeName=$node -l app.kubernetes.io/instance=retail-store-app-catalog
echo "-----------------------------------"
done

Note that it may take a minute for the instances to be created and operational and that nodes that will be removed after consolidation will appear empty.

Pods on node i-02e25edfdcf052a8d (Capacity Type: on-demand):
No resources found
-----------------------------------
Pods on node i-059c896e2ddc3787f (Capacity Type: spot):
NAMESPACE   NAME                                        READY   STATUS    RESTARTS   AGE
default     retail-store-app-catalog-7568d4cffb-svztn   1/1     Running   0          2m13s
-----------------------------------
Pods on node i-074da6ae5127bf5dd (Capacity Type: on-demand):
No resources found
-----------------------------------
Pods on node i-0865727e5109e8eda (Capacity Type: on-demand):
NAMESPACE   NAME                                        READY   STATUS    RESTARTS   AGE
default     retail-store-app-catalog-7568d4cffb-ft88b   1/1     Running   0          2m16s
-----------------------------------
Pods on node i-095f16c14ff78d4e0 (Capacity Type: spot):
NAMESPACE   NAME                                        READY   STATUS    RESTARTS   AGE
default     retail-store-app-catalog-7568d4cffb-gkvqj   1/1     Running   0          2m59s
-----------------------------------
Pods on node i-0995ee2810f2eda4b (Capacity Type: on-demand):
No resources found
-----------------------------------
Pods on node i-0b36d75cd120def01 (Capacity Type: spot):
NAMESPACE   NAME                                        READY   STATUS    RESTARTS   AGE
default     retail-store-app-catalog-7568d4cffb-s6t4f   1/1     Running   0          3m2s
-----------------------------------
Pods on node i-0d7d6046307eecb7e (Capacity Type: spot):
NAMESPACE   NAME                                        READY   STATUS    RESTARTS   AGE
default     retail-store-app-catalog-7568d4cffb-4n4xs   1/1     Running   0          3m3s
We can confirm that the catalog app pods are successfully distributed across both Spot and On-demand capacity types.

Summary
In this lab, we've explored how to effectively combine On-Demand and Spot instances in EKS Auto Mode. We implemented a split-ratio strategy using two NodePools and the capacity-spread label, configuring the Spot NodePool with four unique spread values and the On-Demand NodePool with one value to achieve a 4:1 ratio.

To demonstrate this configuration, we deployed our catalog application with 5 replicas and used topology spread constraints to distribute the pods according to our defined ratio. This approach demonstrates how to balance reliability with cost optimization in EKS cluster management by maintaining a baseline of stable On-Demand capacity while leveraging cost-effective Spot instances for workloads that can handle interruptions.

This lab illustrated how to achieve an optimal balance between reliability and cost efficiency in your EKS cluster management strategy.


Networking
EKS Auto Mode supports several network capabilities that are important for security, cluster operations, and application connectivity inside and outside of the cluster.

This section will cover how to expose applications using Network Load Balancer and Application Load Balancer.

Exposing Applications
Overview | Expose the application | Summary

Overview
In this hands-on lab, we'll learn how to expose applications in Amazon EKS Auto Mode using AWS's managed load balancing solutions.

We will work through a practical scenario to:

Configure an Application Load Balancer (ALB) using Kubernetes Ingress resources
Set up a Network Load Balancer (NLB) using Kubernetes Service resources
Implement path-based routing to share a single ALB across multiple services
Observe load balancing in action with custom response headers showing which pods handle requests
Expose the application
EKS Auto Mode simplifies the process by automatically managing the lifecycle of the ALBs and NLBs that are required for our application. As EKS Auto Mode is Kubernetes conformant, it allows us to use the same Kubernetes constructs of Service  and Ingress  to provision those Load Balancers.

In the remainder of this module, we'll learn how to provision those load balancers with Auto Mode.

Step 1: Set Up IngressClass for ALB
Since Ingresses can be implemented by different controllers, each Ingress should specify a class, a reference to an IngressClass resource that contains additional configuration including the name of the controller that should implement the class.

IngressClass resources contain an optional parameters field. This can be used to reference additional implementation-specific configuration for this class.

To target the EKS Auto Mode ALB load balancing capability controller, we will create an IngressClassParams, which allows us to define an AWS specific configuration for our ALB such as certificates to use, the subnets to use for the ALB ENIs, or the ingress group  configuration to group together multiple ingress objects into a single ALB.

The supported configurations for the IngressClassParams objects are listed  in EKS Auto Mode documentation. Additionally, we will create IngressClass that will use the IngressClassParams and point to the EKS Auto Mode capability. This is a one-time setup required for using ALBs in our cluster. Notice the spec.controller definition in the IngressClass below.

➤ Create the IngressClass and IngressClassParams to further configure the EKS Auto Mode load balancing capability:

cat << EOF >~/environment/ingress.yaml
apiVersion: eks.amazonaws.com/v1
kind: IngressClassParams
metadata:
  name: eks-auto-alb
spec:
  scheme: internet-facing
---
apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  name: eks-auto-alb
  annotations:
    ingressclass.kubernetes.io/is-default-class: "true"
spec:
  controller: eks.amazonaws.com/alb
  parameters:
    apiGroup: eks.amazonaws.com
    kind: IngressClassParams
    name: eks-auto-alb
EOF

kubectl apply -f ~/environment/ingress.yaml

This should produce the following output:

ingressclassparams.eks.amazonaws.com/eks-auto-alb created
ingressclass.networking.k8s.io/eks-auto-alb created
➤ Verify that the resources have been created:

kubectl get ingressclass,ingressclassparams

The output should look like the one below:

NAME                                          CONTROLLER              PARAMETERS                                          AGE
ingressclass.networking.k8s.io/eks-auto-alb   eks.amazonaws.com/alb   IngressClassParams.eks.amazonaws.com/eks-auto-alb   56s

NAME                                                GROUP-NAME   SCHEME            IP-ADDRESS-TYPE   AGE
ingressclassparams.eks.amazonaws.com/eks-auto-alb                internet-facing                     56s
Note the controller used, as well as the SCHEME defined (internet-facing in our case) which is based on our configuration above.

Step 2: Deploy the Retail Store Application
We will now update the UI component to provision an ALB by creating an Ingress object, as we can see with the ingress configuration in the component's Helm chart values.

➤ Re-deploy the UI component with custom values:

cat << EOF >~/environment/values-ui.yaml
app:
  theme: default
  endpoints: 
    catalog: http://retail-store-app-catalog:80
    carts: http://retail-store-app-carts:80
    checkout: http://retail-store-app-checkout:80
    orders: http://retail-store-app-orders:80

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 10
  targetCPUUtilizationPercentage: 80

topologySpreadConstraints:
   - maxSkew: 1
     minDomains: 3
     topologyKey: topology.kubernetes.io/zone
     whenUnsatisfiable: DoNotSchedule
     labelSelector:
       matchLabels:
         app.kubernetes.io/name: ui

ingress:
  enabled: true
  className: eks-auto-alb
  annotations:
    alb.ingress.kubernetes.io/healthcheck-path: /actuator/health/liveness
    alb.ingress.kubernetes.io/healthcheck-interval-seconds: '15'
    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: '5'
    alb.ingress.kubernetes.io/healthy-threshold-count: '2'
    alb.ingress.kubernetes.io/unhealthy-threshold-count: '2'
    alb.ingress.kubernetes.io/success-codes: '200-399'
EOF

helm upgrade -f ~/environment/values-ui.yaml retail-store-app-ui oci://public.ecr.aws/aws-containers/retail-store-sample-ui-chart --version ${RETAIL_STORE_APP_HELM_CHART_VERSION} --hide-notes

This should produce an output similar to the following:

Pulled: public.ecr.aws/aws-containers/retail-store-sample-ui-chart:1.1.0
Digest: sha256:5cd721c10214c306b06c7223367f626f21a8d471eee8f0a576742426f84141f2
Release "retail-store-app-ui" has been upgraded. Happy Helming!
NAME: retail-store-app-ui
LAST DEPLOYED: Sun Feb 16 21:30:01 2025
NAMESPACE: default
STATUS: deployed
REVISION: 5
➤ Wait for all deployments to be ready by using the following command:

kubectl wait --for=condition=available deployments retail-store-app-ui --all

Example output:

deployment.apps/retail-store-app-ui condition met
Step 3: Access the UI application with the provisioned ALB
The Ingress object that we've deployed in the previous step gets translated by EKS Auto Mode into an ALB with the appropriate configurations we've defined in the Ingress object itself, and in the IngressClassParams above.

➤ Retrieve the ALB's DNS endpoint by executing the following command:

export ALB_URL=$(kubectl get ingress retail-store-app-ui -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
# wait for the alb to become active
aws elbv2 wait load-balancer-available --load-balancer-arns $(aws elbv2 describe-load-balancers --query 'LoadBalancers[?DNSName==`'"$ALB_URL"'`].LoadBalancerArn' --output text)

echo "Your application is available at: http://${ALB_URL}"

ALB provisioning takes couple of minutes
Note that ALB provision and targets registration may take several minutes.

At this point, we can use the URL to access the application in a new browser window.

Step 4: Expose Catalog Service Using NLB
In the previous steps, we've used EKS Auto Mode to provision an ALB. We'll now experience how to use EKS Auto Mode to provision NLB using the Kubernetes Service object.

➤ Execute the following command.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
kubectl apply -f - << EOF
apiVersion: v1
kind: Service
metadata:
  name: catalog-nlb
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: "external"
    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: "ip"
    service.beta.kubernetes.io/aws-load-balancer-scheme: "internet-facing"
spec:
  type: LoadBalancer
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
  selector:
    app.kubernetes.io/name: catalog
EOF

➤ Wait for the NLB to be provisioned and get its URL:

export NLB_URL=$(kubectl get service catalog-nlb -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
# wait for the NLB to become active
aws elbv2 wait load-balancer-available --load-balancer-arns $(aws elbv2 describe-load-balancers --query 'LoadBalancers[?
DNSName==`'"$NLB_URL"'`].LoadBalancerArn' --output text)
echo "The catalog service is also available at: http://${NLB_URL}"

Step 5.1: Test Application Load Balancer Access
Let's test access to our application and observe load balancing in action. The catalog service has been configured with 5 replicas in the compute module under "On-Demand & Spot Split Ratio". We will use this to demonstrate load distribution.

➤ First, verify that all catalog pods are running:

kubectl get pods -l app.kubernetes.io/name=catalog,app.kubernetes.io/component=service

You should see that all of the catalog's component pods are in Running state. Now, let's use two terminal windows to observe the load balancing behavior.

➤ In your first terminal, watch the logs from all catalog pods:

kubectl logs -f -l app.kubernetes.io/name=catalog,app.kubernetes.io/component=service --prefix=true

➤ In your second terminal, generate some traffic through the ALB:

# Get the ALB URL
export ALB_URL=$(kubectl get ingress retail-store-app-ui -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
echo "The application is available at: http://${ALB_URL}"

# Generate traffic to see load balancing across pods
CURL_CMD=$(which curl)
for i in {1..15}; do
  echo "Sending request $i..."
  $CURL_CMD -s "http://${ALB_URL}/catalog?request=$i" > /dev/null
  sleep 2
done

You should see detailed logs in your first terminal showing requests being distributed across all three catalog pods. Each log line shows:

Which Pod handled the request (in the prefix)
HTTP method and path
Response status code
Request processing time
Client IP address
Example log output showing distribution across pods:

[pod/retail-store-app-catalog-dcb5d8d4c-6gn82/catalog] 2025/06/05 09:03:52 /appsrc/repository/repository.go:204
[pod/retail-store-app-catalog-dcb5d8d4c-6gn82/catalog] [0.185ms] [rows:4] SELECT _ FROM `tags` ORDER BY display_name asc
[pod/retail-store-app-catalog-dcb5d8d4c-6gn82/catalog] [GIN] 2025/06/05 - 09:03:52 | 200 | 290.527µs | 192.168.75.100 | GET "/catalog/tags"
[pod/retail-store-app-catalog-dcb5d8d4c-6gn82/catalog]
[pod/retail-store-app-catalog-dcb5d8d4c-6gn82/catalog] 2025/06/05 09:03:52 /appsrc/repository/repository.go:190
[pod/retail-store-app-catalog-dcb5d8d4c-6gn82/catalog] [0.061ms] [rows:1] SELECT count(_) FROM `products`
[pod/retail-store-app-catalog-dcb5d8d4c-6gn82/catalog] [GIN] 2025/06/05 - 09:03:52 | 200 | 237.024µs | 192.168.75.100 | GET "/catalog/size?tags="
[pod/retail-store-app-catalog-dcb5d8d4c-6gn82/catalog]
[pod/retail-store-app-catalog-dcb5d8d4c-6gn82/catalog] 2025/06/05 09:03:52 /appsrc/repository/repository.go:154
[pod/retail-store-app-catalog-dcb5d8d4c-6gn82/catalog] [0.103ms] [rows:6] SELECT _ FROM `product_tags` WHERE `product_tags`.`product_id` IN ("a1258cd2-176c-4507-ade6-746dab5ad625","d4edfedb-dbe9-4dd9-aae8-009489394955","79bce3f3-935f-4912-8c62-0d2f3e059405","8757729a-c518-4356-8694-9e795a9b3237","4f18544b-70a5-4352-8e19-0d070f46745d","d77f9ae6-e9a8-4a3e-86bd-b72af75cbc49")
[pod/retail-store-app-catalog-dcb5d8d4c-6gn82/catalog]
[pod/retail-store-app-catalog-dcb5d8d4c-6gn82/catalog] 2025/06/05 09:03:52 /appsrc/repository/repository.go:154
[pod/retail-store-app-catalog-dcb5d8d4c-6gn82/catalog] [0.091ms] [rows:4] SELECT _ FROM `tags` WHERE `tags`.`name` IN ("clothing","food","vehicles","accessories")
[pod/retail-store-app-catalog-dcb5d8d4c-6gn82/catalog]
[pod/retail-store-app-catalog-dcb5d8d4c-6gn82/catalog] 2025/06/05 09:03:52 /appsrc/repository/repository.go:154
[pod/retail-store-app-catalog-dcb5d8d4c-6gn82/catalog] [1.793ms] [rows:6] SELECT \* FROM `products` ORDER BY products.name asc LIMIT 6
[pod/retail-store-app-catalog-dcb5d8d4c-6gn82/catalog] [GIN] 2025/06/05 - 09:03:52 | 200 | 1.873799ms | 192.168.75.100 | GET "/catalog/products?order=&page=1&size=6&tags="
Notice the highlighted lines showing different Pod suffixes (bzq65, 77c2t, tkdqm) handling the requests, demonstrating the load balancing distribution.

The catalog service uses the Gin web framework which provides detailed request logging. We can observe:

Requests being distributed across different Pods (see the Pod names in the log prefix)
Multiple requests being made for each page load (tags, size, and catalog data)
Response times for each request
Client IPs making the requests
Step 5.2: Test Network Load Balancer Access
To test the access to the catalog service directly through the NLB provisioned earlier, ensure that the kubectl logs command is still running on the other terminal:

kubectl logs -f -l app.kubernetes.io/name=catalog,app.kubernetes.io/component=service --prefix=true

Now we can test from the first terminal the access to the catalog service through the NLB by using the NLB DNS name with the appended URI below

curl http://${NLB_URL}/catalog/products | jq

You should expect to see a JSON response from the catalog service, as well as logs from that request on the other terminal.

Step 6: Share ALB Across Multiple Services - Multiple Ingress Pattern
In some use-cases we need to ensure that multiple ingress objects don't create multiple ALBs but rather use the same ALB with multiple routing rules. This is where EKS Auto Mode supports ingress grouping using the IngressClassParams object (see reference in the documentation ). In this step, we will create a new IngressClass object with new IngressClassParams that supports such grouping. For demonstration purposes, we will then create 2 ingress objects: one for the ui component and one for the catalog service. With this configuration, since we've configured the grouping on the IngressClassParams, a single ALB will be created pointing to both of those services. Follow the steps below to achieve that:

➤ 1. Create a new IngressClassParams and IngressClass with group.name configuration:

cat << EOF >~/environment/ingress-class-group.yaml
apiVersion: eks.amazonaws.com/v1
kind: IngressClassParams
metadata:
  name: eks-auto-alb-group-retail
spec:
  scheme: internet-facing
  group:
    name: retail
---
apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  name: eks-auto-alb-group-retail
  annotations:
    ingressclass.kubernetes.io/is-default-class: "true"
spec:
  controller: eks.amazonaws.com/alb
  parameters:
    apiGroup: eks.amazonaws.com
    kind: IngressClassParams
    name: eks-auto-alb-group-retail
EOF

kubectl apply -f ~/environment/ingress-class-group.yaml

➤ 2. Create an ingress object for the ui component (note the use of the newly created IngressClass eks-auto-alb-group-retail ):

kubectl apply -f - << EOF
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: retail-store-shared-group-ui
  annotations:
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/healthcheck-path: /actuator/health/liveness
spec:
  ingressClassName: eks-auto-alb-group-retail
  rules:
    - http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: retail-store-app-ui
                port:
                  number: 80
EOF

➤ 3. Create a second ingress object for the catalog component:

kubectl apply -f - << EOF
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: retail-store-shared-group-catalog
  annotations:
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/healthcheck-path: /health
spec:
  ingressClassName: eks-auto-alb-group-retail
  rules:
  - http:
      paths:
      - path: /catalog
        pathType: Prefix
        backend:
          service:
            name: retail-store-app-catalog
            port:
              number: 80
EOF

➤ 4. Verify the ingresses had been created:

kubectl get ingress

Note that the output ADDRESS of both ingresses of the catalog and the ui has the same DNS. This is because of the IngressClassParams group configuration we've used above.

NAME                                 CLASS                       HOSTS   ADDRESS                                                                 PORTS   AGE
retail-store-app-ui                  eks-auto-alb                *       k8s-default-retailst-70051948b0-467757540.us-west-2.elb.amazonaws.com   80      16m
retail-store-backend-group-catalog   eks-auto-alb-group-retail   *       k8s-retail-5620a3cc93-1836759971.us-west-2.elb.amazonaws.com            80      30s
retail-store-backend-group-ui        eks-auto-alb-group-retail   *       k8s-retail-5620a3cc93-1836759971.us-west-2.elb.amazonaws.com            80      11m
➤ 5. Extract the ALB DNS (note that because both ingresses have the same DNS, we can randomly choose one of them):

# Get the shared ALB URL
export SHARED_ALB_URL=$(kubectl get ingress retail-store-shared-group-ui -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
# wait for the shared ALB to become active
aws elbv2 wait load-balancer-available --load-balancer-arns $(aws elbv2 describe-load-balancers --query 'LoadBalancers[?DNSName==`'"$SHARED_ALB_URL"'`].LoadBalancerArn' --output text)
echo "The shared ALB is available at: http://$SHARED_ALB_URL"

The ALB load balancer creation process can take a couple of minutes, including its DNS propagation, before you will be able to test it.

➤ 6. Use the extracted DNS to access both services through a single ALB. You should expect an HTML output from the / path (coming from the ui component), and a list of items when hitting the /catalog/products path, coming from the catalog component:

# Get the Shared ALB URL
export SHARED_ALB_URL=$(kubectl get ingress retail-store-shared-group-ui -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
echo "The shared ALB is available at: http://$SHARED_ALB_URL"

# Test each shared service endpoint
echo "Testing / endpoint accessing the ui component..."
curl -s "http://$SHARED_ALB_URL/" 

echo "Testing /catalog endpoint accessing the catalog component..."
curl -s "http://$SHARED_ALB_URL/catalog/products" | jq

Summary
In this lab, we've learned how to:

Deploy a real-world microservices application on EKS Auto Mode
Set up ALB ingress for both frontend and backend services
Use path-based routing to organize our microservices
Observe load balancing across multiple pods
Remember that EKS Auto Mode manages the underlying load balancer infrastructure, but we still need to define the desired state through Kubernetes resources.

For production environments:

Use HTTPS/TLS termination for security
Implement proper health checks for your services
Consider using AWS WAF for additional security
Monitor your ALB metrics in CloudWatch
Configure appropriate timeouts and connection settings for your Spring Boot services
Set up proper logging and monitoring for your microservices
Consider implementing circuit breakers and fallbacks




Advanced Networking
Overview | Handle IP exhaustion | Isolate Pod Network | Summary

Overview
Amazon EKS Auto Mode simplifies and automates critical networking tasks for pod and service connectivity by managing the VPC Container Network Interface (CNI) configuration and load balancer provisioning for the cluster.

In modern environments there are several additional use cases that require advanced network configuration.

IP exhaustion
By default, Amazon VPC CNI will assign pods an IP address selected from the primary subnet. The primary subnet is the subnet CIDR that the primary ENI is attached to, usually the subnet of the node/host.

If the subnet CIDR is too small, the CNI may not be able to acquire enough secondary IP addresses to assign to the pods, which is a common challenge for EKS IPv4 clusters.

Additionally, some workloads require to increase pod density  in order to improve resources utilization. In Amazon EKS this is implemented by enabling VPC CNI prefix mode . To implement the prefix mode, instead of a single IP address, VPC CNI configures EC2 to assign /28 IP prefixes (16 IP addresses) to the ENI IP slots. When EC2 allocates a /28 IPv4 prefix to an ENI, it has to be a contiguous block of IP addresses from your subnet. If the subnet is fragmented due to an increased usage of the subnet by AWS services and worker nodes themselves, the prefix attachment may fail, essentially reducing the IP address space utilization.

Infrastructure and application traffic separation
Creating distinct network paths for different types of communication within a Kubernetes cluster is particularly valuable for organizations that need to maintain clear boundaries between their infrastructure management communications and their application workloads. Node-to-node communication typically includes cluster management traffic, infrastructure monitoring, while pod-to-pod communication handles application-specific data flows and service interactions.

Different network configuration for node and pod subnets
Applying a different network configuration to nodes and pods is also a common requirement for more complex systems. This includes customizing network address translation (SNAT) policies, placement on worker nodes and pods in public or private subnets, and defining different tagging to satisfy tools requirements.

Security considerations
The last two use cases are especially relevant use cases where traffic and access control are crucial to the security of the system.

Addressing the use cases
EKS Auto Mode provides advanced networking capabilities  that allow us to implement granular network controls and traffic separation as well as multiple layers of network security utilizing the standard VPC features  and Kubernetes-native network policies .

In this lab, we will show how to simplify solutions that address IP exhaustion and pod network isolation using EKS Auto Mode advanced networking capabilities. We will do so by:

Adding a secondary CIDR block to the cluster VPC
Creating new subnets from the new CIDR block
Targeting the new subnets via EKS AutoMode NodePool and NodeClass configuration
Configuring the application components to utilize the above configuration
Handle IP exhaustion
Review the EKS Auto Mode configuration
➤ Review the current nodes and pods IPs:

kubectl get nodes -o custom-columns=NAME:.metadata.name,INTERNAL-IP:.status.addresses[0].address
kubectl get pods -o wide

As expected, all pods and nodes belong to the same VPC CIDR - 192.168.0.0/16 we defined for our cluster during its creation:

NAME                  INTERNAL-IP
i-00e062e9543d100fc   192.168.9.85
i-016290d3063f80b19   192.168.123.101
i-0486577e8c56711ba   192.168.125.116
i-04ad8119e5de4ad75   192.168.17.136
i-078b82d5fe991368d   192.168.80.147
i-0b23214bab198a3f1   192.168.41.134
i-0e8c82c5d21eafcec   192.168.44.214
NAME                                         READY   STATUS    RESTARTS   AGE    IP               NODE
retail-store-app-carts-849f69cc8d-jndp8      1/1     Running   0          165m   192.168.50.130   i-0b23214bab198a3f1
retail-store-app-catalog-994d4889c-5fhrs     1/1     Running   0          58m    192.168.122.16   i-016290d3063f80b19
retail-store-app-catalog-994d4889c-j949t     1/1     Running   0          58m    192.168.28.240   i-00e062e9543d100fc
retail-store-app-catalog-994d4889c-k2csz     1/1     Running   0          57m    192.168.100.80   i-0486577e8c56711ba
retail-store-app-catalog-994d4889c-kq65z     1/1     Running   0          57m    192.168.15.128   i-04ad8119e5de4ad75
retail-store-app-catalog-994d4889c-t2mqk     1/1     Running   0          58m    192.168.49.48    i-0e8c82c5d21eafcec
retail-store-app-checkout-6df8f44b97-sx7xx   1/1     Running   0          61m    192.168.50.128   i-0b23214bab198a3f1
retail-store-app-orders-5fd7c6cf7-2xzq6      1/1     Running   0          61m    192.168.50.129   i-0b23214bab198a3f1
retail-store-app-ui-7fbf6d97b9-npm8x         1/1     Running   0          55m    192.168.50.132   i-0b23214bab198a3f1
Add a secondary CIDR to the cluster VPC
➤ Execute the following command to store the VPC ID in the terminal:

export VPC_ID=$(aws eks describe-cluster --name $DEMO_CLUSTER_NAME --query 'cluster.resourcesVpcConfig.vpcId' --output text)

➤ In the same terminal, execute the following command to explore all the CIDRs attached to the cluster VPC:

aws ec2 describe-vpcs \
  --vpc-ids $VPC_ID \
  --query 'Vpcs[0].CidrBlockAssociationSet[].CidrBlock'

Once again, as expected, there is only one, original, CIDR.

A reminder: EKS Auto Mode enables  VPC CNI prefix delegation by default.

For the sake of the workshop, let's assume that we've exhausted enough IPs from the original CIDR block, so that it's impossible to provision new /28 blocks, which we require to reduce latency of pod provision or to increase pod density on our worker nodes.

The most straightforward way of dealing with the IP exhaustion issue is to attach a secondary CIDR to our VPC, create and tag subnets from that secondary CIDR and configure EKS to provision nodes and pods from these subnets.

Let's attach a secondary CIDR to the cluster VPC. Our options are outlined in this document . Since we've already used the entire 192.168.0.0/16 block, we need to select a different one.

➤ In the same terminal as the commands above (as we require the VPC_ID) execute the following command:

aws ec2 associate-vpc-cidr-block \
  --vpc-id ${VPC_ID} \
  --cidr-block 10.0.0.0/16

The above command is expected to fail with the following error:

An error occurred (InvalidVpc.Range) when calling the AssociateVpcCidrBlock operation: The CIDR '10.0.0.0/16' is restricted. Use a CIDR from the same private address range as the current VPC CIDR, or use a publicly-routable CIDR.
For additional restrictions, see https://docs.aws.amazon.com/vpc/latest/userguide/vpc-cidr-blocks.html
This is because, as outlined in the VPC CIDR selection restrictions document , you can not combine different CIDRs from different RFC 1918  blocks.

To resolve this and create more private IP address space for our pods we can use  the 100.64.0.0/10 block instead.

➤ Execute the following command:

aws ec2 associate-vpc-cidr-block \
  --vpc-id ${VPC_ID} \
  --cidr-block 100.64.0.0/16

This should succeed now and produce the following output:

{
    "CidrBlockAssociation": {
        "AssociationId": "vpc-cidr-assoc-0df9d27b82606badc",
        "CidrBlock": "100.64.0.0/16",
        "CidrBlockState": {
            "State": "associating"
        }
    },
    "VpcId": "vpc-0413d277b700882f4"
}
➤ After a moment we can verify that the CIDR has been successfully attached to the cluster vpc by executing:

aws ec2 describe-vpcs \
  --vpc-ids $VPC_ID \
  --query 'Vpcs[0].CidrBlockAssociationSet[].CidrBlock'

The output should now contain both the original and the new CIDR:

[
    "192.168.0.0/16",
    "100.64.0.0/16"
]
Create additional subnets in the cluster VPC
We can now create 3 subnets in 3 different Availability Zones from the new secondary CIDR block, as recommended by the resilience best practices.

➤ Execute the following:

export VPC_ID=$(aws eks describe-cluster --name $DEMO_CLUSTER_NAME --query 'cluster.resourcesVpcConfig.vpcId' --output text)

export SUBNET_ID_A=$(aws ec2 create-subnet \
  --vpc-id ${VPC_ID} \
  --cidr-block 100.64.0.0/19 \
  --availability-zone ${AWS_REGION}a \
  --tag-specifications 'ResourceType=subnet,Tags=[{Key=Name,Value=eks-subnet-2a},{Key=advanced-networking,Value=1}]' \
  --query 'Subnet.SubnetId' \
  --output text)

export SUBNET_ID_B=$(aws ec2 create-subnet \
  --vpc-id ${VPC_ID} \
  --cidr-block 100.64.32.0/19 \
  --availability-zone ${AWS_REGION}b \
  --tag-specifications 'ResourceType=subnet,Tags=[{Key=Name,Value=eks-subnet-2b},{Key=advanced-networking,Value=1}]' \
  --query 'Subnet.SubnetId' \
  --output text)

export SUBNET_ID_C=$(aws ec2 create-subnet \
  --vpc-id ${VPC_ID} \
  --cidr-block 100.64.64.0/19 \
  --availability-zone ${AWS_REGION}c \
  --tag-specifications 'ResourceType=subnet,Tags=[{Key=Name,Value=eks-subnet-2c},{Key=advanced-networking,Value=1}]' \
  --query 'Subnet.SubnetId' \
  --output text)

➤ Verify that the subnets have been created properly:

aws ec2 describe-subnets \
  --filters "Name=tag:advanced-networking,Values=1" \
  --query "Subnets[*].CidrBlock"

This should produce the following output:

[
    "100.64.64.0/19",
    "100.64.32.0/19",
    "100.64.0.0/19"
]
For pods to communicate with external resources, we need to associate our new subnets with a route table that defines the required configuration. In this case we can simply use the same route table we've used for the rest of the subnets.

➤ Execute the following (in that same terminal):

export ROUTE_TABLE_ID=$(aws ec2 describe-route-tables \
  --filters "Name=vpc-id,Values=${VPC_ID}" "Name=route.nat-gateway-id,Values=nat-*" \
  --query 'RouteTables[0].RouteTableId' \
  --output text)

aws ec2 associate-route-table \
  --route-table-id ${ROUTE_TABLE_ID} \
  --subnet-id ${SUBNET_ID_A}

aws ec2 associate-route-table \
  --route-table-id ${ROUTE_TABLE_ID} \
  --subnet-id ${SUBNET_ID_B}

aws ec2 associate-route-table \
  --route-table-id ${ROUTE_TABLE_ID} \
  --subnet-id ${SUBNET_ID_C}

Note that assigning a new VPC CIDR automatically updated the main route table to designate the 100.64.0.0/16 as a local target, in the same manner it does for the original 192.168.0.0/16 CIDR.

This expected output is as follows:

{
    "AssociationId": "rtbassoc-0bf6e8322df34a5a9",
    "AssociationState": {
        "State": "associated"
    }
}
{
    "AssociationId": "rtbassoc-0c742c5288fc4a532",
    "AssociationState": {
        "State": "associated"
    }
}
{
    "AssociationId": "rtbassoc-069e88b0bbe1d0fc0",
    "AssociationState": {
        "State": "associated"
    }
}
If we were to deploy an application, for its pods to be scheduled on instances provisioned in the new subnets, it would not actually work.

This is because Auto Mode autoscaling mechanism, via the built-in default NodeClass, doesn't "know" about them.

Create a custom NodeClass and NodePool to utilize the new subnets
To introduce the subnets and to make sure that network communication and connection to AWS services would work properly, we will target the corresponding tag (advanced-networking: '1'), while re-using the IAM role and the original security group that allows the traffic between pods and the control plane.

➤ Create a new NodeClass that targets the new subnets:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
cat << EOF > ~/environment/advanced-networking-nodeclass.yaml
apiVersion: eks.amazonaws.com/v1
kind: NodeClass
metadata:
  name: advanced-networking
spec:
  role: '${DEMO_CLUSTER_NODE_ROLE_NAME}'
  subnetSelectorTerms:
    - tags:
        advanced-networking: '1'
  securityGroupSelectorTerms:
    - tags:
        kubernetes.io/cluster/demo-cluster: owned
EOF

➤ Create a new NodePool that uses the NodeClass above:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
cat << EOF > ~/environment/advanced-networking-nodepool.yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: advanced-networking
spec:
  disruption:
    consolidationPolicy: WhenEmpty
    consolidateAfter: 5m
  template:
    metadata:
      labels:
        role: advanced-networking
    spec:
      nodeClassRef:
        group: eks.amazonaws.com
        kind: NodeClass
        name: advanced-networking
      requirements:
        - key: kubernetes.io/arch
          operator: In
          values: [amd64]
        - key: karpenter.sh/capacity-type
          operator: In
          values: [on-demand]
        - key: eks.amazonaws.com/instance-category
          operator: In
          values: [c, m, r]
        - key: eks.amazonaws.com/instance-cpu
          operator: In
          values: ['4', '8', '16', '32']
EOF

Note that we've added a custom label to the NodePool above to allow targeting these specific worker nodes with a nodeSelector in one of our application components. We only do the explicit targeting to demonstrate that these subnets are fully operational. In a real-world scenario new nodes and pods would consume IPs from the new subnets as required – most notably when there are no more IPs in the original subnets.

➤ Deploy the NodePool and the NodeClass:

kubectl apply -f ~/environment/advanced-networking-nodeclass.yaml
kubectl apply -f ~/environment/advanced-networking-nodepool.yaml

➤ Verify that the created components are ready to be used (may take a couple of seconds):

kubectl get nodepool,nodeclass

To illustrate pods being provisioned in the new subnets, we'll re-deploy the UI application component.

➤ Execute the following command to create a custom values.yaml file:

1
2
3
4
5
6
7
cat << EOF > ~/environment/advanced-networking-values-ui.yaml
replicaCount: 3
autoscaling:
  minReplicas: 3
nodeSelector:
  role: advanced-networking
EOF

We only add the node selector and increase the amount of replicas (to illustrate topology spread across the new subnets) to the file above as we will re-use the rest of the values from the previous Helm chart installation.

➤ Re-deploy the UI component:

1
2
3
4
5
helm upgrade retail-store-app-ui oci://public.ecr.aws/aws-containers/retail-store-sample-ui-chart \
  --version ${RETAIL_STORE_APP_HELM_CHART_VERSION} \
  --values ~/environment/advanced-networking-values-ui.yaml \
  --reuse-values \
  --wait

Note that it will take a minute or so for the new instances to become operational.

➤ We can verify that both the UI pods and their nodes received IPs from the new subnets:

kubectl get nodes -o custom-columns=NAME:.metadata.name,IP:.status.addresses[0].address -l role=advanced-networking
kubectl get pods -l app.kubernetes.io/name=ui -o wide

This should provide an output similar to the following:

NAME                  IP
i-02b5a3d625f34288b   100.64.26.164
i-042c5454735eab393   100.64.58.201
i-0fcfb02997484b44c   100.64.84.98
NAME                                   READY   STATUS    RESTARTS   AGE   IP              NODE
retail-store-app-ui-69db5c4cdc-2tr6s   1/1     Running   0          20m   100.64.47.144   i-042c5454735eab393
retail-store-app-ui-69db5c4cdc-676hh   1/1     Running   0          25m   100.64.3.16     i-02b5a3d625f34288b
retail-store-app-ui-69db5c4cdc-wr8ll   1/1     Running   0          20m   100.64.79.48    i-0fcfb02997484b44c
We have now configured our subnets and the corresponding Auto Mode components to address the common IP exhaustion use case.

However, there are additional considerations, such as traffic separation or application of different security controls between nodes and pods, that require us to take the network configuration and create pod network isolation.

Isolate Pod Network
EKS Auto Mode allows to address these requirements by using subnet selection for pods  NodeClass configuration.

➤ Update the advanced-networking NodeClass by executing:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
cat << EOF > ~/environment/advanced-networking-nodeclass.yaml
apiVersion: eks.amazonaws.com/v1
kind: NodeClass
metadata:
  name: advanced-networking
spec:
  role: '${DEMO_CLUSTER_NODE_ROLE_NAME}'
  subnetSelectorTerms:
    - tags:
        kubernetes.io/role/internal-elb: '1'
  securityGroupSelectorTerms:
    - tags:
        kubernetes.io/cluster/demo-cluster: owned
  podSubnetSelectorTerms:
    - tags:
        advanced-networking: '1'
  podSecurityGroupSelectorTerms:
    - tags:
        kubernetes.io/cluster/demo-cluster: owned
EOF

The code above (see the highlighted lines) ensures that nodes and pods are placed into different subnets, while still allowing control plane and node-to-pod communications. We achieved that by configuring:

the node-level subnet selector to target the original subnets
the pod-level subnet selector (podSubnetSelectorTerms) to target the new subnets we created earlier
the security group selector for both nodes and pods (identical in this example) to target a shared security group that allows traffic between the control plane, nodes and (now) pods
Note that for pods we've specifically targeted the private subnets, using kubernetes.io/role/elb: 1 tag as outlined in the documentation .

Note that using podSecurityGroupSelectorTerms is mandatory when using podSubnetSelectorTerms configuration. Alternatively, we could have used a different security group to also address the traffic separation use case in the same configuration.

Also note that EKS Auto Mode doesn't support  Security Groups per Pod (SGPP).

Finally, keep in mind the following considerations for subnet selectors for pods:

Reduced pod density: fewer pods can run on each node, because the IP slots on the node's primary EMI can no longer be used for pods
Routing configuration: route table and network Access Control List (ACL) of the pod subnets are properly configured to allow the required communications
➤ Deploy the NodeClass (the advanced-networking NodePool doesn't require any changes):

kubectl apply -f ~/environment/advanced-networking-nodeclass.yaml

➤ Verify that the created components are ready to be used:

kubectl get nodepool,nodeclass

Once applied, we don't actually need to do anything else, as Auto Mode will detect the drift  (difference between the cluster state and the configuration outlined in the NodeClass) and reconcile the cluster to the desired state – node and pod subnet separation as we defined.

➤ Verify that the UI pods and their nodes received IPs from different subnets:

kubectl get nodes -o custom-columns=NAME:.metadata.name,IP:.status.addresses[0].address -l role=advanced-networking
kubectl get pods -l app.kubernetes.io/name=ui -o wide

Note that it will take a couple of minutes for the new non-drifted images to become operational.

This should provide an output similar to the following, showing nodes and pods IPs indeed belong to different subnets:

NAME                  IP
i-03fed7a34057e58c2   192.168.164.36
i-042f5d093908ac461   192.168.105.230
i-0a126a6040e63ec87   192.168.154.221
NAME                                   READY   STATUS    RESTARTS   AGE     IP              NODE
retail-store-app-ui-69db5c4cdc-knhv8   1/1     Running   0          5m22s   100.64.34.96    i-0a126a6040e63ec87
retail-store-app-ui-69db5c4cdc-nvtnh   1/1     Running   0          7m11s   100.64.4.96     i-042f5d093908ac461
retail-store-app-ui-69db5c4cdc-vgqm5   1/1     Running   0          6m21s   100.64.81.145   i-03fed7a34057e58c2
➤ We can verify that the application continues to function properly (showing the network setup is working) by extracting the ALB DNS and Ctrl/Cmd-clicking the printed URL:

export SHARED_ALB_URL=$(kubectl get ingress retail-store-shared-group-ui -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
echo "The shared ALB is available at: http://$SHARED_ALB_URL"

Summary
In this lab, we've learned how to address common IP exhaustion and network separation use cases by performing the following:

extending the cluster VPC with secondary CIDR blocks to provide additional IP address space
creating new subnets in the secondary CIDR block
associating a route table to ensure subnet-to-subnet traffic
configuring EKS AutoMode NodePool and NodeClass resources to implement advanced networking (with or without podSubnetSelectorTerms and podSecurityGroupSelectorTerms)


EBS Storage
A StorageClass in Amazon EKS Auto Mode defines how Amazon EBS volumes are automatically provisioned when applications request persistent storage. By configuring a StorageClass, you can specify default settings for your EBS volumes including volume type, encryption, IOPS, and other storage parameters. You can also configure a StorageClass to use AWS KMS keys for encryption management.

In this section you will learn to create and configure StorageClass resources that works with Amazon EKS Auto Mode to provision EBS volumes.



StatefulSets and PersistentVolumes
Overview | Inspect SQL databases | Demonstrate the ephemeral nature of emptyDir | Summary

Overview
The catalog microservice utilizes an SQL database running in a separate Pod in the cluster. Before we dive deeper into how these are deployed, let's review a number of relevant Kubernetes concepts:

A Volume  is a data store which is accessible to the containers in a pod. How that data store comes to be, and the medium that backs it, are determined by the particular volume type used.
An Ephemeral Volume  follows a pod's lifetime, and gets created and deleted along with the pod.
A PersistentVolume  (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using a StorageClass. It is a resource in the cluster just like a node is a cluster resource. PVs are a special kind of Volume with a lifecycle independent of any individual Pod that uses the PV.
A PersistentVolumeClaim  (PVC) is a request for persistent storage by a user. It is the storage equivalent of a pod, wherein pods consume node resources and PVCs consume PV resources. Just as pods can request specific levels of resources (CPU and Memory), claims can request specific size and access modes (e.g. they can be mounted ReadWriteOnce, ReadOnlyMany, ReadWriteMany, or ReadWriteOncePod).
A StatefulSet  runs a group of pods, and maintains a sticky identity for each of those pods. Although individual pods in a StatefulSet are susceptible to failure, the persistent Pod identifiers make it easier to match existing volumes to the new pods that replace any that have failed. This is useful for managing applications, such as databases, that need persistent storage.
A StorageClass  provides a way for administrators to describe a "class" of storage they offer. Different classes might map to quality-of-service levels, or to backup policies, or to arbitrary policies determined by cluster administrators.
Dynamic Volume Provisioning  allows storage volumes to be created on-demand. Without dynamic provisioning, cluster administrators have to manually make calls to their cloud or storage provider to create new storage volumes, and then create PersistentVolume objects to represent them in Kubernetes. The dynamic provisioning feature eliminates the need for cluster administrators to pre-provision storage. Instead, it automatically provisions storage when it is requested by users.
Enable MySQL component for the catalog service
Since we initially deployed the catalog service with an in-memory database, let's first enable a MySQL Pod with no persistent storage for it before demonstrating how to use Amazon EKS Auto Mode for stateful applications. Run the below command:

helm upgrade -i retail-store-app-catalog oci://public.ecr.aws/aws-containers/retail-store-sample-catalog-chart --version ${RETAIL_STORE_APP_HELM_CHART_VERSION} -f - <<EOF
mysql:
  create: true
EOF

Ensure that the MySQL Pod of the catalog service is up and running:

kubectl get pods -l app.kubernetes.io/instance=retail-store-app-catalog -l app.kubernetes.io/component=mysql

Inspect SQL databases
The catalog microservice utilizes a MySQL database running in a Pod in the cluster.

➤ Let's inspect the MySQL database Pod to see its current volume configuration:

kubectl describe statefulset retail-store-app-catalog-mysql

You should see output similar to the below:

Name:               retail-store-app-catalog-mysql-0
Namespace:          default
...
Replicas:           1 desired | 1 total
...
Pod Template:
...
  Containers:
   mysql:
    Image:      public.ecr.aws/docker/library/mysql:8.0
    Port:       3306/TCP
    Host Port:  0/TCP
    Environment:
      MYSQL_ROOT_PASSWORD:  my-secret-pw
      MYSQL_DATABASE:       catalog
      MYSQL_USER:           <set to the key 'username' in secret 'catalog-db'> Optional: false
      MYSQL_PASSWORD:       <set to the key 'password' in secret 'catalog-db'>  Optional: false
    Mounts:
      /var/lib/mysql from data (rw)
  Volumes:
   data:
    Type:          EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:
    SizeLimit:     <unset>
  Node-Selectors:  <none>
  Tolerations:     <none>
Volume Claims:     <none>
...
We can make the following observations:

The MySQL database is deployed as a StatefulSet with a single replica.
The Pod template includes a single mysql container, with a data volume of type emptyDir.
Demonstrate the ephemeral nature of emptyDir
An emptyDir volume is first created when a Pod is assigned to a node, and exists as long as that Pod is running on that node. As the name implies, the emptyDir volume is initially empty. All containers in the Pod can read and write the same files in the emptyDir volume, though that volume can be mounted on the same or different paths in each container. When a Pod is removed from a node for any reason, the data in the emptyDir is deleted permanently. Therefore emptyDir is not a good fit for our SQL databases.

We can demonstrate the ephemeral nature of emptyDir by starting a shell session inside the MySQL container and creating a test file. After that we'll delete the Pod that is running in our StatefulSet. Because the Pod is using an emptyDir and not a PV, the file will not survive a Pod restart.

➤ First let's run a command inside our MySQL container to create a file in the /tmp directory:

kubectl exec retail-store-app-catalog-mysql-0 -- bash -c "echo 123 > /tmp/test.txt"

➤ Now, let's verify our test.txt file was created in the tmp directory:

kubectl exec retail-store-app-catalog-mysql-0 -- cat /tmp/test.txt

You should see the contents of the file we created:

123
➤ Now, let's remove the current retail-store-app-catalog-mysql Pod to force the StatefulSet controller to automatically re-create a new retail-store-app-catalog-mysql Pod:

kubectl delete pod retail-store-app-catalog-mysql-0

➤ Wait for the Pod to be re-created:

kubectl wait --for=condition=Ready pod retail-store-app-catalog-mysql-0 --timeout=30s

After a few seconds you should see:

pod/retail-store-app-catalog-mysql-0 condition met
➤ Verify the Pod is running:

kubectl get pod retail-store-app-catalog-mysql-0

The output should be similar to the following:

NAME                                   READY   STATUS    RESTARTS   AGE
retail-store-app-catalog-mysql-0   1/1     Running   0          48s
➤ Check for the presence of test.txt in the /tmp directory:

kubectl exec retail-store-app-catalog-mysql-0 -- cat /tmp/test.txt

With the following output:

cat: /tmp/test.txt: No such file or directory
command terminated with exit code 1
You can see that the test.txt file no longer exists due to emptyDir volumes being ephemeral.

Summary
In this section, we performed the following steps:

Inspected the StatefulSet resources associated with the catalog MySQL database.
Verified that this database is provisioned using an emptyDir volume.
Demonstrated the ephemeral nature of emptyDir volumes.
In the next section, we will define a default StorageClass and use this to create a persistent volume for the catalog MySQL database.

Default Storage Class using EBS CSI driver
Overview | Update the components | Summary

Overview
In the previous section we saw that the databases for the catalog and orders services are using ephemeral emptyDir volumes for storage. In the remainder of this module, we will create a StorageClass which we will then use to replace these volumes with persistent volumes using the EBS CSI driver. We will also learn how we can use the StorageClass to configure a volume parameter, such as a KMS key to encrypt the volume.

In this section we will create a default StorageClass and use this to create a persistent volume for the catalog MySQL database. In the next section, we will then create an additional StorageClass for the orders PostgreSQL database which increases the amount of provisioned IOPS.

Update the components
Create a default StorageClass with KMS encryption
➤ First, let's create a KMS key as follows:

KEY_ID=$(aws kms create-key --tags TagKey=Name,TagValue=eks-automode-workshop --query 'KeyMetadata.KeyId' --output text)
KEY_ARN=$(aws kms describe-key --key-id $KEY_ID --query 'KeyMetadata.Arn' --output text)
echo "Key Id:" $KEY_ID
echo "Key Arn:" $KEY_ARN

➤ Next, let's create an IAM resource policy JSON document for the KMS key, which allows the CSI service that runs on the EC2 managed instance to assume that role to encrypt & decrypt the data written to the EBS volume:

cat >key-policy.json <<EOF
{
    "Version": "2012-10-17",
    "Id": "key-auto-policy-3",
    "Statement": [
        {
            "Sid": "iam-kms",
            "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::$AWS_ACCOUNT_ID:root"
            },
            "Action": "kms:*",
            "Resource": "*"
        },
        {
            "Sid": "ec2-kms",
            "Effect": "Allow",
            "Principal": {
                "AWS": "*"
            },
            "Action": [
                "kms:Encrypt",
                "kms:Decrypt",
                "kms:ReEncrypt*",
                "kms:GenerateDataKey*",
                "kms:CreateGrant",
                "kms:DescribeKey"
            ],
            "Resource": "*",
            "Condition": {
                "StringEquals": {
                    "kms:CallerAccount": "$AWS_ACCOUNT_ID",
                    "kms:ViaService": "ec2.$AWS_REGION.amazonaws.com"
                }
            }
        }
    ]
}
EOF

➤ Now let's attach this policy document to the KMS key:

aws kms put-key-policy --key-id $KEY_ID --policy file://key-policy.json

➤ Finally, let's create a new storage class using the KMS key:

cat >~/environment/ebs-kms-sc.yaml <<EOF
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: eks-auto-ebs-kms-sc
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: ebs.csi.eks.amazonaws.com
volumeBindingMode: WaitForFirstConsumer
parameters:
  type: gp3
  encrypted: "true"
  kmsKeyId: $KEY_ID
EOF

kubectl apply -f ~/environment/ebs-kms-sc.yaml

➤ Let's inspect the StorageClass we just created:

kubectl describe storageclass eks-auto-ebs-kms-sc

This should produce the following output:

Name:            eks-auto-ebs-kms-sc
IsDefaultClass:  Yes
Annotations:     kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"},"name":"eks-auto-ebs-kms-sc"},"parameters":{"encrypted":"true","kmsKeyId":"2d61cc69-7f98-474d-a663-b682872a9f6a","type":"gp3"},"provisioner":"ebs.csi.eks.amazonaws.com","volumeBindingMode":"WaitForFirstConsumer"}
,storageclass.kubernetes.io/is-default-class=true
Provisioner:           ebs.csi.eks.amazonaws.com
Parameters:            encrypted=true,kmsKeyId=2d61cc69-7f98-474d-a663-b682872a9f6a,type=gp3
AllowVolumeExpansion:  <unset>
MountOptions:          <none>
ReclaimPolicy:         Delete
VolumeBindingMode:     WaitForFirstConsumer
Events:                <none>
We can make the following observations:

eks-auto-ebs-kms-sc is configured as the default storage class.
The associated provisioner is ebs.csi.eks.amazonaws.com. This provisioner is automatically made available by EKS Auto Mode.
The EBS volume type is set to gp3 (defaults to 3000 IOPS).
The ReclaimPolicy is set to Delete, which means that when the associated PVC is deleted, this results in the deletion of both the PV object in Kubernetes, as well as the associated storage asset in the external infrastructure.
The VolumeBindingMode  is set to WaitForFirstConsumer. This mode delays the binding and provisioning of a PersistentVolume until a Pod using the PersistentVolumeClaim is created. PersistentVolumes will be selected or provisioned conforming to the topology that is specified by the pod's scheduling constraints. This is needed in situations where storage backends are topology-constrained and not globally accessible from all Nodes in the cluster (as would be the case where multiple AZs are used).
Encryption is configured for volumes using the KMS key we created.
Update the catalog MySQL database Pod
Now that we have a default StorageClass in place, let's update the catalog service to use it. Since many of StatefulSet fields, including volumeClaimTemplates, cannot be modified, we will uninstall the catalog component and then reinstall it, so we can update the volume type from emptyDir to a Persistent Volume.

➤ First, uninstall the current catalog service:

helm uninstall retail-store-app-catalog

Re-launch the catalog service using:

helm upgrade -i retail-store-app-catalog oci://public.ecr.aws/aws-containers/retail-store-sample-catalog-chart --version ${RETAIL_STORE_APP_HELM_CHART_VERSION} -f - <<EOF
app:
  persistence:
    provider: mysql
    endpoint: ""
    database: "catalog"

    secret:
      create: true
      name: catalog-db
      username: catalog
      password: "mysqlcatalog123"

mysql:
  create: true
  persistentVolume:
    enabled: true
    accessMode:
      - ReadWriteOnce
    size: 30Gi
EOF

Verify that the PVC for the catalog MySQL database has been created
The re-launched catalog service should now have an associated PersistentVolumeClaim.

➤ We can see this by running:

kubectl describe statefulset retail-store-app-catalog-mysql

This time the output shows:

Name:               retail-store-app-catalog-mysql
Namespace:          default
...
  Containers:
   mysql:
    Image:      public.ecr.aws/docker/library/mysql:8.0
    Port:       3306/TCP
    Host Port:  0/TCP
    Environment:
      MYSQL_ROOT_PASSWORD:  my-secret-pw
      MYSQL_DATABASE:       catalog
      MYSQL_USER:           <set to the key 'username' in secret 'catalog-db'>  Optional: false
      MYSQL_PASSWORD:       <set to the key 'password' in secret 'catalog-db'>  Optional: false
    Mounts:
      /var/lib/mysql from data (rw)
  Volumes:         <none>
  Node-Selectors:  <none>
  Tolerations:     <none>
Volume Claims:
  Name:          data
  StorageClass:
  Labels:        <none>
  Annotations:   <none>
  Capacity:      30Gi
Let's inspect the PVC resource.

➤ To list all PVCs use:

kubectl get pvc

You should see:

NAME                                    STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          VOLUMEATTRIBUTESCLASS   AGE
data-retail-store-app-catalog-mysql-0   Bound    pvc-45a9e6ab-ed7c-47e1-9576-c3f01f33d327   30Gi       RWO            eks-auto-ebs-csi-sc   <unset>                 4h43m
data-retail-store-app-catalog-mysql-0 is the PVC created for the MySQL DB.

➤ Let's inspect it using:

kubectl describe pvc data-retail-store-app-catalog-mysql-0

This should produce the following output:

Name:          data-retail-store-app-catalog-mysql-0
Namespace:     default
StorageClass:  eks-auto-ebs-kms-sc
Status:        Bound
Volume:        pvc-c5c4dec1-0ce5-4a00-982d-233c7d5bfdbb
Labels:        ...
Annotations:   pv.kubernetes.io/bind-completed: yes
               pv.kubernetes.io/bound-by-controller: yes
               volume.beta.kubernetes.io/storage-provisioner: ebs.csi.eks.amazonaws.com
               volume.kubernetes.io/selected-node: i-04a55c853d8acf24f
               volume.kubernetes.io/storage-provisioner: ebs.csi.eks.amazonaws.com
Finalizers:    [kubernetes.io/pvc-protection]
Capacity:      30Gi
Access Modes:  RWO
VolumeMode:    Filesystem
Used By:       retail-store-app-catalog-mysql-0
Events:        <none>
From the output we can see that the PVC is bound to a specific PV (in this case pvc-c5c4dec1-0ce5-4a00-982d-233c7d5bfdbb), and that this has been provisioned using ebs.csi.eks.amazonaws.com with a capacity of 30Gi as specified in values-catalog.yaml.

➤ We can also inspect the PV as follows:

kubectl describe pv $(kubectl get pvc data-retail-store-app-catalog-mysql-0 -o jsonpath="{.spec.volumeName}")

The output should be similar to the below:

Name:              pvc-c5c4dec1-0ce5-4a00-982d-233c7d5bfdbb
Labels:            <none>
Annotations:       pv.kubernetes.io/provisioned-by: ebs.csi.eks.amazonaws.com
                   volume.kubernetes.io/provisioner-deletion-secret-name:
                   volume.kubernetes.io/provisioner-deletion-secret-namespace:
Finalizers:        [external-provisioner.volume.kubernetes.io/finalizer kubernetes.io/pv-protection external-attacher/ebs-csi-eks-amazonaws-com]
StorageClass:      eks-auto-ebs-kms-sc
Status:            Bound
Claim:             default/data-retail-store-app-catalog-mysql-0
Reclaim Policy:    Delete
Access Modes:      RWO
VolumeMode:        Filesystem
Capacity:          30Gi
Node Affinity:
  Required Terms:
    Term 0:        topology.kubernetes.io/zone in [us-west-2a]
Message:
Source:
    Type:              CSI (a Container Storage Interface (CSI) volume source)
    Driver:            ebs.csi.eks.amazonaws.com
    FSType:            ext4
    VolumeHandle:      vol-05938b7ff13ef8ebf
    ReadOnly:          false
    VolumeAttributes:      storage.kubernetes.io/csiProvisionerIdentity=1735365022188-8218-ebs.csi.eks.amazonaws.com
Events:                <none>
The VolumeHandle references the Amazon EBS Volume ID associated with the PV. Let's use this to inspect the EBS volume and check that it has been created correctly.

Verify that the EBS volume for the catalog MySQL database has been created correctly
➤ Obtain the underlying Amazon EBS Volume ID as follows:

MYSQL_PV_NAME=$(kubectl get pvc data-retail-store-app-catalog-mysql-0 -o jsonpath="{.spec.volumeName}")
MYSQL_EBS_VOL_ID=$(kubectl get pv $MYSQL_PV_NAME -o jsonpath="{.spec.csi.volumeHandle}")
echo EBS Volume ID: $MYSQL_EBS_VOL_ID

➤ Display the details for the EBS volume:

aws ec2 describe-volumes --volume-ids $MYSQL_EBS_VOL_ID --query Volumes[0]

Note the following section of the output, proving that KMS encryption is enabled with the correct key:

    ...
    "Encrypted": true,
    "KmsKeyId": "arn:aws:kms:us-west-2:845041152230:key/2d61cc69-7f98-474d-a663-b682872a9f6a",
    ...
    "Iops": 3000,
    ...
Note also that the Iops attribute is set to the gp3 default of 3000.

Summary
In this section, we performed the following steps:

Created a KMS key.
Attached a key policy that enables EC2 instances in the account to use the KMS key for encrypting EBS volumes.
Created a default StorageClass configured to use this key for encryption, and using a standard gp3 volume.
Updated the configuration of the catalog service to use the default StorageClass for creating a persistent volume.
Verified that the EBS volume was created correctly, using the correct KMS key and also the default IOPS setting for gp3 volumes.
In the next section we will add a second StorageClass which provisions higher IOPS for gp3 volumes, and explicitly configures the orders PostgreSQL database to use this.



Multiple EBS Storage Classes
Overview | Update StatefulSet configuration | Summary

Overview
In the previous section we created a default StorageClass and then configured the catalog service to use this to create a PV for its MySQL database.

The default StorageClass we created uses a KMS key to encrypt volumes, and also uses the default gp3 setting for IOPS (which we verified to be 3000).

Now suppose we were expecting a higher volume of traffic on the orders database, and needed to increase the IOPS accordingly.

In this section, we will create a second StorageClass that provisions an encrypted gp3 volume with 6000 IOPS, and then explicitly configure the orders PostgreSQL StatefulSet to use a PV based on this StorageClass.

Update StatefulSet configuration
Create a second StorageClass with 6000 IOPS
➤ Create a StorageClass using the same KMS key from the previous section, but also specifying an IOPS value:

cat >~/environment/ebs-iops-kms-sc.yaml <<EOF
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: eks-auto-ebs-iops-kms-sc
provisioner: ebs.csi.eks.amazonaws.com
volumeBindingMode: WaitForFirstConsumer
parameters:
  type: gp3
  iops: "6000"
  encrypted: "true"
  kmsKeyId: $KEY_ID
EOF

kubectl apply -f ~/environment/ebs-iops-kms-sc.yaml

Note that this time we did not include an annotation to make this the default StorageClass. This means we will need to reference this StorageClass explicitly in order to use it.

Update the orders PostgreSQL database pod
Since we can't update some of the StatefulSet fields such as the persistentVolumeClaimRetentionPolicy, we will first have to uninstall the current version of the orders service, and reinstall it again.

➤ Execute the following command:

helm uninstall retail-store-app-orders

➤ Create a values-orders.yaml as follows:

helm upgrade -i retail-store-app-orders oci://public.ecr.aws/aws-containers/retail-store-sample-orders-chart \
  --version ${RETAIL_STORE_APP_HELM_CHART_VERSION} -f - <<EOF
app:
  persistence:
    provider: postgres
    endpoint: ""
    database: "orders"

    secret:
      create: true
      name: orders-db
      username: orders
      password: "postgres123"

postgresql:
  create: true
  persistentVolume:
    enabled: true
    accessMode:
      - ReadWriteOnce
    size: 20Gi
    storageClass: eks-auto-ebs-iops-kms-sc
EOF

➤ Wait until the orders service is up and running again (it may restart and take a minute):

kubectl wait --for=condition=Ready pod -l app.kubernetes.io/instance=retail-store-app-orders --namespace default --timeout=300s

Verify that the PVC for the orders PostgreSQL database has been created
➤ List all PVCs:

kubectl get pvc

You should see:

NAME                                        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS               VOLUMEATTRIBUTESCLASS   AGE
data-retail-store-app-catalog-mysql-0       Bound    pvc-c5c4dec1-0ce5-4a00-982d-233c7d5bfdbb   30Gi       RWO            eks-auto-ebs-kms-sc        <unset>                 93m
data-retail-store-app-orders-postgresql-0   Bound    pvc-b1e886af-a019-4d98-adb0-d321e5dcab80   20Gi       RWO            eks-auto-ebs-iops-kms-sc   <unset>                 6m55s
data-retail-store-app-orders-postgresql-0 is the PVC created for the PostgreSQL DB.

➤ Inspect it using:

kubectl describe pvc data-retail-store-app-orders-postgresql-0

➤ You can also inspect the PV as follows:

kubectl describe pv $(kubectl get pvc data-retail-store-app-orders-postgresql-0 -o jsonpath="{.spec.volumeName}")

Verify that the EBS volume for the orders PostgreSQL database has been created correctly
Let us verify that the IOPS attribute has been set correctly.

➤ Obtain the underlying AWS EBS Volume ID as follows:

PGSQL_PV_NAME=$(kubectl get pvc data-retail-store-app-orders-postgresql-0 -o jsonpath="{.spec.volumeName}")
PGSQL_EBS_VOL_ID=$(kubectl get pv $PGSQL_PV_NAME -o jsonpath="{.spec.csi.volumeHandle}")
echo EBS Volume ID: $PGSQL_EBS_VOL_ID

➤ Display the details for the EBS volume:

aws ec2 describe-volumes --volume-ids $PGSQL_EBS_VOL_ID --query Volumes[0]

Note the following sections of the output, proving that IOPS is set to 6000 and KMS encryption is enabled with the correct key:

    ...
    "Encrypted": true,
    "KmsKeyId": "arn:aws:kms:us-west-2:845041152230:key/2d61cc69-7f98-474d-a663-b682872a9f6a",
    "Size": 20,
    ...
    "Iops": 6000,
    ...
Summary
In this section, we performed the following steps:

Created a StorageClass configured to use a KMS key for encryption, and using a standard gp3 volume with 6000 provisioned IOPS.
Updated the configuration of the orders service to use this new StorageClass for creating a persistent volume.
Verified that the EBS volume was created correctly, using the correct KMS key and also the specified provisioned IOPS setting.



Module 3 - Cluster Upgrades
Understanding Kubernetes upgrades in EKS
Amazon Elastic Kubernetes Service (EKS) requires careful planning for upgrades. The upstream Kubernetes project (Kubernetes Releases ) undergoes continuous improvement, with regular updates introducing new functionalities, design enhancements, and bug corrections. Minor version releases typically occur every four months, and each version receives community support for approximately 1 year  following its launch.

Why keep EKS updated?
Maintaining current Amazon EKS versions is crucial for:

Security: Protecting Kubernetes clusters against vulnerabilities
Stability: Ensuring reliable performance and compatibility
Innovation: Accessing the latest platform features and capabilities
For detailed information about EKS versions and updates, refer to the Amazon EKS Kubernetes versions documentation .

EKS Auto Mode: enhanced shared responsibility
A Kubernetes deployment consists of both control plane and data plane components (worker nodes).

Amazon EKS has always managed the Kubernetes control plane and upgrades. Before Amazon EKS Auto Mode, cluster owners were responsible for initiating upgrades for both the control plane and data plane. This includes upgrading worker nodes in Self Managed node groups, Managed Node Groups, and other add-ons.

Amazon EKS Auto Mode represents a significant advancement in Kubernetes cluster management, expanding AWS's responsibility to include the automatic upgrading of cluster infrastructure. This enhancement covers both worker nodes and core cluster capabilities, substantially reducing the operational burden on customers.

To illustrate the shared responsibility model under Amazon EKS Auto Mode, please refer to the image below: Shared responsibility model with auto mode

Component Management
Red areas: represent components fully managed by AWS
Green areas: indicate the aspects that remain under customer management
Kubernetes Version Structure
From the Kubernetes versioning documentation : Versions are expressed as x.y.z, where:

x: Major version
y: Minor version (Released every ~4 months)
z: Patch version (Monthly releases)
The Kubernetes version indicates both control-plane (apiserver, controller-manager, etc.) and data-plane (e.g., kubelet, kube-proxy, etc.) components.

Amazon EKS Version Management
Amazon EKS provides a managed Kubernetes control plane with a range of supported versions :

Standard Support: 14 months
Extended Support: Additional 12 months (optional with additional costs, which can be disabled )
Version Compatibility: Data-plane components (e.g., kubelet, kube-proxy) should match the control plane minor version but can also stay on lower versions up to the allowed version skew policy . When using node auto-scalers such as Karpenter, the AMI minor version can be automatically discovered to match the EKS Control plane version.
Amazon EKS platform versions - Kubernetes control-plane patch versions upgrade
Amazon EKS periodically releases new platform versions  to enable new control plane settings and provide security fixes. Each Kubernetes minor version may have multiple associated platform versions. These EKS Platform versions are automatically upgraded in a gradual rollout process with no manual intervention required from the customer. Importantly, new Amazon EKS platform versions don't introduce breaking changes or cause service interruptions.

Staying current with the latest Kubernetes minor version is crucial for maintaining a secure and efficient EKS environment. This approach aligns with the shared responsibility model in Amazon EKS, ensuring that clusters run with the latest security patches and bug fixes, thereby reducing security vulnerabilities. Additionally, it offers improved performance, scalability, and reliability, ultimately providing better service to applications and customers.

Example
If a cluster runs Kubernetes 1.25, AWS might update the platform version from eks.1 to eks.2 automatically to apply security patches while maintaining the same Kubernetes version.

Add-ons Management
The last part of an upgrade process is to upgrade the add-ons  which are system wide applications that provide additional capabilities for other business applications that run on the cluster. For example:

Observability tools
Security implementations
Networking solutions
Storage integrations
Amazon EKS provides additional guidance on upgrades as part of the EKS best practices guide which you can find in the following link 

After understanding the Kubernetes version cadence and compatibility, next we'll experiment with how Amazon EKS Auto Mode manages zero touch upgrades of cluster infrastructure that includes nodes and the core capabilities.


Understanding upgrades with Amazon EKS Auto Mode
Overview | Upgrade flow process | Time controls | Summary

Overview
Cluster upgrade flow Amazon EKS Auto Mode revolutionizes Kubernetes cluster management by providing zero-touch updates for our entire cluster infrastructure.

Upgrade flow process
1. Version Verification
Check the latest available Amazon EKS version and verify compatibility with current applications
Initiate a version update when needed
2. Control Plane and cluster component updates
Initiate control plane version upgrade
With Auto Mode, any of the managed cluster capabilities will also be automatically updated to ensure compatibility across versions
3. Data Plane Update
Node rolling update Graceful node updates follow this process:

Node replacement with latest AMIs using Karpenter's Drift management capability
Rolling update strategy
Respect for Pod Disruption Budgets (PDBs)
Time controls
Expiration Settings
Default: worker nodes will be fully automatically upgraded no later than 14 days
Custom NodePools: Up to 21 days
Termination Grace Period
Amazon EKS Auto Mode sets a default of 24 hours for terminationGracePeriod. This period defines the amount of time a Node can be drained before Karpenter forcibly cleans up the node. During draining, pods blocking eviction (such as those with PDBs and do-not-disrupt annotations) will be respected until the terminationGracePeriod is reached, after which those pods will be forcibly deleted.

Disruption Controls
We can limit the rate at which Amazon EKS Auto Mode disrupts nodes through the NodePool's spec.disruption.budgets. Disruption budgets allow us to control the rate of worker node upgrades or ensure upgrades only happen during specific dates and times (using schedule).

Disruption budgets can be configured for the following disruption reasons:

Empty - when a node is has no running pods
Underutilized - when a node can be removed or replaced with a smaller instance type
Drifted - when a node has diverged from its desired state (for example when the EKS control plane version differs from worker node version)
By default, the Amazon EKS Auto Mode general-purpose NodePool has the following disruption budget:

...
spec:
  disruption:
    consolidateAfter: 0s
    consolidationPolicy: WhenEmptyOrUnderutilized
    budgets:
    - nodes: 10%
...
With this budget, Amazon EKS Auto Mode will disrupt at most 10% of active nodes at a time.

...
spec:
  disruption:
    consolidateAfter: 0s
    consolidationPolicy: WhenEmptyOrUnderutilized
    budgets:
    - nodes: 10%
    - schedule: 0 9 * * mon-fri
      duration: 8h
      nodes: "0"
      reasons:
      - Drifted
...
With this configuration, we limit Amazon EKS Auto Mode from disrupting nodes ("0") during weekday business hours if Drift is triggered, while allowing at most 10% of active nodes to be disrupted at all other times.

Summary
Now that we understand how Amazon EKS Auto Mode handles upgrades, let's proceed to upgrading the cluster to the most recent Kubernetes version.



Upgrading the cluster
Upgrade | Summary

Upgrade the cluster
In this section of the workshop, we'll gain hands-on experience with Amazon EKS Auto Mode's automatic cluster infrastructure updates. We'll begin by updating the control plane.

Check Current Versions
➤ First, let's check the current Amazon EKS control plane version:

aws eks describe-cluster --region $AWS_REGION --name $DEMO_CLUSTER_NAME --query "cluster.version" --output text

This is the output:

1.32
We can see that our Amazon EKS control plane is running Amazon EKS version 1.32.

➤ Now, let's observe the current worker nodes (data plane) version:

kubectl get nodes

The output should be similar to the following:

NAME                  STATUS   ROLES    AGE   VERSION
i-01c27cd2a458478a7   Ready    <none>   78m   v1.32.3-eks-156189c
i-06a7bbee7d6f29f45   Ready    <none>   56m   v1.32.3-eks-156189c
...
We can see that the worker nodes are also running Amazon EKS version 1.32.

Look for deprecated APIs and new versions for additional components
Under the shared responsibility model, we should review the Kubernetes release  page, and for each version we're upgrading to, verify that there are no API deprecations or changes that might affect our business applications.

Additionally, we need to check for new versions of any 3rd party or community add-ons installed in our cluster, and upgrade them after completing the EKS cluster upgrade.

Upgrade the control plane
Let's upgrade our Amazon EKS control plane version to 1.33.

Note: While there are multiple ways to upgrade an Amazon EKS control plane (using eksctl, AWS Console, AWS CLI, or Infrastructure as Code), we'll use the AWS CLI for simplicity in this workshop.

➤ Execute the following command:

aws eks update-cluster-version --region $AWS_REGION --name $DEMO_CLUSTER_NAME --kubernetes-version 1.33

You will see output similar to this:

{
    "update": {
        "id": "7e15217f-3ed4-3eaa-91e2-b1fc05bd1c89",
        "status": "InProgress",
        "type": "VersionUpdate",
        "params": [
            {
                "type": "Version",
                "value": "1.33"
            },
            {
                "type": "PlatformVersion",
                "value": "eks.4"
            }
        ],
        "createdAt": "2025-06-08T16:32:24.518000+00:00",
        "errors": []
    }
}
In the output, we can see the Amazon EKS version is being updated to 1.33. Also note the PlatformVersion parameter which is documented in the Amazon EKS platform version  documentation. At the time of creating this workshop, this output is eks.4, but it may differ in future platform versions of Amazon EKS.

Note
The control plane upgrade process typically takes about 10 minutes. Depending on your available time, we advise you to either take a small break here, or progress to the Migration patterns module, and get back to check the upgrade once you're done.
Summary
In this section we've initiated an upgrade of the Amazon EKS Auto Mode cluster control plane from version 1.32 to 1.33. With Auto Mode, AWS will handle the worker node upgrades automatically, demonstrating the zero-touch infrastructure update capability that makes Kubernetes operations simpler.

Checking the upgrade
Check the upgrade | Summary

Check the upgrade
Note that upgrading the cluster may take several minutes.

➤ Using the aws eks describe-cluster AWS CLI command, we will check if the upgrade process is complete, and our cluster version is now 1.33. If it's not, please allow additional time for the upgrade process to complete.

aws eks describe-cluster --region $AWS_REGION --name $DEMO_CLUSTER_NAME --query "cluster.version" --output text

1.33
The control plane is now running Amazon EKS version 1.33.

➤ Now let's verify the worker nodes version:

kubectl get nodes

NAME                  STATUS   ROLES    AGE     VERSION
i-01774cb623e95980b   Ready    <none>   6m33s   v1.33.0-eks-987fa8d
...
We can see that the worker nodes have also been upgraded to Amazon EKS version 1.33.

Amazon EKS Auto Mode has automatically detected that the EKS control plane version was upgraded, and it triggered node replacement through its drift detection mechanism. Let's examine the related events that occurred during this process.

➤ Execute the following command to view the drift-related events:

kubectl get events | grep Drifted

19m         Normal    DisruptionLaunching              nodeclaim/advanced-networking-vpthn                               Launching NodeClaim: Drifted
32s         Normal    DisruptionTerminating            nodeclaim/advanced-networking-vpthn                               Disrupting NodeClaim: Drifted
19m         Normal    DisruptionTerminating            node/i-03e6de8867bb6e67e                                          Disrupting Node: Drifted
2m2s        Normal    DisruptionTerminating            node/i-04a21a34422cd8e7d                                          Disrupting Node: Drifted
...
This output shows the orchestrated node replacement process in action. Amazon EKS Auto Mode follows a carefully sequenced approach:

First, it launches new replacement nodes
Then it evicts pods from the old nodes
It waits until the new replacement nodes are in the Ready state before terminating the old nodes
When Pod Disruption Budgets (PDBs) are configured, Auto Mode respects these budgets by using a backoff retry eviction strategy. Pods will never be forcibly deleted during normal operation. However, pods that fail to shut down will prevent a node from being deleted until the terminationGracePeriod is reached, at which point those pods will be forcibly deleted.

Summary
Congratulations! In this section, we have successfully updated our cluster to the latest version of Kubernetes. We've observed how Amazon EKS Auto Mode automatically manages the worker node upgrades to match the control plane version, demonstrating the zero-touch infrastructure update capabilities that simplify Kubernetes operations.

Module 4 - Migrate an Amazon EKS Cluster to EKS Auto Mode
In this module, we will explore migration strategies from an existing Amazon EKS cluster to EKS Auto Mode.

We will use an existing Amazon EKS cluster with several different compute options for the worker nodes that host the cluster's applications and operational software, and gradually migrate from each of these options to Amazon EKS Auto Mode.

Table of Contents
Cluster configuration
Enabling EKS Auto Mode
Migrating from AWS Fargate
Migrating from EKS Managed Node Groups
Migrating from a Self-Managed Karpenter
Migrating Networking Resources
Migrating Stateful Components
Removing non-EKS Auto Mode Add-ons and Compute
Cluster configuration
Setup | Overview | Cluster Add-ons | Self-Managed Karpenter Configuration | Demo Application

Setup
During the provisioning of this workshop we've created several Amazon EKS clusters, with one specifically intended for this module.

➤ Before we begin, let's switch the current kubeconfig context by executing:

kubectl config use-context arn:aws:eks:${AWS_REGION}:${AWS_ACCOUNT_ID}:cluster/${MIGRATION_CLUSTER_NAME}

You should receive an output similar to:

Switched to context "<a cluster ARN>".
Verify the cluster setup by executing the following command:

kubectl get nodes -o json | jq -r '.items[].metadata.labels | ."kubernetes.io/hostname" + " | " + ."topology.kubernetes.io/zone" + " | " + (."eks.amazonaws.com/capacityType" // if ."eks.amazonaws.com/compute-type" == "fargate" then "FARGATE" else "KARPENTER" end) + " | " + (."eks.amazonaws.com/nodegroup" // if ."karpenter.sh/nodepool" then "nodepool: " + ."karpenter.sh/nodepool" else "profile: apps" end)' | column -t | sort -k 5

The command above shows the distribution of compute instances in the cluster across compute options and should look similar to the following:

fargate-ip-192-168-116-56.us-west-2.compute.internal   |  us-west-2c  |  FARGATE    |  profile:    apps
fargate-ip-192-168-180-168.us-west-2.compute.internal  |  us-west-2b  |  FARGATE    |  profile:    apps
fargate-ip-192-168-191-115.us-west-2.compute.internal  |  us-west-2b  |  FARGATE    |  profile:    apps
ip-192-168-100-194.us-west-2.compute.internal          |  us-west-2c  |  KARPENTER  |  nodepool:   apps
ip-192-168-155-246.us-west-2.compute.internal          |  us-west-2a  |  KARPENTER  |  nodepool:   apps
...
ip-192-168-173-151.us-west-2.compute.internal          |  us-west-2b  |  KARPENTER  |  nodepool:   apps
ip-192-168-28-67.us-west-2.compute.internal            |  us-west-2c  |  ON_DEMAND  |  apps-mng
ip-192-168-51-140.us-west-2.compute.internal           |  us-west-2a  |  ON_DEMAND  |  apps-mng
ip-192-168-83-58.us-west-2.compute.internal            |  us-west-2b  |  ON_DEMAND  |  apps-mng
ip-192-168-26-158.us-west-2.compute.internal           |  us-west-2c  |  ON_DEMAND  |  system-mng  
ip-192-168-33-46.us-west-2.compute.internal            |  us-west-2a  |  ON_DEMAND  |  system-mng  
ip-192-168-86-255.us-west-2.compute.internal           |  us-west-2b  |  ON_DEMAND  |  system-mng  
Note that self-managed Karpenter nodes distribution may differ due to the dynamic nature of Karpenter provision and consolidation (hence ... in the output above), but the nodes should follow the same approximate distribution.

Overview
The compute options in the cluster, as shown in the output above, include:

An AWS Fargate profile  that allows us to target specific applications to be scheduled on AWS Fargate . These are the lines marked by profile: apps.
Several Amazon EKS managed node groups  that host the cluster operational software and some of the applications. These are the lines marked by system-mng for the cluster-critical managed node group and apps-mng for the applications' managed node group.
Self-managed Karpenter  nodes that host the rest of the applications in the cluster. These are marked by nodepool: apps.
The cluster also contains the required Amazon EKS add-ons :

The Amazon VPC CNI plugin for Kubernetes  add-on, which provides native VPC networking for the cluster
The CoreDNS  add-on, which serves as the Kubernetes cluster DNS server
The Kube-proxy  add-on, which maintains network rules on each Amazon EC2 worker node
The EKS Pod Identity Agent  add-on, which manages AWS credentials for the cluster applications
In addition, the cluster includes a self-managed AWS Load Balancer Controller , that provisions and configures Elastic Load Balancers (Network Load Balancers  for Service resources and Application Load Balancers  for Ingress resources), to expose the cluster applications to traffic.

➤ We can validate that all the applications and controllers are operational (in a Running state) by executing:

kubectl get pods -A

Note that due to the provision process, some Pods may have a non-zero RESTARTS count. As long as these aren't recent (dozens of minutes ago), your cluster is in the desired state.

Cluster Add-ons
The cluster add-ons mentioned above are either DaemonSets (VPC CNI, kube-proxy, and Pod Identity agent) or Deployments (CoreDNS, Karpenter, and Application Load Balancer Controller) and thus their required compute capacity is handled differently.

The DaemonSets will be deployed on every worker node in the cluster, while the rest will be deployed on a specifically configured system-mng managed node group.

The system-mng node group is provisioned along with the cluster and its configuration can be represented, for reference, by the following CloudFormation snippet:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
...
  SystemNodegroup:
    Type: AWS::EKS::Nodegroup
    Properties:
      NodegroupName: system-mng
      ...
      AmiType: AL2023_ARM_64_STANDARD
      NodeRole: !GetAtt NodeRole.Arn
      InstanceTypes:
        - t4g.small
      ScalingConfig:
        MinSize: 0
        DesiredSize: 3
        MaxSize: 3
      Labels:
        role: system-mng
      Taints:
        - Key: role
          Value: system-mng
          Effect: NO_SCHEDULE
      Subnets:
        ...
...
Note the role=system label and the role=system taint in the configuration of the managed node group. These are set to ensure that only the selected software/applications can be scheduled onto this node group's instances.

The relevant add-ons then define a matching toleration and target the node group above using a matching node selector.

➤ View the node group and its configuration in the AWS EKS console  and navigating to the workshop migration cluster, which should look like this:

 system-mng Managed Node Group Labels 

➤ Verify the relevant, non-DaemonSet add-ons deployment by executing the following command:

export SYSTEM_NODES=$(kubectl get nodes -o json | jq -r '[.items[].metadata.labels | select(."eks.amazonaws.com/nodegroup" == "system-mng") | ."kubernetes.io/hostname"] | join("\\|")')
export DAEMONSETS_PODS=$(kubectl get ds -n kube-system -o json | jq -r '[.items[].metadata.name] | join ("\\|")')

kubectl get pods --all-namespaces -o wide | grep "${SYSTEM_NODES}" | grep -v "${DAEMONSETS_PODS}"

The output should show at least CoreDNS, Karpenter, and Application Load Balancer Controller, similar to the following:

kube-system   aws-load-balancer-controller-7df858d998-g95mt   1/1     Running   0          7h44m   192.168.174.154   ip-192-168-171-81.us-west-2.compute.internal            <none>           <none>
kube-system   aws-load-balancer-controller-7df858d998-lt62r   1/1     Running   0          7h44m   192.168.126.38    ip-192-168-121-172.us-west-2.compute.internal           <none>           <none>
kube-system   coredns-7d597b58bc-5ftmq                        1/1     Running   0          7h59m   192.168.122.77    ip-192-168-121-172.us-west-2.compute.internal           <none>           <none>
kube-system   coredns-7d597b58bc-rjts4                        1/1     Running   0          7h59m   192.168.122.238   ip-192-168-121-172.us-west-2.compute.internal           <none>           <none>
kube-system   ebs-csi-controller-86cbbcbb67-cltl2             6/6     Running   0          7h46m   192.168.142.236   ip-192-168-150-45.us-west-2.compute.internal            <none>           <none>
kube-system   ebs-csi-controller-86cbbcbb67-rzvks             6/6     Running   0          7h46m   192.168.168.215   ip-192-168-171-81.us-west-2.compute.internal            <none>           <none>
kube-system   karpenter-df586dcf5-4rdrv                       1/1     Running   0          7h44m   192.168.184.184   ip-192-168-171-81.us-west-2.compute.internal            <none>           <none>
kube-system   karpenter-df586dcf5-slc6k                       1/1     Running   0          7h44m   192.168.154.243   ip-192-168-150-45.us-west-2.compute.internal            <none>           <none>
kube-system   metrics-server-76dfb8cb4b-kfkjw                 1/1     Running   0          7h59m   192.168.103.3     ip-192-168-121-172.us-west-2.compute.internal           <none>           <none>
kube-system   metrics-server-76dfb8cb4b-xlj6b                 1/1     Running   0          7h59m   192.168.122.113   ip-192-168-121-172.us-west-2.compute.internal           <none>           <none>
Self-Managed Karpenter Configuration
To allow the self-managed Karpenter to provision instances for our applications' Pods, we need to define at least one NodeClass and one NodePool.

The configuration applied to the cluster can be represented, for reference, by the following partial snippet:

EC2NodeClass:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
apiVersion: karpenter.k8s.aws/v1
kind: EC2NodeClass
metadata:
  name: apps
spec:
  amiSelectorTerms:
    - alias: al2023@latest
  role: KarpenterNodeRole-${MIGRATION_CLUSTER_NAME}
  subnetSelectorTerms:
    - tags:
        karpenter.sh/discovery: ${MIGRATION_CLUSTER_NAME}
        Name: "*/SubnetPrivate*"
  securityGroupSelectorTerms:
    - tags:
        karpenter.sh/discovery: ${MIGRATION_CLUSTER_NAME}
NodePool:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: apps
spec:
  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 30s
  template:
    metadata:
      labels:
        role: apps-karpenter
    spec:
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: apps
      taints:
        - key: role
          value: apps-karpenter
          effect: NoSchedule
      requirements:
        - key: kubernetes.io/arch
          operator: In
          values: [amd64, arm64]
        - key: kubernetes.io/os
          operator: In
          values: [linux]
        - key: karpenter.sh/capacity-type
          operator: In
          values: [on-demand]
        - key: node.kubernetes.io/instance-category
          operator: In
          values: [c, m, r]
        - key: karpenter.k8s.aws/instance-generation
          operator: Gt
          values: ['4']
        - key: karpenter.k8s.aws/instance-size
          operator: NotIn
          values: [nano, micro, small, medium]
Note the role=apps-karpenter label and the role=apps-karpenter taint in the configuration of the node pool above. These are set, in the same manner as the system-mng node group above, to ensure that only the selected applications can be scheduled using this Karpenter node pool.

Demo Application
To demonstrate the migration process, we will use the demo retail application – a sample application designed to illustrate container-related concepts on AWS.

➤ We can explore the application by executing the command below, Ctrl/Cmd-clicking the URL in the output, and adding a couple of items to the cart:

kubectl get ingress -n apps retail-store-app-ui-main \
  -o jsonpath="http://{.status.loadBalancer.ingress[*].hostname}{'\n'}"

The application  contains several components in various languages and frameworks that use pre-built container images for both x86-64 and ARM64 CPU architectures:


The application components are deployed as follows:

Component	Compute Option
Orders	Fargate
Catalog	Managed Node Group
Catalog MySQL database	Managed Node Group
Checkout	Self-Managed Karpenter
Carts	Self-Managed Karpenter
UI	Self-Managed Karpenter
➤ Print out the distribution of instances across all capacity types by executing the following command:

kubectl get nodes -o json | jq -r '.items[].metadata.labels | ."kubernetes.io/hostname" + " | " + ."topology.kubernetes.io/zone" + " | " + (."eks.amazonaws.com/capacityType" // if ."eks.amazonaws.com/compute-type" == "fargate" then "FARGATE" else "KARPENTER" end) + " | " + (."eks.amazonaws.com/nodegroup" // if ."karpenter.sh/nodepool" then "nodepool: " + ."karpenter.sh/nodepool" else "profile: apps" end)' | column -t | sort -k 5

This should produce a result similar to the following:

fargate-ip-192-168-100-164.us-west-2.compute.internal  |  us-west-2a  |  FARGATE    |  profile:    apps
fargate-ip-192-168-138-127.us-west-2.compute.internal  |  us-west-2b  |  FARGATE    |  profile:    apps
fargate-ip-192-168-152-183.us-west-2.compute.internal  |  us-west-2b  |  FARGATE    |  profile:    apps
ip-192-168-103-234.us-west-2.compute.internal          |  us-west-2a  |  KARPENTER  |  nodepool:   apps
ip-192-168-131-142.us-west-2.compute.internal          |  us-west-2b  |  KARPENTER  |  nodepool:   apps
ip-192-168-190-197.us-west-2.compute.internal          |  us-west-2c  |  KARPENTER  |  nodepool:   apps
ip-192-168-104-189.us-west-2.compute.internal          |  us-west-2a  |  ON_DEMAND  |  apps-mng
ip-192-168-132-240.us-west-2.compute.internal          |  us-west-2b  |  ON_DEMAND  |  apps-mng
ip-192-168-163-151.us-west-2.compute.internal          |  us-west-2c  |  ON_DEMAND  |  apps-mng
ip-192-168-121-172.us-west-2.compute.internal          |  us-west-2a  |  ON_DEMAND  |  system-mng  
ip-192-168-150-45.us-west-2.compute.internal           |  us-west-2b  |  ON_DEMAND  |  system-mng  
ip-192-168-171-81.us-west-2.compute.internal           |  us-west-2c  |  ON_DEMAND  |  system-mng  
➤ Verify that all the components of the application are deployed as described in the table above:

export DAEMONSETS_PODS=$(kubectl get ds -n kube-system -o json | jq -r '[.items[].metadata.name] | join ("\\|")')
kubectl get pods -n apps -o wide | grep -v "${DAEMONSETS_PODS}"

Now that we have an overall view of the application, we can start the migration process by enabling the EKS Auto Mode for the cluster.

Enabling EKS Auto Mode
Configuring the Cluster | Configuring EKS Auto Mode | Preparing the Application

Configuring the Cluster
Before enabling EKS Auto Mode on our cluster, we need to adjust certain IAM permissions to ensure its proper operation. This includes:

Updating the cluster's IAM role trust policy and permissions
Creating an IAM role for the EKS Auto Mode worker nodes
Let's explore these permissions in more detail in the following sections.

1. Update the Cluster IAM Role
EKS Auto Mode includes several Kubernetes capabilities as core components. These components, that would otherwise have to be managed as add-ons or self-managed controllers, include built-in support for Pod IP address assignments, Pod network policies, local DNS services, GPU plug-ins, health checkers, and EBS CSI storage.

To manage these components and ensure their function, EKS Auto Mode requires additional permissions, which are now a part of the cluster IAM role.

As described in the EKS Auto Mode documentation , the following IAM policies should be added to the cluster role, in addition to the existing AmazonEKSClusterPolicy:

AmazonEKSComputePolicy 
AmazonEKSBlockStoragePolicy 
AmazonEKSNetworkingPolicy 
AmazonEKSLoadBalancingPolicy 
In addition, EKS Auto Mode requires the sts:TagSession action to be added to the cluster IAM role's trust policy (you can view its current state in the IAM console).

➤ Create a trust policy JSON file:

cat << EOF > trust-policy.json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "Service": "eks.amazonaws.com"
            },
            "Action": [
                "sts:AssumeRole",
                "sts:TagSession"
            ]
        }
    ]
}
EOF

➤ Update the cluster IAM role and remove the trust-policy.json file:

aws iam update-assume-role-policy \
  --role-name ${MIGRATION_CLUSTER_ROLE_NAME} \
  --policy-document file://trust-policy.json

rm trust-policy.json

➤ Add the required permissions to the cluster IAM role by executing:

for POLICY_ARN in \
  "arn:aws:iam::aws:policy/AmazonEKSComputePolicy" \
  "arn:aws:iam::aws:policy/AmazonEKSBlockStoragePolicy" \
  "arn:aws:iam::aws:policy/AmazonEKSNetworkingPolicy" \
  "arn:aws:iam::aws:policy/AmazonEKSLoadBalancingPolicy"
do
  echo "Attaching policy ${POLICY_ARN} to IAM role ${MIGRATION_CLUSTER_ROLE_NAME}..."
  aws iam attach-role-policy --role-name ${MIGRATION_CLUSTER_ROLE_NAME} \
    --policy-arn ${POLICY_ARN}
done

➤ Verify that the IAM role has been updated properly:

aws iam get-role --role-name ${MIGRATION_CLUSTER_ROLE_NAME} | \
  jq -r '.Role.AssumeRolePolicyDocument.Statement[].Action[]'

aws iam list-attached-role-policies --role-name ${MIGRATION_CLUSTER_ROLE_NAME} | \
  jq -r '.AttachedPolicies[].PolicyName'

The output should include (note the AmazonEKSClusterPolicy that the role already had attached before):

sts:AssumeRole
sts:TagSession
AmazonEKSClusterPolicy
AmazonEKSNetworkingPolicy
AmazonEKSComputePolicy
AmazonEKSBlockStoragePolicy
AmazonEKSLoadBalancingPolicy
2. Create the Node IAM Role
A Node IAM role contains the minimal permissions required for EKS components (kubelet and Pod Identity agent) to perform their function.

➤ Create a trust policy JSON file:

cat << EOF > trust-policy.json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "Service": "ec2.amazonaws.com"
            },
            "Action": [
                "sts:AssumeRole"
            ]
        }
    ]
}
EOF

➤ Create the Node IAM role and remove the trust-policy.json file:

aws iam create-role \
  --role-name ${MIGRATION_CLUSTER_NAME}-auto-mode-node-role \
  --assume-role-policy-document file://trust-policy.json

rm trust-policy.json

➤ Attach the required policies:

for POLICY_ARN in \
  "arn:aws:iam::aws:policy/AmazonEKSWorkerNodeMinimalPolicy" \
  "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryPullOnly"
do
  echo "Attaching policy ${POLICY_ARN} to IAM role ${MIGRATION_CLUSTER_NAME}-auto-mode-node-role..."
  aws iam attach-role-policy --role-name ${MIGRATION_CLUSTER_NAME}-auto-mode-node-role \
    --policy-arn ${POLICY_ARN}
done

➤ Verify the created (or existing) role, its trust policy, and attached managed policies:

aws iam get-role \
  --role-name ${MIGRATION_CLUSTER_NAME}-auto-mode-node-role | \
  jq -r '.Role.AssumeRolePolicyDocument.Statement[].Action'

aws iam list-attached-role-policies \
  --role-name ${MIGRATION_CLUSTER_NAME}-auto-mode-node-role | \
  jq -r '.AttachedPolicies[].PolicyName'

The command above should produce the following output:

sts:AssumeRole
AmazonEKSWorkerNodeMinimalPolicy
AmazonEC2ContainerRegistryPullOnly
We can now enable EKS Auto Mode.

3. Enable Amazon EKS Auto Mode
➤ Define the node role ARN as an environment variable:

export NODE_ROLE_ARN=$(aws iam get-role \
  --role-name ${MIGRATION_CLUSTER_NAME}-auto-mode-node-role \
  --query "Role.Arn" --output text)

➤ Execute the following command to enable EKS Auto Mode (and verify that there are no errors in the output):

aws eks update-cluster-config \
  --region ${AWS_REGION} \
  --name ${MIGRATION_CLUSTER_NAME} \
  --compute-config "{\"nodeRoleArn\": \"${NODE_ROLE_ARN}\", \"nodePools\": [\"system\"], \"enabled\": true}" \
  --kubernetes-network-config '{"elasticLoadBalancing":{"enabled": true}}' \
  --storage-config '{"blockStorage":{"enabled": true}}'

➤ After 5 - 10 seconds, execute the following command to wait for the cluster EKS Auto Mode to become enabled:

aws eks wait cluster-active --name ${MIGRATION_CLUSTER_NAME}

You can also verify that the process of enabling EKS Auto Mode has started by navigating to the AWS EKS console , selecting the migration cluster, and ensuring that the corresponding panel looks like this (note the spinning icon on the Manage button):

Enabling EKS Auto Mode progress

After a couple of minutes, the cluster should enable EKS Auto Mode:

Enabled EKS Auto Mode

Before proceeding with the migration, let's make sure the Auto Mode autoscaling configuration exists in the cluster.

➤ Verify that all node pools (including those managed by the self-managed Karpenter) and node classes are fully operational (READY=True):

kubectl get nodepool,nodeclass,ec2nodeclass

You should see an output similar to the following, which shows the original self-managed Karpenter NodePool and EC2NodeClass, as well as the EKS Auto Mode NodePool and (a new CRD) NodeClass:

NAME                           NODECLASS   NODES   READY   AGE
nodepool.karpenter.sh/apps     apps        8       True    89m
nodepool.karpenter.sh/system   default     0       True    8m47s

NAME                                  ROLE                                    READY   AGE
nodeclass.eks.amazonaws.com/default   migration-cluster-auto-mode-node-role   True    8m47s

NAME                                  READY   AGE
ec2nodeclass.karpenter.k8s.aws/apps   True    89m
➤ Extract (if you already closed its tab in the browser) the application URL:

kubectl get ingress -n apps retail-store-app-ui-main \
  -o jsonpath="http://{.status.loadBalancer.ingress[*].hostname}{'\n'}"

➤ Verify that the application is operational by Ctrl/Cmd-clicking the URL in the output (or refreshing the tab).

We are now ready to start the migration process of our applications to EKS Auto Mode-managed instances.

Configuring EKS Auto Mode
EKS Auto Mode manages most infrastructure components automatically, while allowing customization to better fit the needs of the workloads.

EKS Auto Mode includes two built-in NodePools (and a NodeClass) that define how the cluster compute capacity is provisioned, out of the box:

The system node pool, intended for cluster-critical applications
The general-purpose node pool
While the general-purpose would work for most general applications' needs and help customers to get started (as seen in the Getting Started section of one of the previous modules), for the purposes of the migration, we will use a different node pool.

Note that we opted out of creating the general-purpose node pool in the update-cluster-config command above.

We will use the configuration of the apps node pool we already have as a starting point, with some minor adjustments, to create a new custom node pool for EKS Auto Mode.

Note that EKS Auto Mode doesn't allow changing the general-purpose node pool and the corresponding default node class. Hence, it is recommended to analyze the Kubernetes scheduling constraints in use such as labels, taints, and tolerations, affinity, and anti-affinity rules, and create EKS Auto Mode node pools accordingly.

Create a Custom EKS Auto Mode Configuration (NodeClass and NodePool)
➤ Create the apps-auto-mode-np.yml file:

cat << EOF > apps-auto-mode-np.yml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: apps-auto-mode
spec:
  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 30s
  template:
    metadata:
      labels:
        role: apps-auto-mode
    spec:
      nodeClassRef:
        group: eks.amazonaws.com
        kind: NodeClass
        name: default
      taints:
        - key: role
          value: apps-auto-mode
          effect: NoSchedule
      requirements:
        - key: kubernetes.io/arch
          operator: In
          values: [amd64, arm64]
        - key: kubernetes.io/os
          operator: In
          values: [linux]
        - key: karpenter.sh/capacity-type
          operator: In
          values: [on-demand]
        - key: eks.amazonaws.com/instance-category
          operator: In
          values: [c, m, r]
        - key: eks.amazonaws.com/instance-generation
          operator: Gt
          values: ["4"]
EOF

To adjust the apps NodePool to EKS Auto Mode, we’ve introduced the following changes:

Renamed the node pool to apps-auto-mode
Changed the node class reference to the EKS Auto Mode default NodeClass
Renamed the label and the taint values to apps-auto-mode
Removed the karpenter.k8s.aws/instance-generation as it is restricted by EKS Auto Mode
➤ Deploy the node pool:

kubectl apply -f apps-auto-mode-np.yml

➤ Verify that all node pools and node classes are fully operational (READY=True):

kubectl get nodepool,nodeclass,ec2nodeclass

Note that at this point no instances are registered with any of the node pools:

NAME                                   NODECLASS   NODES   READY   AGE
nodepool.karpenter.sh/apps             apps        8       True    92m
nodepool.karpenter.sh/apps-auto-mode   default     0       True    56s
nodepool.karpenter.sh/system           default     0       True    12m

NAME                                  ROLE                                    READY   AGE
nodeclass.eks.amazonaws.com/default   migration-cluster-auto-mode-node-role   True    12m

NAME                                  READY   AGE
ec2nodeclass.karpenter.k8s.aws/apps   True    92m
Preparing the Application
Before applying any changes, we need to ensure that the components in the cluster can withstand them with minimal impact. If we were to simply delete the nodes, all Pods associated with the nodes would be terminated and need to be re-scheduled at the same time, causing disruption to their services.

In Kubernetes, there are several ways of controlling disruptions  and the decision to apply them depends on the type of disruption  we are about to introduce.

These ways are PodDisruptionBudgets  and Deployment strategy .

Since our application components are deployed using Helm, which updates the Deployment resource, we will use a Deployment strategy, which is already included in our application components and looks like this:

...
strategy:
  rollingUpdate:
    maxUnavailable: 1
  type: RollingUpdate...
Note that while the above strategy suits our needs in the context of the workshop, for your applications, you should create strategies that match your applications' requirements.

In addition to ensuring a controlled migration process above, we also need to migrate the network resources created by/for the application components.

Our retail store demo application defines an Ingress  resource to expose its components to external traffic.

This Ingress is then handled by the self-managed AWS Load Balancer Controller to provision the Application Load Balancer above and define rules according to the Ingress definitions.

We will discuss migrating the network components in greater detail in a dedicated section further in the workshop, but the process will rely on DNS migration from the current setup to a network path we will create by leveraging the EKS Auto Mode load balancing capabilities.

With EKS Auto Mode enabled and the application prepared, we can move to migrating the application components, compute option by compute option.

Enabling EKS Auto Mode
Configuring the Cluster | Configuring EKS Auto Mode | Preparing the Application

Configuring the Cluster
Before enabling EKS Auto Mode on our cluster, we need to adjust certain IAM permissions to ensure its proper operation. This includes:

Updating the cluster's IAM role trust policy and permissions
Creating an IAM role for the EKS Auto Mode worker nodes
Let's explore these permissions in more detail in the following sections.

1. Update the Cluster IAM Role
EKS Auto Mode includes several Kubernetes capabilities as core components. These components, that would otherwise have to be managed as add-ons or self-managed controllers, include built-in support for Pod IP address assignments, Pod network policies, local DNS services, GPU plug-ins, health checkers, and EBS CSI storage.

To manage these components and ensure their function, EKS Auto Mode requires additional permissions, which are now a part of the cluster IAM role.

As described in the EKS Auto Mode documentation , the following IAM policies should be added to the cluster role, in addition to the existing AmazonEKSClusterPolicy:

AmazonEKSComputePolicy 
AmazonEKSBlockStoragePolicy 
AmazonEKSNetworkingPolicy 
AmazonEKSLoadBalancingPolicy 
In addition, EKS Auto Mode requires the sts:TagSession action to be added to the cluster IAM role's trust policy (you can view its current state in the IAM console).

➤ Create a trust policy JSON file:

cat << EOF > trust-policy.json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "Service": "eks.amazonaws.com"
            },
            "Action": [
                "sts:AssumeRole",
                "sts:TagSession"
            ]
        }
    ]
}
EOF

➤ Update the cluster IAM role and remove the trust-policy.json file:

aws iam update-assume-role-policy \
  --role-name ${MIGRATION_CLUSTER_ROLE_NAME} \
  --policy-document file://trust-policy.json

rm trust-policy.json

➤ Add the required permissions to the cluster IAM role by executing:

for POLICY_ARN in \
  "arn:aws:iam::aws:policy/AmazonEKSComputePolicy" \
  "arn:aws:iam::aws:policy/AmazonEKSBlockStoragePolicy" \
  "arn:aws:iam::aws:policy/AmazonEKSNetworkingPolicy" \
  "arn:aws:iam::aws:policy/AmazonEKSLoadBalancingPolicy"
do
  echo "Attaching policy ${POLICY_ARN} to IAM role ${MIGRATION_CLUSTER_ROLE_NAME}..."
  aws iam attach-role-policy --role-name ${MIGRATION_CLUSTER_ROLE_NAME} \
    --policy-arn ${POLICY_ARN}
done

➤ Verify that the IAM role has been updated properly:

aws iam get-role --role-name ${MIGRATION_CLUSTER_ROLE_NAME} | \
  jq -r '.Role.AssumeRolePolicyDocument.Statement[].Action[]'

aws iam list-attached-role-policies --role-name ${MIGRATION_CLUSTER_ROLE_NAME} | \
  jq -r '.AttachedPolicies[].PolicyName'

The output should include (note the AmazonEKSClusterPolicy that the role already had attached before):

sts:AssumeRole
sts:TagSession
AmazonEKSClusterPolicy
AmazonEKSNetworkingPolicy
AmazonEKSComputePolicy
AmazonEKSBlockStoragePolicy
AmazonEKSLoadBalancingPolicy
2. Create the Node IAM Role
A Node IAM role contains the minimal permissions required for EKS components (kubelet and Pod Identity agent) to perform their function.

➤ Create a trust policy JSON file:

cat << EOF > trust-policy.json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "Service": "ec2.amazonaws.com"
            },
            "Action": [
                "sts:AssumeRole"
            ]
        }
    ]
}
EOF

➤ Create the Node IAM role and remove the trust-policy.json file:

aws iam create-role \
  --role-name ${MIGRATION_CLUSTER_NAME}-auto-mode-node-role \
  --assume-role-policy-document file://trust-policy.json

rm trust-policy.json

➤ Attach the required policies:

for POLICY_ARN in \
  "arn:aws:iam::aws:policy/AmazonEKSWorkerNodeMinimalPolicy" \
  "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryPullOnly"
do
  echo "Attaching policy ${POLICY_ARN} to IAM role ${MIGRATION_CLUSTER_NAME}-auto-mode-node-role..."
  aws iam attach-role-policy --role-name ${MIGRATION_CLUSTER_NAME}-auto-mode-node-role \
    --policy-arn ${POLICY_ARN}
done

➤ Verify the created (or existing) role, its trust policy, and attached managed policies:

aws iam get-role \
  --role-name ${MIGRATION_CLUSTER_NAME}-auto-mode-node-role | \
  jq -r '.Role.AssumeRolePolicyDocument.Statement[].Action'

aws iam list-attached-role-policies \
  --role-name ${MIGRATION_CLUSTER_NAME}-auto-mode-node-role | \
  jq -r '.AttachedPolicies[].PolicyName'

The command above should produce the following output:

sts:AssumeRole
AmazonEKSWorkerNodeMinimalPolicy
AmazonEC2ContainerRegistryPullOnly
We can now enable EKS Auto Mode.

3. Enable Amazon EKS Auto Mode
➤ Define the node role ARN as an environment variable:

export NODE_ROLE_ARN=$(aws iam get-role \
  --role-name ${MIGRATION_CLUSTER_NAME}-auto-mode-node-role \
  --query "Role.Arn" --output text)

➤ Execute the following command to enable EKS Auto Mode (and verify that there are no errors in the output):

aws eks update-cluster-config \
  --region ${AWS_REGION} \
  --name ${MIGRATION_CLUSTER_NAME} \
  --compute-config "{\"nodeRoleArn\": \"${NODE_ROLE_ARN}\", \"nodePools\": [\"system\"], \"enabled\": true}" \
  --kubernetes-network-config '{"elasticLoadBalancing":{"enabled": true}}' \
  --storage-config '{"blockStorage":{"enabled": true}}'

➤ After 5 - 10 seconds, execute the following command to wait for the cluster EKS Auto Mode to become enabled:

aws eks wait cluster-active --name ${MIGRATION_CLUSTER_NAME}

You can also verify that the process of enabling EKS Auto Mode has started by navigating to the AWS EKS console , selecting the migration cluster, and ensuring that the corresponding panel looks like this (note the spinning icon on the Manage button):

Enabling EKS Auto Mode progress

After a couple of minutes, the cluster should enable EKS Auto Mode:

Enabled EKS Auto Mode

Before proceeding with the migration, let's make sure the Auto Mode autoscaling configuration exists in the cluster.

➤ Verify that all node pools (including those managed by the self-managed Karpenter) and node classes are fully operational (READY=True):

kubectl get nodepool,nodeclass,ec2nodeclass

You should see an output similar to the following, which shows the original self-managed Karpenter NodePool and EC2NodeClass, as well as the EKS Auto Mode NodePool and (a new CRD) NodeClass:

NAME                           NODECLASS   NODES   READY   AGE
nodepool.karpenter.sh/apps     apps        8       True    89m
nodepool.karpenter.sh/system   default     0       True    8m47s

NAME                                  ROLE                                    READY   AGE
nodeclass.eks.amazonaws.com/default   migration-cluster-auto-mode-node-role   True    8m47s

NAME                                  READY   AGE
ec2nodeclass.karpenter.k8s.aws/apps   True    89m
➤ Extract (if you already closed its tab in the browser) the application URL:

kubectl get ingress -n apps retail-store-app-ui-main \
  -o jsonpath="http://{.status.loadBalancer.ingress[*].hostname}{'\n'}"

➤ Verify that the application is operational by Ctrl/Cmd-clicking the URL in the output (or refreshing the tab).

We are now ready to start the migration process of our applications to EKS Auto Mode-managed instances.

Configuring EKS Auto Mode
EKS Auto Mode manages most infrastructure components automatically, while allowing customization to better fit the needs of the workloads.

EKS Auto Mode includes two built-in NodePools (and a NodeClass) that define how the cluster compute capacity is provisioned, out of the box:

The system node pool, intended for cluster-critical applications
The general-purpose node pool
While the general-purpose would work for most general applications' needs and help customers to get started (as seen in the Getting Started section of one of the previous modules), for the purposes of the migration, we will use a different node pool.

Note that we opted out of creating the general-purpose node pool in the update-cluster-config command above.

We will use the configuration of the apps node pool we already have as a starting point, with some minor adjustments, to create a new custom node pool for EKS Auto Mode.

Note that EKS Auto Mode doesn't allow changing the general-purpose node pool and the corresponding default node class. Hence, it is recommended to analyze the Kubernetes scheduling constraints in use such as labels, taints, and tolerations, affinity, and anti-affinity rules, and create EKS Auto Mode node pools accordingly.

Create a Custom EKS Auto Mode Configuration (NodeClass and NodePool)
➤ Create the apps-auto-mode-np.yml file:

cat << EOF > apps-auto-mode-np.yml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: apps-auto-mode
spec:
  disruption:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 30s
  template:
    metadata:
      labels:
        role: apps-auto-mode
    spec:
      nodeClassRef:
        group: eks.amazonaws.com
        kind: NodeClass
        name: default
      taints:
        - key: role
          value: apps-auto-mode
          effect: NoSchedule
      requirements:
        - key: kubernetes.io/arch
          operator: In
          values: [amd64, arm64]
        - key: kubernetes.io/os
          operator: In
          values: [linux]
        - key: karpenter.sh/capacity-type
          operator: In
          values: [on-demand]
        - key: eks.amazonaws.com/instance-category
          operator: In
          values: [c, m, r]
        - key: eks.amazonaws.com/instance-generation
          operator: Gt
          values: ["4"]
EOF

To adjust the apps NodePool to EKS Auto Mode, we’ve introduced the following changes:

Renamed the node pool to apps-auto-mode
Changed the node class reference to the EKS Auto Mode default NodeClass
Renamed the label and the taint values to apps-auto-mode
Removed the karpenter.k8s.aws/instance-generation as it is restricted by EKS Auto Mode
➤ Deploy the node pool:

kubectl apply -f apps-auto-mode-np.yml

➤ Verify that all node pools and node classes are fully operational (READY=True):

kubectl get nodepool,nodeclass,ec2nodeclass

Note that at this point no instances are registered with any of the node pools:

NAME                                   NODECLASS   NODES   READY   AGE
nodepool.karpenter.sh/apps             apps        8       True    92m
nodepool.karpenter.sh/apps-auto-mode   default     0       True    56s
nodepool.karpenter.sh/system           default     0       True    12m

NAME                                  ROLE                                    READY   AGE
nodeclass.eks.amazonaws.com/default   migration-cluster-auto-mode-node-role   True    12m

NAME                                  READY   AGE
ec2nodeclass.karpenter.k8s.aws/apps   True    92m
Preparing the Application
Before applying any changes, we need to ensure that the components in the cluster can withstand them with minimal impact. If we were to simply delete the nodes, all Pods associated with the nodes would be terminated and need to be re-scheduled at the same time, causing disruption to their services.

In Kubernetes, there are several ways of controlling disruptions  and the decision to apply them depends on the type of disruption  we are about to introduce.

These ways are PodDisruptionBudgets  and Deployment strategy .

Since our application components are deployed using Helm, which updates the Deployment resource, we will use a Deployment strategy, which is already included in our application components and looks like this:

...
strategy:
  rollingUpdate:
    maxUnavailable: 1
  type: RollingUpdate...
Note that while the above strategy suits our needs in the context of the workshop, for your applications, you should create strategies that match your applications' requirements.

In addition to ensuring a controlled migration process above, we also need to migrate the network resources created by/for the application components.

Our retail store demo application defines an Ingress  resource to expose its components to external traffic.

This Ingress is then handled by the self-managed AWS Load Balancer Controller to provision the Application Load Balancer above and define rules according to the Ingress definitions.

We will discuss migrating the network components in greater detail in a dedicated section further in the workshop, but the process will rely on DNS migration from the current setup to a network path we will create by leveraging the EKS Auto Mode load balancing capabilities.

With EKS Auto Mode enabled and the application prepared, we can move to migrating the application components, compute option by compute option.

Migrating from AWS Fargate
Migrate the orders Component

In this section of the module, we will demonstrate how to migrate applications deployed in Fargate to EKS Auto Mode using the orders component of our retail store application.

We will update the component to migrate to EKS Auto Mode by adding the relevant node selector and tolerations to align with the requirements defined in the apps-auto-mode EKS Auto Mode node pool.

Let's make sure we check the application health and availability by visiting the application load balancer URL from a browser and adding a few items to the cart.

We can access the application UI by executing the following command to extract the Application Load Balancer DNS name and Ctrl/Cmd-clicking on the URL in the output:

kubectl get ingress -n apps retail-store-app-ui-main \
  -o jsonpath="http://{.status.loadBalancer.ingress[*].hostname}{'\n'}"

Migrate the orders Component
➤ Observe the current state of the orders component:

kubectl get pods -n apps -l app.kubernetes.io/instance=retail-store-app-orders -o wide

The output should show, similar to the below, that the orders component is currently hosted exclusively on Fargate nodes.

NAME                                       READY   STATUS    RESTARTS   AGE     IP            NODE                                                   NOMINATED NODE   READINESS GATES
retail-store-app-orders-5d5d9d9dfc-7wc6s   1/1     Running   0          3h29m   10.0.74.111   fargate-ip-10-0-74-111.eu-central-1.compute.internal   <none>           <none>
retail-store-app-orders-5d5d9d9dfc-fkks2   1/1     Running   0          3h24m   10.0.85.172   fargate-ip-10-0-85-172.eu-central-1.compute.internal   <none>           <none>
retail-store-app-orders-5d5d9d9dfc-mhw9h   1/1     Running   0          3h24m   10.0.49.132   fargate-ip-10-0-49-132.eu-central-1.compute.internal   <none>           <none>
➤ You can review the distribution of instances across all capacity types by executing the following command:

kubectl get nodes -o json | jq -r '.items[].metadata.labels | ."kubernetes.io/hostname" + " | " + ."topology.kubernetes.io/zone" + " | " + (."eks.amazonaws.com/capacityType" // if ."eks.amazonaws.com/compute-type" == "fargate" then "FARGATE" else "KARPENTER" end) + " | " + (."eks.amazonaws.com/nodegroup" // if ."karpenter.sh/nodepool" then "nodepool: " + ."karpenter.sh/nodepool" else "profile: apps" end)' | column -t | sort -k 5

The command output should show the Fargate instances that host the orders component above:

fargate-ip-192-168-116-56.us-west-2.compute.internal   |  us-west-2c  |  FARGATE    |  profile:    apps
fargate-ip-192-168-180-168.us-west-2.compute.internal  |  us-west-2b  |  FARGATE    |  profile:    apps
fargate-ip-192-168-191-115.us-west-2.compute.internal  |  us-west-2b  |  FARGATE    |  profile:    apps
ip-192-168-103-128.us-west-2.compute.internal          |  us-west-2c  |  KARPENTER  |  nodepool:   apps
ip-192-168-114-228.us-west-2.compute.internal          |  us-west-2c  |  KARPENTER  |  nodepool:   apps
ip-192-168-127-58.us-west-2.compute.internal           |  us-west-2c  |  KARPENTER  |  nodepool:   apps
ip-192-168-175-162.us-west-2.compute.internal          |  us-west-2b  |  KARPENTER  |  nodepool:   apps
ip-192-168-28-67.us-west-2.compute.internal            |  us-west-2c  |  ON_DEMAND  |  apps-mng
ip-192-168-51-140.us-west-2.compute.internal           |  us-west-2a  |  ON_DEMAND  |  apps-mng
ip-192-168-83-58.us-west-2.compute.internal            |  us-west-2b  |  ON_DEMAND  |  apps-mng
ip-192-168-26-158.us-west-2.compute.internal           |  us-west-2c  |  ON_DEMAND  |  system-mng  
ip-192-168-33-46.us-west-2.compute.internal            |  us-west-2a  |  ON_DEMAND  |  system-mng  
ip-192-168-86-255.us-west-2.compute.internal           |  us-west-2b  |  ON_DEMAND  |  system-mng  
➤ Create the orders-values.yml file for the orders component (see here  for the whole values.yml file):

cat << EOF > orders-values.yml
replicaCount: 3

podAnnotations:
  eks.amazonaws.com/compute-type: ec2

nodeSelector:
  role: apps-auto-mode

tolerations:
  - key: role
    value: apps-auto-mode
    operator: Equal
    effect: NoSchedule

topologySpreadConstraints:
  - maxSkew: 1
    minDomains: 3
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        app.kubernetes.io/instance: retail-store-app-orders
EOF

To ensure that the orders components are hosted on instances provisioned through the EKS Auto Mode node pool we defined in the previous section, we've added the corresponding node selector and tolerations.

Note that we added a Pod annotation: eks.amazonaws.com/compute-type above to ensure that Pods currently deployed on Fargate instances would gradually migrate to EKS Auto Mode.

Had we created the node selector only, without specifying the annotation, the Pods would be terminated and remained in the Pending state, since the apps Fargate profile instances do not have the corresponding label.

Alternatively, had we deleted the apps Fargate profile, that would cause all orders Pods to be terminated and scheduled at the same time, causing disruption to the application's services.

➤ In a separate VS Code terminal, execute the following command to observe the migration process:

kubectl get pods -n apps -l app.kubernetes.io/name=orders -w

➤ Update the orders component:

1
2
3
4
5
helm upgrade retail-store-app-orders oci://public.ecr.aws/aws-containers/retail-store-sample-orders-chart \
  --version ${RETAIL_STORE_APP_HELM_CHART_VERSION} \
  --namespace apps \
  --values orders-values.yml \
  --wait

➤ Observe that the application remains operational, while the component is being migrated, by navigating to the application UI, Ctrl/Cmd-click the URL in the output, and adding a couple of items to the cart:

kubectl get ingress -n apps retail-store-app-ui-main \
  -o jsonpath="http://{.status.loadBalancer.ingress[*].hostname}{'\n'}"

➤ After a couple of minutes, verify that all orders Pods are scheduled on the apps-auto-mode node pool by executing:

export APPS_KARPENTER_AUTO_MODE_NODES=$(kubectl get nodes -o json | jq -r '[.items[].metadata.labels | select(."karpenter.sh/nodepool" == "apps-auto-mode") | ."kubernetes.io/hostname"] | join("\\|")')
kubectl get pods -n apps -l app.kubernetes.io/instance=retail-store-app-orders -o wide | grep "${APPS_KARPENTER_AUTO_MODE_NODES}"

The expected output should be similar to the following, where the instance name being of the form i-xxxxxxxxxxxxxxxxx indicates that it belongs to EKS Auto Mode:

retail-store-app-orders-55c98b46d4-cd68l   1/1     Running   0          5m39s   192.168.8.160    i-0a075bf0169b36e2c
retail-store-app-orders-55c98b46d4-cn6mm   1/1     Running   0          6m21s   192.168.74.64    i-0b694975e1e8ac9c7
retail-store-app-orders-55c98b46d4-fzxt6   1/1     Running   0          2m18s   192.168.42.224   i-001a27779836de720
➤ We can, once more, review the distribution of instances across all capacity types by executing the following command and observing the change (removal of Fargate nodes and a new type of nodes provisioned via EKS Auto Mode apps-auto-mode node pool)

kubectl get nodes -o json | jq -r '.items[].metadata.labels | ."kubernetes.io/hostname" + " | " + ."topology.kubernetes.io/zone" + " | " + (."eks.amazonaws.com/capacityType" // if ."eks.amazonaws.com/compute-type" == "fargate" then "FARGATE" else "KARPENTER" end) + " | " + (."eks.amazonaws.com/nodegroup" // if ."karpenter.sh/nodepool" then "nodepool: " + ."karpenter.sh/nodepool" else "profile: apps" end)' | column -t | sort -k 5

The output should show instances provisioned through new EKS Auto Mode nodepool and no Fargate instances in the cluster:

ip-192-168-153-77.us-west-2.compute.internal   |  us-west-2b  |  KARPENTER  |  nodepool:   apps
ip-192-168-165-109.us-west-2.compute.internal  |  us-west-2c  |  KARPENTER  |  nodepool:   apps
ip-192-168-97-157.us-west-2.compute.internal   |  us-west-2a  |  KARPENTER  |  nodepool:   apps
i-001a27779836de720                            |  us-west-2b  |  KARPENTER  |  nodepool:   apps-auto-mode
i-0a075bf0169b36e2c                            |  us-west-2a  |  KARPENTER  |  nodepool:   apps-auto-mode
i-0b694975e1e8ac9c7                            |  us-west-2c  |  KARPENTER  |  nodepool:   apps-auto-mode
ip-192-168-136-11.us-west-2.compute.internal   |  us-west-2b  |  ON_DEMAND  |  apps-mng
ip-192-168-171-62.us-west-2.compute.internal   |  us-west-2c  |  ON_DEMAND  |  apps-mng
ip-192-168-98-145.us-west-2.compute.internal   |  us-west-2a  |  ON_DEMAND  |  apps-mng
ip-192-168-126-11.us-west-2.compute.internal   |  us-west-2a  |  ON_DEMAND  |  system-mng  
ip-192-168-151-213.us-west-2.compute.internal  |  us-west-2b  |  ON_DEMAND  |  system-mng  
ip-192-168-184-34.us-west-2.compute.internal   |  us-west-2c  |  ON_DEMAND  |  system-mng  


Migrating from a Self-Managed Karpenter
Migrate the Checkout Component | Migrate the Carts Component | Migrate the UI Component

In this section of the module, we will demonstrate how to migrate applications from a self-managed Karpenter to EKS Auto Mode.

In the current setup, the self-managed Karpenter controller handles the compute requirements for the checkout, carts, and UI components of the retail store application.

We will update these components to migrate to EKS Auto Mode by adding the relevant node selector and tolerations to align with the requirements defined in the apps-auto-mode EKS Auto Mode node pool.

Migrate the checkout Component
➤ Create the checkout-values.yml file for the checkout component (see here  for the whole values.yml file):

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
cat << EOF > checkout-values.yml
replicaCount: 3

nodeSelector:
  role: apps-auto-mode

tolerations:
  - key: role
    value: apps-auto-mode
    operator: Equal
    effect: NoSchedule

redis:
  nodeSelector:
    role: apps-auto-mode
  tolerations:
    - key: role
      value: apps-auto-mode
      operator: Equal
      effect: NoSchedule
EOF

➤ In a separate IDE terminal, execute the following command to observe the migration process:

kubectl get pods -n apps -l app.kubernetes.io/name=checkout -w

➤ Update the checkout component:

1
2
3
4
5
helm upgrade retail-store-app-checkout oci://public.ecr.aws/aws-containers/retail-store-sample-checkout-chart \
  --version ${RETAIL_STORE_APP_HELM_CHART_VERSION} \
  --namespace apps \
  --values checkout-values.yml \
  --wait

➤ After a couple of minutes, we can verify that all checkout Pods are scheduled on the apps-auto-mode nodes by executing:

export APPS_KARPENTER_AUTO_MODE_NODES=$(kubectl get nodes -o json | jq -r '[.items[].metadata.labels | select(."karpenter.sh/nodepool" == "apps-auto-mode") | ."kubernetes.io/hostname"] | join("\\|"'))
kubectl get pods -n apps -l app.kubernetes.io/instance=retail-store-app-checkout -o wide | grep "${APPS_KARPENTER_AUTO_MODE_NODES}"

Migrate the carts Component
➤ Create the carts-values.yml file for the carts component (see here  for the whole values.yml file):

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
cat << EOF > carts-values.yml
replicaCount: 3

nodeSelector:
  role: apps-auto-mode

tolerations:
  - key: role
    value: apps-auto-mode
    operator: Equal
    effect: NoSchedule

dynamodb:
  nodeSelector:
    role: apps-auto-mode
  tolerations:
    - key: role
      value: apps-auto-mode
      operator: Equal
      effect: NoSchedule
EOF

➤ In a separate IDE terminal, execute the following command to observe the migration process:

kubectl get pods -n apps -l app.kubernetes.io/name=carts -w

➤ Now, let's update the carts component:

1
2
3
4
5
helm upgrade retail-store-app-carts oci://public.ecr.aws/aws-containers/retail-store-sample-cart-chart \
  --version ${RETAIL_STORE_APP_HELM_CHART_VERSION} \
  --namespace apps \
  --values carts-values.yml \
  --wait

➤ After a couple of minutes, verify that all carts Pods are scheduled on the apps-auto-mode nodes by executing:

export APPS_KARPENTER_AUTO_MODE_NODES=$(kubectl get nodes -o json | jq -r '[.items[].metadata.labels | select(."karpenter.sh/nodepool" == "apps-auto-mode") | ."kubernetes.io/hostname"] | join("\\|")')
kubectl get pods -n apps -l app.kubernetes.io/instance=retail-store-app-carts -o wide | grep "${APPS_KARPENTER_AUTO_MODE_NODES}"

Migrate the UI Component
➤ Create the ui-values.yml file for the UI component (see here  for the whole values.yml file):

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
cat << EOF > ui-values.yml
replicaCount: 3

app:
  theme: default
  endpoints:
    catalog: http://retail-store-app-catalog:80
    carts: http://retail-store-app-carts:80
    checkout: http://retail-store-app-checkout:80
    orders: http://retail-store-app-orders:80

nodeSelector:
  role: apps-auto-mode

tolerations:
  - key: role
    value: apps-auto-mode
    operator: Equal
    effect: NoSchedule
EOF

➤ In a separate IDE terminal, execute the following command to observe the migration process:

kubectl get pods -n apps -l app.kubernetes.io/name=ui -w

➤ Update the UI component:

1
2
3
4
5
6
helm upgrade retail-store-app-ui oci://public.ecr.aws/aws-containers/retail-store-sample-ui-chart \
  --version ${RETAIL_STORE_APP_HELM_CHART_VERSION} \
  --namespace apps \
  --values ui-values.yml \
  --reuse-values \
  --wait

➤ After a couple of minutes, verify that all ui Pods are scheduled on the apps-auto-mode nodes by executing:

export APPS_KARPENTER_AUTO_MODE_NODES=$(kubectl get nodes -o json | jq -r '[.items[].metadata.labels | select(."karpenter.sh/nodepool" == "apps-auto-mode") | ."kubernetes.io/hostname"] | join("\\|")')
kubectl get pods -n apps -l app.kubernetes.io/instance=retail-store-app-ui -o wide | grep "${APPS_KARPENTER_AUTO_MODE_NODES}"

➤ View all the Pods (excluding DaemonSets) and their nodes to confirm the migration:

export DAEMONSETS_PODS=$(kubectl get ds -n kube-system -o json | jq -r '[.items[].metadata.name] | join ("\\|")')
kubectl get pods -A -o wide | grep -v "${DAEMONSETS_PODS}"

We can omit DaemonSets to reduce visual clutter, since they do not impact the compute distribution.

The expected output should be similar to the following (omitting the kube-system namespace components for brevity):

NAMESPACE     NAME                                            READY   STATUS    RESTARTS   AGE     IP                NODE                                            NOMINATED NODE   READINESS GATES
apps          retail-store-app-carts-7f8d59668-68nsc          1/1     Running   0          4m15s   192.168.46.178    i-0d26a453b47d25d9f                             <none>           <none>
apps          retail-store-app-carts-7f8d59668-pbwzm          1/1     Running   0          4m15s   192.168.8.242     i-085c7fe5654a651c8                             <none>           <none>
apps          retail-store-app-carts-7f8d59668-pgvjm          1/1     Running   0          3m55s   192.168.74.149    i-0ce7c07eb76652c22                             <none>           <none>
apps          retail-store-app-catalog-59d4c7ddcf-47zdb       1/1     Running   0          11m     192.168.8.240     i-085c7fe5654a651c8                             <none>           <none>
apps          retail-store-app-catalog-59d4c7ddcf-99tgc       1/1     Running   0          11m     192.168.46.176    i-0d26a453b47d25d9f                             <none>           <none>
apps          retail-store-app-catalog-59d4c7ddcf-n4l6h       1/1     Running   0          11m     192.168.74.147    i-0ce7c07eb76652c22                             <none>           <none>
apps          retail-store-app-catalog-mysql-0                1/1     Running   0          2d6h    192.168.119.154   ip-192-168-112-81.us-west-2.compute.internal    <none>           <none>
apps          retail-store-app-checkout-6d9f5b4d4c-6t9kf      1/1     Running   0          5m21s   192.168.8.241     i-085c7fe5654a651c8                             <none>           <none>
apps          retail-store-app-checkout-6d9f5b4d4c-ckczm      1/1     Running   0          4m58s   192.168.74.148    i-0ce7c07eb76652c22                             <none>           <none>
apps          retail-store-app-checkout-6d9f5b4d4c-rd769      1/1     Running   0          5m21s   192.168.46.177    i-0d26a453b47d25d9f                             <none>           <none>
apps          retail-store-app-orders-7f8bfccb58-5vkh7        1/1     Running   0          14m     192.168.74.144    i-0ce7c07eb76652c22                             <none>           <none>
apps          retail-store-app-orders-7f8bfccb58-9gw88        1/1     Running   0          13m     192.168.74.146    i-0ce7c07eb76652c22                             <none>           <none>
apps          retail-store-app-orders-7f8bfccb58-b72kt        1/1     Running   0          14m     192.168.74.145    i-0ce7c07eb76652c22                             <none>           <none>
apps          retail-store-app-ui-fb8d55cc9-65vq9             1/1     Running   0          49s     192.168.46.179    i-0d26a453b47d25d9f                             <none>           <none>
apps          retail-store-app-ui-fb8d55cc9-vpjkw             1/1     Running   0          49s     192.168.8.243     i-085c7fe5654a651c8                             <none>           <none>
apps          retail-store-app-ui-fb8d55cc9-vr85v             1/1     Running   0          32s     192.168.74.150    i-0ce7c07eb76652c22                             <none>           <none>
...
You can see that all other application components' Pods in the apps namespace are running on EKS Auto Mode instances, while the MySQL and cluster operation software (controllers and add-ons) are not.

➤ View the distribution of instances across capacity types:

kubectl get nodes -o json | jq -r '.items[].metadata.labels | ."kubernetes.io/hostname" + " | " + ."topology.kubernetes.io/zone" + " | " + (."eks.amazonaws.com/capacityType" // if ."eks.amazonaws.com/compute-type" == "fargate" then "FARGATE" else "KARPENTER" end) + " | " + (."eks.amazonaws.com/nodegroup" // if ."karpenter.sh/nodepool" then "nodepool: " + ."karpenter.sh/nodepool" else "profile: apps" end)' | column -t | sort -k 5

The result should be similar to the following:

i-085c7fe5654a651c8                            |  us-west-2a  |  KARPENTER  |  nodepool:   apps-auto-mode
i-0ce7c07eb76652c22                            |  us-west-2c  |  KARPENTER  |  nodepool:   apps-auto-mode
i-0d26a453b47d25d9f                            |  us-west-2b  |  KARPENTER  |  nodepool:   apps-auto-mode
...
ip-192-168-112-81.us-west-2.compute.internal   |  us-west-2a  |  ON_DEMAND  |  apps-mng
ip-192-168-150-83.us-west-2.compute.internal   |  us-west-2b  |  ON_DEMAND  |  apps-mng
ip-192-168-184-40.us-west-2.compute.internal   |  us-west-2c  |  ON_DEMAND  |  apps-mng
ip-192-168-133-200.us-west-2.compute.internal  |  us-west-2b  |  ON_DEMAND  |  system-mng  
ip-192-168-171-235.us-west-2.compute.internal  |  us-west-2c  |  ON_DEMAND  |  system-mng  
ip-192-168-99-105.us-west-2.compute.internal   |  us-west-2a  |  ON_DEMAND  |  system-mng  
Observing the `apps` node pool
Note that you may continue to see the apps node pool with its now empty nodes, while Karpenter terminates them.

We can verify (by running kubectl describe node... commands for system-mng and apps-mng node group nodes) that the only non-DaemonSet pods that remain on them are the operational software (like self-managed Karpenter) and the catalog component's MySQL pod.

We now successfully migrated all the application components pods (again, excluding catalog component's MySQL database) to EKS Auto Mode.

Migrating Networking Resources
Create New Load Balancers | Migrate to the New Load Balancers

In this section, we will demonstrate how to migrate networking resources, specifically AWS Application Load Balancer (ALB) and Network Load Balancer (NLB).

When using the AWS Load Balancer Controller, as in our setup, creating a Kubernetes Ingress resource automatically provisions an AWS ALB, while creating a Kubernetes Service resource of type LoadBalancer provisions an AWS NLB for application traffic distribution.

Create New Load Balancers
In our workshop setup, the UI application is configured with a Kubernetes Ingress resource and is accessible through an AWS ALB.

Access the UI
You can access the application UI by executing the following command to extract the ALB DNS name and Ctrl/Cmd-clicking on the URL in the output:

kubectl get ingress -n apps retail-store-app-ui-main \
  -o jsonpath="http://{.status.loadBalancer.ingress[*].hostname}{'\n'}"

When migrating applications that use LoadBalancers to EKS Auto Mode, we need to create new load balancers, as existing ones, managed by the self-managed AWS Load Balancer controller, cannot be migrated to EKS Auto Mode.

See here  for additional information on migrating resources in existing Amazon EKS clusters.

In the UI component Helm implementation, we have defined Ingress resources as an array , enabling us to create a new Ingress resource while maintaining the existing Ingress-created ALB during migration.

➤ View the application's Ingress resources:

kubectl get ingress -n apps

If you're not using Helm, we recommend creating a new Ingress resource with a different name that uses the Ingress className created specifically for EKS Auto Mode.

So, before proceeding with the migration, we will create a new IngressClass.

This setup ensures that the necessary infrastructure configurations are in place to support ingress requirements for applications deployed later. Typically, this is a step performed by the platform team once after the cluster is created.

➤ Define the new IngressClass:

cat << EOF > ingress-class.yml
apiVersion: eks.amazonaws.com/v1
kind: IngressClassParams
metadata:
  name: eks-auto-mode-alb
spec:
  scheme: internet-facing
---
apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  name: eks-auto-mode-alb
spec:
  controller: eks.amazonaws.com/alb
  parameters:
    apiGroup: eks.amazonaws.com
    kind: IngressClassParams
    name: eks-auto-mode-alb
EOF

➤ Now let's deploy the IngressClass:

kubectl apply -f ingress-class.yml

With EKS Auto Mode enabled and the application prepared, we can move to migrating the application components, compute option by compute option.

In the following Helm configuration, we are configuring a new Ingress resource by using a new IngressClass with the className: eks-auto-alb for the second Ingress resource definition.

➤ Create the ui-ingress-values.yml file for the UI component (see here  for the whole values.yml file):

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
cat << EOF > ui-ingress-values.yml
ingresses:
  - enabled: true
    className: alb
    name: main
    annotations:
      alb.ingress.kubernetes.io/scheme: internet-facing
      alb.ingress.kubernetes.io/target-type: ip
  - enabled: true
    className: eks-auto-mode-alb
    name: main-auto-mode
    annotations:
      alb.ingress.kubernetes.io/scheme: internet-facing
      alb.ingress.kubernetes.io/target-type: ip
EOF

Note that we need to define both the old and the new Ingress resources to avoid overriding the original configuration.

➤ Update the UI component:

1
2
3
4
5
6
helm upgrade retail-store-app-ui oci://public.ecr.aws/aws-containers/retail-store-sample-ui-chart \
  --version ${RETAIL_STORE_APP_HELM_CHART_VERSION} \
  --namespace apps \
  --values ui-ingress-values.yml \
  --reuse-values \
  --wait

➤ Verify that there are now two Ingress resources:

kubectl get ingress -n apps

➤ After a couple of minutes, let's verify that we can access the application UI via the new ALB by executing the following command to extract the DNS name and Ctrl/Cmd-clicking on the URL in the output:

export AUTO_MODE_ALB_URL=$(kubectl get ingress -n apps retail-store-app-ui-main-auto-mode -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')

aws elbv2 wait load-balancer-available --load-balancer-arns $(aws elbv2 describe-load-balancers --query 'LoadBalancers[?DNSName==`'"$AUTO_MODE_ALB_URL"'`].LoadBalancerArn' --output text)
echo "The shared ALB is available at: http://$AUTO_MODE_ALB_URL"

➤ Also let's verify that we can access the application UI via the old ALB by executing the following command to extract the DNS name and Ctrl/Cmd-clicking on the URL in the output:

kubectl get ingress -n apps retail-store-app-ui-main \
  -o jsonpath="http://{.status.loadBalancer.ingress[*].hostname}{'\n'}"

Migrate to the New Load Balancers
To ensure a successful migration, we should follow several best practices:

Schedule migrations during low-traffic periods and implement a DNS-based migration strategy for zero downtime.

EKS Auto Mode maintains compatibility with external-dns , which automatically registers new load balancers with Route 53. If we're not using external-dns, we'll need to update DNS entries either manually or through automation.

During the migration period, it is essential to maintain both the old and the new load balancers while continuously monitoring performance metrics.

Have a comprehensive rollback plan ready for any unexpected issues.

Once testing confirms a successful migration, we can remove the old load balancer by deleting its Ingress resource. In this example, we'll execute a helm upgrade to update the ingress array, removing the old ingress entry.

➤ Create the ui-single-ingress-values.yml file:

1
2
3
4
5
6
7
8
9
cat << EOF > ui-single-ingress-values.yml
ingresses:
  - enabled: true
    className: eks-auto-mode-alb
    name: main-auto-mode
    annotations:
      alb.ingress.kubernetes.io/scheme: internet-facing
      alb.ingress.kubernetes.io/target-type: ip
EOF

Note that the array above removes the second Ingress resource by overriding the entire ingresses array.

➤ Update the UI component:

1
2
3
4
5
6
helm upgrade retail-store-app-ui oci://public.ecr.aws/aws-containers/retail-store-sample-ui-chart \
  --version ${RETAIL_STORE_APP_HELM_CHART_VERSION} \
  --namespace apps \
  --values ui-single-ingress-values.yml \
  --reuse-values \
  --wait

➤ After a couple of minutes, let's verify that only one Ingress resource remains:

kubectl get ingress -n apps

➤ Let's verify that we can access the application UI via the new ALB by executing the following command to extract the DNS name and Ctrl/Cmd-clicking on the URL in the output:

kubectl get ingress -n apps retail-store-app-ui-main-auto-mode \
  -o jsonpath="http://{.status.loadBalancer.ingress[*].hostname}{'\n'}"

➤ Let's verify that we can no longer retrieve the old ALB (expected to receive error):

kubectl get ingress -n apps retail-store-app-ui-main \
  -o jsonpath="http://{.status.loadBalancer.ingress[*].hostname}{'\n'}"

Note that SSL certificates can be shared between your old and new load balancers. However, ensure that your certificate is configured with appropriate Subject Alternative Names (SANs) that match all domains served by both ALBs, particularly if you're changing domain names during migration.

Server Name Indication (SNI) enables this functionality, allowing the ALB to select the correct certificate based on the client's requested hostname.

We have now migrated almost all of our retail store application, with the catalog component's MySQL database as the only thing remaining.

Let's fix that!

Migrating Stateful Components
Overview | Migrate the catalog component's MySQL database

Overview
In our application, we have multiple stateful components:

Orders PostgreSQL database
Catalog MySQL database
Checkout Redis in-memory
Carts in-cluster DynamoDB
Sample Retail Application

Applications deployed in Amazon EKS can use several AWS storage services, including Amazon S3 , Amazon EBS , and Amazon EFS , through an appropriate CSI driver .

For simplicity and brevity during this workshop, most of the stateful sub-components (Redis, PostgreSQL, and DynamoDB) use their pods' file system (via the emptyDir ephemeral volumes) or memory. Only the catalog component's MySQL database relies on the Amazon EBS CSI driver  to provision an EBS volume to host its filesystem.

Specifically, it uses dynamic provisioning  to trigger the automatic provisioning and attachment of an EBS volume when the MySQL StatefulSet is created.

Workshop stateful components
➤ We can specifically explore these components by executing the following command:

kubectl get sts -n apps -o wide

➤ We can verify that executing the following command and verifying that only a single pod, the MySQL StatefulSet, still runs on non-EKS Auto Mode compute:

kubectl get pods -n apps -o wide

Catalog component stateful components
➤ View the catalog component's MySQL StatefulSet pods:

kubectl get sts -n apps -l app.kubernetes.io/component=mysql

➤ View the underlying PersistentVolume and PersistentVolumeClaim:

kubectl get -n apps pv,pvc

➤ View the StorageClasses:

kubectl get sc

You can see the connection between these components:

Dynamic provision

Migration challenges
The main challenges in handling the migration of EBS-based stateful components that rely on dynamic provisioning can be summarized as follows:

It is not possible to transfer the “ownership” of a PersistentVolume, as the spec.persistentvolumesource (which contains the reference to the handling driver) is immutable after creation.

The StorageClass attribute in a PersistentVolumeClaim object is immutable and cannot be updated.

It is not possible to attach both EBS CSI and Auto Mode (empty) volumes to the same node and mount them into the same Pod to perform an application-level migration of data.

Migrate the catalog component's MySQL database
We will demonstrate the migration of these components using the catalog component's MySQL as an example, but any stateful application that uses the EBS CSI driver in the same manner as above can follow the same process we are going to outline and execute below.

Additionally, for simplicity, we will call OSS EBS CSI driver-managed (and adjacent) resources “EBS CSI resources” and the Amazon EKS Auto Mode managed (and adjacent) resources “Auto Mode resources”.

In light of the challenges listed above, the migration will introduce a short downtime to the application – in this case, the catalog component's MySQL database.

For that purpose, we have developed a migration tool  that automates the process to reduce operational overhead and minimize the incurred downtime.

ATTENTION
Note that the migration process requires deleting the existing PersistentVolumeClaim/PersistentVolume and re-creating them with the new StorageClass. You must validate this process in an identical non-production environment first.

Prerequisites
The migration process validates and requires that:

The EBS volume behind the PersistentVolume must be unattached from any EC2 instances. This means scaling down the StatefulSet or Deployment to allow the volume to detach.

The new StorageClass (that belongs to EKS Auto Mode) must have a volumeBindingMode of WaitForFirstConsumer to prevent the immediate creation of a PersistentVolume.

The existing PersistentVolume must have a reclaim policy of Retain to ensure that the EBS volume remains when the PersistentVolume is deleted.

The calling process of the tool needs the appropriate Kubernetes permissions to Create/Delete the PersistentVolumeClaim and PersistentVolume (see here  for more information).

The calling process of the tool needs the appropriate AWS IAM permissions to call DescribeVolume, CreateTags on the EBS volume, and optionally, but recommended, CreateSnapshot.

➤ Execute the following:

export MYSQL_STS_NAME=retail-store-app-catalog-mysql
export MYSQL_POD_NAME=${MYSQL_STS_NAME}-0
export ORIGINAL_PVC_NAME=data-${MYSQL_POD_NAME}
echo ${ORIGINAL_PVC_NAME}

Download and install the tool
➤ Download the tool:

curl -sSL -o eks-auto-mode-ebs-migration-tool https://github.com/awslabs/eks-auto-mode-ebs-migration-tool/releases/download/v0.3.1/eks-auto-mode-ebs-migration-tool_Linux_x86_64

chmod +x eks-auto-mode-ebs-migration-tool

./eks-auto-mode-ebs-migration-tool

➤ Create a new EKS Auto Mode StorageClass manifest:

cat << EOF > am-storage-class.yml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: gp3-auto
provisioner: ebs.csi.eks.amazonaws.com
allowVolumeExpansion: true
reclaimPolicy: Retain
volumeBindingMode: WaitForFirstConsumer
parameters:
  type: gp3
  fsType: ext4
EOF

➤ Create the storage class:

kubectl apply -f am-storage-class.yml

➤ Execute the tool in the (default) dry-run mode to assess the changes planned to be done to the relevant objects and resources:

./eks-auto-mode-ebs-migration-tool \
  --cluster-name ${MIGRATION_CLUSTER_NAME} \
  --pvc-name ${ORIGINAL_PVC_NAME} \
  --namespace apps \
  --storageclass gp3-auto

As you can see, the EBS volume is still attached to the EC2 instance on which the catalog component's StatefulSet Pod is running, so it can't yet be migrated, as mentioned in the prerequisites.

E0606 09:50:35.341373 296117 main.go:135] "Precondition checks failed" err="can't migrate volume vol-07698e9d8d3917a0b that is still attached to an instance"
➤ For the sake of completeness, verify that it is the same EC2 instance by extracting its name from the Pod and via the volume:

export ORIGINAL_PV_NAME=$(kubectl get -n apps pvc ${ORIGINAL_PVC_NAME} -o jsonpath='{.spec.volumeName}')
export VOLUME_ID=$(kubectl get -n apps pv ${ORIGINAL_PV_NAME} -o jsonpath='{.spec.csi.volumeHandle}')

# Get the node name where the MySQL Pod runs on
kubectl get pods -n apps ${MYSQL_POD_NAME} -o json | jq -r '.spec.nodeName'

# Get the node name that the EBS Volume of the PVC is attached to
aws ec2 describe-instances --instance-ids $(aws ec2 describe-volumes --volume-ids ${VOLUME_ID} --output text --query='Volumes[].Attachments[].InstanceId') \
  --output text \
  --query='Reservations[].Instances[].PrivateDnsName'

The expected output is the same node name twice (one from the kubectl command, and one from the aws cli command):

ip-192-168-104-189.us-west-2.compute.internal
ip-192-168-104-189.us-west-2.compute.internal
Before scaling down the catalog component's StatefulSet to allow the volume to detach, consider other factors that may interfere with the number of replicas, most notably active application autoscaling tools like Horizontal Pod Autoscaling (HPA)  or KEDA .

It is recommended that you adjust their configuration (or disable them, if possible) to prevent scaling events during the EBS volume migration.

Our catalog component's StatefulSet doesn't have an HPA attached, so we can safely proceed with scaling it down.

➤ Execute the following command:

kubectl scale statefulsets -n apps ${MYSQL_STS_NAME} --replicas=0

During this time, the application should not be available.

➤ Execute the tool (after 10 - 15 seconds) again to verify that all prerequisites are fulfilled:

./eks-auto-mode-ebs-migration-tool \
  --cluster-name ${MIGRATION_CLUSTER_NAME} \
  --pvc-name ${ORIGINAL_PVC_NAME} \
  --namespace apps \
  --storageclass gp3-auto

➤ View the current PV and PVC state:

kubectl get pv,pvc -n apps

This should produce a result similar to the following:

NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                                        STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE
persistentvolume/pvc-c4bb4d38-3e92-4826-82d0-680c0d327bf7   10Gi       RWO            Retain           Bound    apps/data-retail-store-app-catalog-mysql-0   gp3            <unset>                          64m

NAME                                                          STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
persistentvolumeclaim/data-retail-store-app-catalog-mysql-0   Bound    pvc-c4bb4d38-3e92-4826-82d0-680c0d327bf7   10Gi       RWO            gp3            <unset>                 64m
➤ Execute the tool in the "mutate" mode (--dry-run=false) and answer YES when prompted:

./eks-auto-mode-ebs-migration-tool \
  --cluster-name ${MIGRATION_CLUSTER_NAME} \
  --pvc-name ${ORIGINAL_PVC_NAME} \
  --namespace apps \
  --storageclass gp3-auto \
  --dry-run=false

➤ Once the migration is completed, view the current PV and PVC state:

kubectl get pv,pvc -n apps

The expected output should be similar to the previous one (both PV and PVC should have the Bound status), but with new IDs and a new, EKS Auto Mode CSI driver's storage class:

NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                                        STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE
persistentvolume/pvc-6ac68662-ac3b-43f0-9fd3-c73b59c2e899   10Gi       RWO            Retain           Bound    apps/data-retail-store-app-catalog-mysql-0   gp3-auto       <unset>                          23m

NAME                                                          STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
persistentvolumeclaim/data-retail-store-app-catalog-mysql-0   Bound    pvc-6ac68662-ac3b-43f0-9fd3-c73b59c2e899   10Gi       RWO            gp3-auto       <unset>                 23m
Note that we have to execute the migration tool for each of the relevant PVCs, for example for data-retail-store-app-catalog-mysql-1, data-retail-store-app-catalog-mysql-2 etc...

After migrating all the PV and PVC objects to use the Auto Mode storage capability, we need to update our application (the catalog StatefulSet) to point to the new StorageClass that uses Auto Mode capability for storage too. Since the StatefulSet's storageClassName field is immutable, we will first need to delete the StatefulSet, and then reapply it with the right StorageClass configuration (and amount of replicas).

➤ Delete the StatefulSet of the catalog-mysql:

1
kubectl -n apps delete sts retail-store-app-catalog-mysql

We now will scale the application by updating the catalog component's Helm values.yml file, which would both migrate the MySQL Pod to use EKS Auto Mode compute and reset the number of replicas to 1.

For your applications, you should apply the above steps in a way that fits your configuration.

➤ Update the catalog-values.yml file for the catalog component (see here  for the whole values.yml file):

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
cat << EOF > catalog-values.yml
replicaCount: 3

nodeSelector:
  role: apps-auto-mode

tolerations:
  - key: role
    value: apps-auto-mode
    operator: Equal
    effect: NoSchedule

app:
  persistence:
    provider: mysql
    endpoint: ""
    database: "catalog"

    secret:
      create: true
      name: catalog-db
      username: catalog
      password: "mysqlcatalog123"

mysql:
  nodeSelector:
    role: apps-auto-mode
  tolerations:
    - key: role
      value: apps-auto-mode
      operator: Equal
      effect: NoSchedule
  persistentVolume:
    storageClass: "gp3-auto"
EOF

Pay attention to the new storageClass configuration of gp3-auto.

➤ Update the catalog component:

1
2
3
4
5
6
helm upgrade -i retail-store-app-catalog oci://public.ecr.aws/aws-containers/retail-store-sample-catalog-chart \
  --version ${RETAIL_STORE_APP_HELM_CHART_VERSION} \
  --namespace apps \
  --values catalog-values.yml \
  --reuse-values \
  --wait

➤ After a few moments, verify that we can access the application UI, via the ALB by executing the following command to extract the DNS name and Ctrl/Cmd-clicking on the URL in the output:

kubectl get ingress -n apps retail-store-app-ui-main-auto-mode \
  -o jsonpath="http://{.status.loadBalancer.ingress[*].hostname}{'\n'}"

➤ Verify that all pods are scheduled on the apps-auto-mode nodes by executing:

export APPS_KARPENTER_AUTO_MODE_NODES=$(kubectl get nodes -o json | jq -r '[.items[].metadata.labels | select(."karpenter.sh/nodepool" == "apps-auto-mode") | ."kubernetes.io/hostname"] | join("\\|")')
kubectl get pods -n apps -o wide | grep "${APPS_KARPENTER_AUTO_MODE_NODES}"

➤ View the distribution of instances across capacity types:

kubectl get nodes -o json | jq -r '.items[].metadata.labels | ."kubernetes.io/hostname" + " | " + ."topology.kubernetes.io/zone" + " | " + (."eks.amazonaws.com/capacityType" // if ."eks.amazonaws.com/compute-type" == "fargate" then "FARGATE" else "KARPENTER" end) + " | " + (."eks.amazonaws.com/nodegroup" // if ."karpenter.sh/nodepool" then "nodepool: " + ."karpenter.sh/nodepool" else "profile: apps" end)' | column -t | sort -k 5

The result should be similar to the following:

i-085c7fe5654a651c8                            |  us-west-2a  |  KARPENTER  |  nodepool:   apps-auto-mode
i-0ce7c07eb76652c22                            |  us-west-2c  |  KARPENTER  |  nodepool:   apps-auto-mode
i-0d26a453b47d25d9f                            |  us-west-2b  |  KARPENTER  |  nodepool:   apps-auto-mode
...
ip-192-168-112-81.us-west-2.compute.internal   |  us-west-2a  |  ON_DEMAND  |  apps-mng
ip-192-168-150-83.us-west-2.compute.internal   |  us-west-2b  |  ON_DEMAND  |  apps-mng
ip-192-168-184-40.us-west-2.compute.internal   |  us-west-2c  |  ON_DEMAND  |  apps-mng
ip-192-168-133-200.us-west-2.compute.internal  |  us-west-2b  |  ON_DEMAND  |  system-mng  
ip-192-168-171-235.us-west-2.compute.internal  |  us-west-2c  |  ON_DEMAND  |  system-mng  
ip-192-168-99-105.us-west-2.compute.internal   |  us-west-2a  |  ON_DEMAND  |  system-mng  
We have now migrated our retail store application to EKS Auto Mode and all that remains is to remove the no-longer-required add-ons and compute resources that belong to the now-unused managed node groups!

Removing non-EKS Auto Mode Add-ons and Compute
Uninstall Karpenter | Uninstall the AWS Load Balancer Controller | Remove the Add-ons | Delete the Node Groups | Delete the Fargate Profile

After migrating to EKS Auto Mode, which includes several Kubernetes capabilities as core components we no longer require the following components and compute options:

The AWS Fargate apps profile
The apps-mng managed node group
The system-mng managed node group
The CoreDNS add-on
The Amazon VPC CNI add-on
The kube-proxy add-on
The EBS CSI Driver add-on
The EKS Pod Identity Agent add-on
The self-managed AWS Load Balancer Controller
The self-managed Karpenter
We will now remove these components from the migration cluster.

Uninstall the Self-Managed AWS Load Balancer Controller
➤ First, we'll uninstall the AWS Load Balancer Controller Helm chart and delete its CRDs:

helm uninstall --namespace kube-system aws-load-balancer-controller
kubectl delete -k "github.com/aws/eks-charts/stable/aws-load-balancer-controller/crds?ref=master"

➤ Extract the AWS Load Balancer Controller's role Pod identity association:

export LBC_POD_IDENTITY_ASSOCIATION_ID=$(aws eks list-pod-identity-associations \
  --cluster-name ${MIGRATION_CLUSTER_NAME} \
  --namespace kube-system \
  --service-account aws-load-balancer-controller \
  --query 'associations[*].associationId' \
  --output text)

➤ Now we'll detach and delete the AWS Load Balancer Controller policy:

export LBC_ROLE_NAME=$(aws eks describe-pod-identity-association \
  --cluster-name ${MIGRATION_CLUSTER_NAME} \
  --association-id ${LBC_POD_IDENTITY_ASSOCIATION_ID} | jq -r '.association.roleArn | split("/") | .[1]')

export LBC_POLICY_ARN=$(aws iam list-attached-role-policies \
  --role-name ${LBC_ROLE_NAME} | \
  jq -r '.AttachedPolicies[].PolicyArn')

aws iam detach-role-policy \
    --role-name ${LBC_ROLE_NAME} \
    --policy-arn ${LBC_POLICY_ARN}

aws iam delete-policy \
    --policy-arn ${LBC_POLICY_ARN}

➤ Finally, we'll delete the Pod identity association:

aws eks delete-pod-identity-association \
  --cluster-name ${MIGRATION_CLUSTER_NAME} \
  --association-id ${LBC_POD_IDENTITY_ASSOCIATION_ID}

Uninstall the Self-Managed Karpenter
➤ Verify that there are no self-managed Karpenter-created nodes:

kubectl get nodes -o json | jq -r '.items[].metadata.labels | ."kubernetes.io/hostname" + " | " + ."topology.kubernetes.io/zone" + " | " + (."eks.amazonaws.com/capacityType" // if ."eks.amazonaws.com/compute-type" == "fargate" then "FARGATE" else "KARPENTER" end) + " | " + (."eks.amazonaws.com/nodegroup" // if ."karpenter.sh/nodepool" then "nodepool: " + ."karpenter.sh/nodepool" else "profile: apps" end)' | column -t | sort -k 5

➤ Now let's delete the NodePool and NodeClass:

kubectl delete nodepool apps
kubectl delete ec2nodeclass apps

➤ Next, we'll uninstall Karpenter:

helm uninstall --namespace kube-system karpenter

➤ Delete the self-managed Karpenter nodes access entry:

aws eks delete-access-entry \
  --cluster-name ${MIGRATION_CLUSTER_NAME} \
  --principal-arn arn:aws:iam::${AWS_ACCOUNT_ID}:role/KarpenterNodeRole-${MIGRATION_CLUSTER_NAME}

➤ Extract the Karpenter role Pod identity association:

export KARPENTER_POD_IDENTITY_ASSOCIATION_ID=$(aws eks list-pod-identity-associations \
  --cluster-name ${MIGRATION_CLUSTER_NAME} \
  --namespace kube-system \
  --service-account karpenter \
  --query 'associations[*].associationId' \
  --output text)

➤ Extract the Karpenter controller role name:

export KARPENTER_ROLE_NAME=$(aws eks describe-pod-identity-association \
  --cluster-name ${MIGRATION_CLUSTER_NAME} \
  --association-id ${KARPENTER_POD_IDENTITY_ASSOCIATION_ID} | jq -r '.association.roleArn | split("/") | .[1]')

➤ Detach the Karpenter policy:

aws iam detach-role-policy \
  --role-name ${KARPENTER_ROLE_NAME} \
  --policy-arn arn:aws:iam::${AWS_ACCOUNT_ID}:policy/KarpenterControllerPolicy-${MIGRATION_CLUSTER_NAME}

➤ Delete the Pod identity association:

aws eks delete-pod-identity-association \
  --cluster-name ${MIGRATION_CLUSTER_NAME} \
  --association-id ${KARPENTER_POD_IDENTITY_ASSOCIATION_ID}

➤ Remove Karpenter-related instance profiles:

KARPENTER_NODE_ROLE=KarpenterNodeRole-${MIGRATION_CLUSTER_NAME}
KARPENTER_INSTANCE_PROFILES=$(aws iam list-instance-profiles-for-role \
  --role-name ${KARPENTER_NODE_ROLE} \
  --query 'InstanceProfiles[*].InstanceProfileName' \
  --output text)

for profile in ${KARPENTER_INSTANCE_PROFILES}; do
  echo "Removing role from instance profile ${profile}"
  aws iam remove-role-from-instance-profile --instance-profile-name "${profile}" --role-name ${KARPENTER_NODE_ROLE}
  echo "Deleting instance profile ${profile}"
  aws iam delete-instance-profile --instance-profile-name "${profile}"
done

➤ Delete the Karpenter role:

aws iam delete-role --role-name ${KARPENTER_ROLE_NAME}

➤ Remove the remaining Karpenter-related resources:

aws cloudformation delete-stack --stack-name Karpenter-${MIGRATION_CLUSTER_NAME}
aws cloudformation wait stack-delete-complete --stack-name Karpenter-${MIGRATION_CLUSTER_NAME}

aws ec2 describe-launch-templates --filters "Name=tag:karpenter.k8s.aws/cluster,Values=${MIGRATION_CLUSTER_NAME}" |
    jq -r ".LaunchTemplates[].LaunchTemplateName" |
    xargs -I{} aws ec2 delete-launch-template --launch-template-name {}

Remove the Unnecessary Amazon EKS Add-ons
➤ Now we'll remove the Amazon EKS Add-ons that are no longer required since they are provided as core components in EKS Auto Mode:

aws eks delete-addon --cluster-name ${MIGRATION_CLUSTER_NAME} --addon-name metrics-server
aws eks delete-addon --cluster-name ${MIGRATION_CLUSTER_NAME} --addon-name coredns
aws eks delete-addon --cluster-name ${MIGRATION_CLUSTER_NAME} --addon-name vpc-cni
aws eks delete-addon --cluster-name ${MIGRATION_CLUSTER_NAME} --addon-name kube-proxy
aws eks delete-addon --cluster-name ${MIGRATION_CLUSTER_NAME} --addon-name aws-ebs-csi-driver
aws eks delete-addon --cluster-name ${MIGRATION_CLUSTER_NAME} --addon-name eks-pod-identity-agent

Delete the Managed Node Groups
We have now successfully migrated the application and removed all the unnecessary controllers and add-ons. With EKS Auto Mode handling our compute needs, we no longer require either of the managed node groups.

➤ Let's delete the node groups:

eksctl delete nodegroup \
  --region ${AWS_REGION} \
  --cluster ${MIGRATION_CLUSTER_NAME} \
  --name=apps-mng

eksctl delete nodegroup \
  --region ${AWS_REGION} \
  --cluster ${MIGRATION_CLUSTER_NAME} \
  --name=system-mng

Removal of the node groups and termination of their instances may take a couple of minutes.

➤ Verify that all non-EKS Auto Mode compute options were removed:

kubectl get nodes -o json | jq -r '.items[].metadata.labels | ."kubernetes.io/hostname" + " | " + ."topology.kubernetes.io/zone" + " | " + (."eks.amazonaws.com/capacityType" // if ."eks.amazonaws.com/compute-type" == "fargate" then "FARGATE" else "KARPENTER" end) + " | " + (."eks.amazonaws.com/nodegroup" // if ."karpenter.sh/nodepool" then "nodepool: " + ."karpenter.sh/nodepool" else "profile: apps" end)' | column -t | sort -k 5

Note that we've created the node groups using eksctl, so your process may slightly differ.

Delete the Fargate Profile
➤ Finally, let's delete the Fargate profile that is no longer needed:

aws eks delete-fargate-profile \
  --cluster-name ${MIGRATION_CLUSTER_NAME} \
  --fargate-profile-name apps

Cleanup Environment
If you have run the workshop In your own AWS Account it is recommended to clean up resources so no extra costs will occur.

Sign-in in to the AWS Management Console, make sure to change to the AWS Region you deployed the workshop in, use the Region selector in the upper-right corner of the page.

1. Delete the eks-automode-workshop CloudFormation Stack
Navigate to the AWS CloudFormation console 
In the Navigation pane, choose Stacks.
Search for the eks-automode-workshop Stack.
Choose the eks-automode-workshop Stack and Click Delete, a popup will open, click Delete.
Deleting the eks-automode-workshop stack will automatically delete all the additional stacks created as part of it.
Deletion is completed when you are no longer able to see the eks-automode-workshop stack and all its related stacks.
Note
This process might take a while (approximately 30 minutes).

Delete the CloudFormation Stack
2. Module 2: EBS Storage - Delete EBS volumes used by the demo cluster.
Navigate to the Amazon Elastic Compute Cloud (EC2) console 
In the Navigation pane, choose Volumes.
Search and Select the EBS volumes provisioned by the EBS CSI driver, you can use the search filter KubernetesCluster = demo-cluster
Choose Actions, and choose Delete Volume.
To confirm deletion, type delete in the field, and then click Delete.
Delete the EBS volumes
3. Module 2: EBS Storage - Delete the AWS Key Management Service (KMS) key
Navigate to the AWS Key Management Service console 
In the Navigation pane, choose Customer managed keys.
Search and Select the KMS key created for the default StorageClass encryption, you can use the search filter Name = eks-automode-workshop
Choose Key actions, and choose Schedule key deletion.
(Optional) Reduce the waiting period to a minimum of 7 days.
Ensure that the Keys to delete table only includes the keys from the workshop.
Select Confirm that you want to schedule these keys for deletion after a X day waiting period.
Click Schedule deletion.
Delete the EBS KMS Key
4. Module 4: Migration - Delete the migration cluster Node IAM Role
Navigate to the Amazon IAM Identity and Access Management (IAM) console 
In the Navigation pane, choose Roles
In the Search bar, Search for migration-cluster-auto-mode-node-role
Select the Node IAM Role and Choose Delete.
Confirm deletion by entering the role name in the text input field and click Delete.
Delete the Node IAM Role
